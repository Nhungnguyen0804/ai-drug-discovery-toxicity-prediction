{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13647884,"sourceType":"datasetVersion","datasetId":8676130}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:58:54.803029Z","iopub.execute_input":"2025-11-10T14:58:54.803356Z","iopub.status.idle":"2025-11-10T14:58:54.822076Z","shell.execute_reply.started":"2025-11-10T14:58:54.803333Z","shell.execute_reply":"2025-11-10T14:58:54.821276Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/tox21data/tox21.csv\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!pip install torch-geometric\n!pip install rdkit-pypi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:58:54.823348Z","iopub.execute_input":"2025-11-10T14:58:54.823584Z","iopub.status.idle":"2025-11-10T14:59:01.148891Z","shell.execute_reply.started":"2025-11-10T14:58:54.823568Z","shell.execute_reply":"2025-11-10T14:59:01.147916Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.7.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.13.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.10.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.1.3)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.22.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.10.5)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: rdkit-pypi in /usr/local/lib/python3.11/dist-packages (2022.9.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi) (11.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit-pypi) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit-pypi) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit-pypi) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit-pypi) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit-pypi) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit-pypi) (2024.2.0)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GATConv, global_mean_pool, global_max_pool, global_add_pool\nfrom torch_geometric.explain import Explainer, GNNExplainer\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:01.150028Z","iopub.execute_input":"2025-11-10T14:59:01.150282Z","iopub.status.idle":"2025-11-10T14:59:01.155941Z","shell.execute_reply.started":"2025-11-10T14:59:01.150258Z","shell.execute_reply":"2025-11-10T14:59:01.155242Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom rdkit import Chem\nimport torch\nfrom torch_geometric.data import Data\nimport random\n\n# ============================================================================\n# LOAD D·ªÆ LI·ªÜU\n# ============================================================================\n\nTOX21 = \"/kaggle/input/tox21data/tox21.csv\"\ndf = pd.read_csv(TOX21)\n\nprint('='*70)\nprint('üìä LOAD DATA')\nprint('='*70)\nprint(df.head())\n\nlabel_cols = [\n    \"NR-AR\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\",\n    \"NR-ER\", \"NR-ER-LBD\", \"NR-PPAR-gamma\",\n    \"SR-ARE\", \"SR-ATAD5\", \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n]\n\nsmiles_df = df[\"smiles\"].tolist()\nmol_ids_df = df[\"mol_id\"].tolist()\nlabels_df = df[label_cols].values  # (n_samples, 12)\n\nprint(f\"\\n‚úÖ SMILES: {len(smiles_df)} molecules\")\nprint(f\"‚úÖ Labels shape: {labels_df.shape}\")\n\n# ============================================================================\n# T·∫†O LABELS_CLEAN + MASK\n# ============================================================================\n\nlabels_clean = np.nan_to_num(labels_df, nan=0.0)\nmask = (~np.isnan(labels_df)).astype(np.float32)\n\nprint(f\"‚úÖ Labels_clean shape: {labels_clean.shape}\")\nprint(f\"‚úÖ Mask shape: {mask.shape}\")\n\n# ============================================================================\n# BUILD DATASET\n# ============================================================================\n\ndataset = []\nfailed = 0\n\nprint(f\"\\n{'='*70}\")\nprint(\"üèóÔ∏è  BUILD DATASET\")\nprint(f\"{'='*70}\")\n\nfor i, smi in enumerate(smiles_df):\n    # Convert SMILES ‚Üí Mol\n    try:\n        mol = Chem.MolFromSmiles(smi, sanitize=False)\n        Chem.SanitizeMol(mol)\n    except:\n        failed += 1\n        continue\n    \n    if mol is None:\n        failed += 1\n        continue\n    \n    # ===== NODE FEATURES =====\n    atom_features = []\n    periodic_table = Chem.GetPeriodicTable()\n    \n    for atom in mol.GetAtoms():\n        atomic_num = atom.GetAtomicNum()\n        valence_electrons = periodic_table.GetNOuterElecs(atomic_num)\n        features = [\n            atomic_num,\n            atom.GetTotalValence(),\n            atom.GetTotalDegree(),\n            int(atom.GetIsAromatic()),\n            atom.GetFormalCharge(),\n            valence_electrons,           \n            atom.GetNumImplicitHs()      \n        ]\n        atom_features.append(features)\n    \n    x = torch.tensor(atom_features, dtype=torch.float)\n    \n    # ===== EDGE INDEX =====\n    edge_index = []\n    for bond in mol.GetBonds():\n        a = bond.GetBeginAtomIdx()\n        b = bond.GetEndAtomIdx()\n        edge_index.append([a, b])\n        edge_index.append([b, a])\n    \n    if len(edge_index) == 0:\n        edge_index = torch.zeros((2, 0), dtype=torch.long)\n    else:\n        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    \n    # ===== LABELS (GRAPH-LEVEL!) =====\n    y = torch.tensor(labels_clean[i], dtype=torch.float)        # (12,) ‚úÖ\n    m = torch.tensor(mask[i], dtype=torch.float)                # (12,) ‚úÖ\n    \n    # ===== T·∫†O DATA OBJECT =====\n    data = Data(\n        x=x,\n        edge_index=edge_index,\n        y=y,\n        mask=m,\n        mol_id=mol_ids_df[i],\n        num_nodes=x.shape[0]\n    )\n    \n    dataset.append(data)\n\nprint(f\"‚úÖ Dataset created: {len(dataset)} molecules\")\nprint(f\"‚ùå Failed to parse: {failed} SMILES\")\n\n# Ki·ªÉm tra sample\nsample = dataset[0]\nprint(f\"\\nüìå Sample Data Object:\")\nprint(f\"   x.shape: {sample.x.shape}\")\nprint(f\"   edge_index.shape: {sample.edge_index.shape}\")\nprint(f\"   y.shape: {sample.y.shape} ‚úÖ (ph·∫£i l√† (12,))\")\nprint(f\"   mask.shape: {sample.mask.shape} ‚úÖ (ph·∫£i l√† (12,))\")\nprint(f\"   num_nodes: {sample.num_nodes}\")\nprint(f\"   mol_id: {sample.mol_id}\")\n\n# ============================================================================\n# SPLIT DATASET\n# ============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"üìä SPLIT DATASET\")\nprint(f\"{'='*70}\")\n\nrandom.seed(42)\nrandom.shuffle(dataset)\n\ntotal = len(dataset)\nlen_train = int(0.8 * total)\nlen_val = int(0.1 * total)\nlen_test = total - len_train - len_val\n\nprint(f\"T·ªïng s·ªë m·∫´u: {total}\")\nprint(f\"Train: {len_train} (80%)\")\nprint(f\"Val:   {len_val} (10%)\")\nprint(f\"Test:  {len_test} (10%)\")\n\ntrain_dataset = dataset[:len_train]\nval_dataset = dataset[len_train : len_train + len_val]\ntest_dataset = dataset[len_train + len_val :]\n\n# ============================================================================\n# VERIFY\n# ============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"‚úÖ VERIFY DATASET\")\nprint(f\"{'='*70}\")\n\nprint(f\"\\nüìå Train Sample:\")\nprint(f\"   x.shape: {train_dataset[0].x.shape}\")\nprint(f\"   y.shape: {train_dataset[0].y.shape} ‚úÖ\")\n\nprint(f\"\\nüìå Val Sample:\")\nprint(f\"   x.shape: {val_dataset[0].x.shape}\")\nprint(f\"   y.shape: {val_dataset[0].y.shape} ‚úÖ\")\n\nprint(f\"\\nüìå Test Sample:\")\nprint(f\"   x.shape: {test_dataset[0].x.shape}\")\nprint(f\"   y.shape: {test_dataset[0].y.shape} ‚úÖ\")\n\n# ============================================================================\n# SAVE\n# ============================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(\"üíæ SAVE DATASET\")\nprint(f\"{'='*70}\")\n\ntorch.save(train_dataset, \"train.pt\")\ntorch.save(val_dataset, \"val.pt\")\ntorch.save(test_dataset, \"test.pt\")\n\nprint(\"‚úÖ train.pt saved\")\nprint(\"‚úÖ val.pt saved\")\nprint(\"‚úÖ test.pt saved\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"‚ú® DONE!\")\nprint(f\"{'='*70}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:01.157884Z","iopub.execute_input":"2025-11-10T14:59:01.158099Z","iopub.status.idle":"2025-11-10T14:59:06.435209Z","shell.execute_reply.started":"2025-11-10T14:59:01.158084Z","shell.execute_reply":"2025-11-10T14:59:06.434523Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìä LOAD DATA\n======================================================================\n   NR-AR  NR-AR-LBD  NR-AhR  NR-Aromatase  NR-ER  NR-ER-LBD  NR-PPAR-gamma  \\\n0    0.0        0.0     1.0           NaN    NaN        0.0            0.0   \n1    0.0        0.0     0.0           0.0    0.0        0.0            0.0   \n2    NaN        NaN     NaN           NaN    NaN        NaN            NaN   \n3    0.0        0.0     0.0           0.0    0.0        0.0            0.0   \n4    0.0        0.0     NaN           0.0    0.0        0.0            0.0   \n\n   SR-ARE  SR-ATAD5  SR-HSE  SR-MMP  SR-p53   mol_id  \\\n0     1.0       0.0     0.0     0.0     0.0  TOX3021   \n1     NaN       0.0     NaN     0.0     0.0  TOX3020   \n2     0.0       NaN     0.0     NaN     NaN  TOX3024   \n3     NaN       0.0     NaN     0.0     0.0  TOX3027   \n4     0.0       0.0     0.0     NaN     0.0  TOX3028   \n\n                                              smiles  \n0                       CCOc1ccc2nc(S(N)(=O)=O)sc2c1  \n1                          CCN1C(=O)NC(c2ccccc2)C1=O  \n2  CC[C@]1(O)CC[C@H]2[C@@H]3CCC4=CCCC[C@@H]4[C@H]...  \n3                    CCCN(CC)C(CC)C(=O)Nc1c(C)cccc1C  \n4                          CC(O)(P(=O)(O)O)P(=O)(O)O  \n\n‚úÖ SMILES: 8014 molecules\n‚úÖ Labels shape: (8014, 12)\n‚úÖ Labels_clean shape: (8014, 12)\n‚úÖ Mask shape: (8014, 12)\n\n======================================================================\nüèóÔ∏è  BUILD DATASET\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Dataset created: 8014 molecules\n‚ùå Failed to parse: 0 SMILES\n\nüìå Sample Data Object:\n   x.shape: torch.Size([16, 7])\n   edge_index.shape: torch.Size([2, 34])\n   y.shape: torch.Size([12]) ‚úÖ (ph·∫£i l√† (12,))\n   mask.shape: torch.Size([12]) ‚úÖ (ph·∫£i l√† (12,))\n   num_nodes: 16\n   mol_id: TOX3021\n\n======================================================================\nüìä SPLIT DATASET\n======================================================================\nT·ªïng s·ªë m·∫´u: 8014\nTrain: 6411 (80%)\nVal:   801 (10%)\nTest:  802 (10%)\n\n======================================================================\n‚úÖ VERIFY DATASET\n======================================================================\n\nüìå Train Sample:\n   x.shape: torch.Size([43, 7])\n   y.shape: torch.Size([12]) ‚úÖ\n\nüìå Val Sample:\n   x.shape: torch.Size([31, 7])\n   y.shape: torch.Size([12]) ‚úÖ\n\nüìå Test Sample:\n   x.shape: torch.Size([11, 7])\n   y.shape: torch.Size([12]) ‚úÖ\n\n======================================================================\nüíæ SAVE DATASET\n======================================================================\n‚úÖ train.pt saved\n‚úÖ val.pt saved\n‚úÖ test.pt saved\n\n======================================================================\n‚ú® DONE!\n======================================================================\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"train_dataset = torch.load(\"train.pt\", weights_only=False)\nval_dataset   = torch.load(\"val.pt\", weights_only=False)\ntest_dataset  = torch.load(\"test.pt\", weights_only=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:06.436093Z","iopub.execute_input":"2025-11-10T14:59:06.436381Z","iopub.status.idle":"2025-11-10T14:59:07.815746Z","shell.execute_reply.started":"2025-11-10T14:59:06.436352Z","shell.execute_reply":"2025-11-10T14:59:07.814930Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"data = train_dataset[0]\nprint(data)\nprint(\"Node feature matrix (x):\")\nprint(data.x)\nprint(\"x shape:\", data.x.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:07.816548Z","iopub.execute_input":"2025-11-10T14:59:07.816817Z","iopub.status.idle":"2025-11-10T14:59:07.824746Z","shell.execute_reply.started":"2025-11-10T14:59:07.816789Z","shell.execute_reply":"2025-11-10T14:59:07.824025Z"}},"outputs":[{"name":"stdout","text":"Data(x=[43, 7], edge_index=[2, 98], y=[12], mask=[12], mol_id='TOX197', num_nodes=43)\nNode feature matrix (x):\ntensor([[ 6.,  4.,  4.,  0.,  0.,  4.,  3.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  1.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  3.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  2.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  0.],\n        [ 6.,  4.,  3.,  0.,  0.,  4.,  0.],\n        [ 8.,  2.,  1.,  0.,  0.,  6.,  0.],\n        [ 7.,  3.,  3.,  0.,  0.,  5.,  0.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  2.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  2.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  2.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  0.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  0.],\n        [ 8.,  2.,  2.,  0.,  0.,  6.,  1.],\n        [ 8.,  2.,  2.,  0.,  0.,  6.,  0.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  0.],\n        [ 7.,  3.,  3.,  0.,  0.,  5.,  1.],\n        [ 6.,  4.,  3.,  0.,  0.,  4.,  0.],\n        [ 8.,  2.,  1.,  0.,  0.,  6.,  0.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  0.],\n        [ 6.,  4.,  3.,  0.,  0.,  4.,  1.],\n        [ 6.,  4.,  3.,  0.,  0.,  4.,  0.],\n        [ 6.,  4.,  3.,  1.,  0.,  4.,  0.],\n        [ 6.,  4.,  3.,  1.,  0.,  4.,  1.],\n        [ 6.,  4.,  3.,  1.,  0.,  4.,  1.],\n        [ 6.,  4.,  3.,  1.,  0.,  4.,  1.],\n        [ 6.,  4.,  3.,  1.,  0.,  4.,  0.],\n        [ 7.,  3.,  3.,  1.,  0.,  5.,  0.],\n        [ 6.,  4.,  3.,  1.,  0.,  4.,  0.],\n        [35.,  1.,  1.,  0.,  0.,  7.,  0.],\n        [ 6.,  4.,  3.,  1.,  0.,  4.,  0.],\n        [ 6.,  4.,  3.,  1.,  0.,  4.,  0.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  2.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  0.],\n        [ 7.,  3.,  3.,  0.,  0.,  5.,  0.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  3.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  2.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  1.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  3.],\n        [ 6.,  4.,  4.,  0.,  0.,  4.,  3.],\n        [ 6.,  4.,  3.,  0.,  0.,  4.,  0.],\n        [ 8.,  2.,  1.,  0.,  0.,  6.,  0.],\n        [ 7.,  3.,  3.,  0.,  0.,  5.,  0.]])\nx shape: torch.Size([43, 7])\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def custom_collate_fn(data_list):\n    \"\"\"\n    Custom collate function ƒë·ªÉ fix batch.y shape\n    \n    V·∫§Nƒê·ªÄ: \n    - PyG Batch n·ªëi batch.y th√†nh 1D: (7, 12) ‚Üí (84,)\n    \n    GI·∫¢I PH√ÅP:\n    - Reshape l·∫°i th√†nh (num_graphs, 12)\n    \"\"\"\n    \n    # T·∫°o batch b√¨nh th∆∞·ªùng\n    batch = Batch.from_data_list(data_list)\n    \n    # ===== FIX: RESHAPE batch.y =====\n    num_graphs = batch.batch.max().item() + 1\n    num_classes = 12\n    \n    # N·∫øu batch.y b·ªã flatten, reshape l·∫°i\n    if batch.y.dim() == 1 and batch.y.shape[0] == num_graphs * num_classes:\n        batch.y = batch.y.view(num_graphs, num_classes)\n    \n    # T∆∞∆°ng t·ª± cho mask\n    if hasattr(batch, 'mask') and batch.mask.dim() == 1 and batch.mask.shape[0] == num_graphs * num_classes:\n        batch.mask = batch.mask.view(num_graphs, num_classes)\n    \n    return batch\n\n# ============================================================================\n# PH·∫¶N 2: EXTRACT LABELS - ƒê∆†NGI·∫¢N (v√¨ batch.y ƒë√£ (num_graphs, 12))\n# ============================================================================\n\ndef extract_graph_labels(batch, num_classes, device):\n    \"\"\"\n    L·∫•y graph-level labels t·ª´ batch\n    \n    ƒê∆°n gi·∫£n v√¨ batch.y ƒë√£ ƒë∆∞·ª£c reshape th√†nh (num_graphs, 12)\n    \"\"\"\n    \n    num_graphs = batch.batch.max().item() + 1\n    \n    # ===== batch.y ƒë√£ l√† (num_graphs, 12) =====\n    if batch.y.shape[0] == num_graphs and batch.y.shape[1] == num_classes:\n        return batch.y\n    \n    # ===== Ho·∫∑c batch.y v·∫´n l√† 1D (fallback) =====\n    elif batch.y.dim() == 1 and batch.y.shape[0] == num_graphs * num_classes:\n        return batch.y.view(num_graphs, num_classes)\n    \n    else:\n        print(f\"‚ö†Ô∏è Unexpected batch.y shape: {batch.y.shape}\")\n        print(f\"   Expected: ({num_graphs}, {num_classes}) or ({num_graphs * num_classes},)\")\n        return torch.zeros(num_graphs, num_classes, device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:07.825496Z","iopub.execute_input":"2025-11-10T14:59:07.825804Z","iopub.status.idle":"2025-11-10T14:59:07.836722Z","shell.execute_reply.started":"2025-11-10T14:59:07.825776Z","shell.execute_reply":"2025-11-10T14:59:07.836056Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## GAT MODEL","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch_geometric.nn import GATConv, GATv2Conv, global_mean_pool, global_max_pool, global_add_pool\nimport numpy as np\nfrom sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, accuracy_score\n\n# ============================================================================\n# üöÄ IMPROVED GAT ARCHITECTURE - OPTIMIZED FOR YOUR DATA\n# ============================================================================\n\nclass GAT(nn.Module):\n    \"\"\"\n    State-of-the-art GAT cho multi-label classification v·ªõi imbalanced data:\n    \n    Features:\n    - GATv2Conv (better than GAT)\n    - Multiple pooling strategies (mean + max + sum)\n    - Batch Normalization\n    - Residual connections at graph level\n    - Dropout regularization\n    - Better weight initialization\n    \"\"\"\n    def __init__(self, input_dim=7, hidden_dim=128, num_heads=8,\n                 num_layers=3, embedding_dim=512, dropout=0.2, \n                 edge_dim=None, add_self_loops=True):\n        super().__init__()\n        \n        self.num_layers = num_layers\n        self.dropout = dropout\n        \n        # GAT Layers - using GATv2 (more expressive)\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n        \n        # Input layer\n        self.convs.append(\n            GATv2Conv(\n                input_dim, \n                hidden_dim, \n                heads=num_heads,\n                dropout=dropout,\n                concat=True,\n                add_self_loops=add_self_loops,\n                edge_dim=edge_dim\n            )\n        )\n        self.batch_norms.append(nn.BatchNorm1d(hidden_dim * num_heads))\n        \n        # Hidden layers\n        for i in range(num_layers - 2):\n            self.convs.append(\n                GATv2Conv(\n                    hidden_dim * num_heads,\n                    hidden_dim,\n                    heads=num_heads,\n                    dropout=dropout,\n                    concat=True,\n                    add_self_loops=add_self_loops,\n                    edge_dim=edge_dim\n                )\n            )\n            self.batch_norms.append(nn.BatchNorm1d(hidden_dim * num_heads))\n        \n        # Final layer - average attention heads\n        self.convs.append(\n            GATv2Conv(\n                hidden_dim * num_heads,\n                hidden_dim,\n                heads=num_heads,\n                dropout=dropout,\n                concat=False,  # Average instead of concat\n                add_self_loops=add_self_loops,\n                edge_dim=edge_dim\n            )\n        )\n        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n        \n        # Graph-level pooling projection\n        # Concat mean + max + sum = 3 * hidden_dim\n        pooling_dim = hidden_dim * 3\n        \n        # MLP for graph embedding\n        self.graph_mlp = nn.Sequential(\n            nn.Linear(pooling_dim, embedding_dim),\n            nn.BatchNorm1d(embedding_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            \n            nn.Linear(embedding_dim, embedding_dim),\n            nn.BatchNorm1d(embedding_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout)\n        )\n        \n        # Initialize weights\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        \"\"\"Xavier initialization for better gradient flow\"\"\"\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.BatchNorm1d):\n            nn.init.ones_(module.weight)\n            nn.init.zeros_(module.bias)\n    \n    def forward(self, x, edge_index, batch, edge_attr=None):\n        \"\"\"\n        Args:\n            x: Node features [num_nodes, input_dim]\n            edge_index: Edge indices [2, num_edges]\n            batch: Batch assignment [num_nodes]\n            edge_attr: Edge features [num_edges, edge_dim] (optional)\n        \"\"\"\n        \n        # Message passing\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index, edge_attr=edge_attr)\n            x = self.batch_norms[i](x)\n            x = torch.relu(x)\n            x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n        \n        # Multiple pooling strategies\n        x_mean = global_mean_pool(x, batch)\n        x_max = global_max_pool(x, batch)\n        x_sum = global_add_pool(x, batch)\n        \n        # Concatenate poolings\n        graph_repr = torch.cat([x_mean, x_max, x_sum], dim=-1)\n        \n        # Graph-level MLP\n        graph_embedding = self.graph_mlp(graph_repr)\n        \n        return graph_embedding\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:07.837395Z","iopub.execute_input":"2025-11-10T14:59:07.837625Z","iopub.status.idle":"2025-11-10T14:59:07.856534Z","shell.execute_reply.started":"2025-11-10T14:59:07.837605Z","shell.execute_reply":"2025-11-10T14:59:07.855979Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class MultiLabelClassifier(nn.Module):\n    \"\"\"\n    Advanced classifier v·ªõi temperature scaling v√† multiple hidden layers\n    \"\"\"\n    def __init__(self, embedding_dim=512, num_classes=12, \n                 hidden_dim=256, dropout=0.3, temperature=1.0):\n        super().__init__()\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(embedding_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            \n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.BatchNorm1d(hidden_dim // 2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            \n            nn.Linear(hidden_dim // 2, num_classes)\n        )\n        \n        # Temperature parameter for calibration\n        self.temperature = nn.Parameter(torch.ones(1) * temperature)\n        \n        # Initialize\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n    \n    def forward(self, x):\n        logits = self.classifier(x)\n        # Apply temperature scaling\n        scaled_logits = logits / torch.clamp(self.temperature, min=0.5, max=2.0)\n        return scaled_logits\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:07.858287Z","iopub.execute_input":"2025-11-10T14:59:07.858453Z","iopub.status.idle":"2025-11-10T14:59:07.874800Z","shell.execute_reply.started":"2025-11-10T14:59:07.858441Z","shell.execute_reply":"2025-11-10T14:59:07.874253Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# ============================================================================\n# üî• ASYMMETRIC LOSS - BEST FOR IMBALANCED MULTI-LABEL\n\nclass AsymmetricLossOptimized(nn.Module):\n    \"\"\"\n    Asymmetric Loss specifically designed for imbalanced multi-label\n   \n    \"\"\"\n    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, \n                 disable_torch_grad_focal_loss=True):\n        super().__init__()\n        self.gamma_neg = gamma_neg\n        self.gamma_pos = gamma_pos\n        self.clip = clip\n        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n    \n    def forward(self, x, y):\n        \"\"\"\n        Args:\n            x: logits [batch_size, num_classes]\n            y: targets [batch_size, num_classes] (0 or 1)\n        \"\"\"\n        # Probabilities\n        x_sigmoid = torch.sigmoid(x)\n        xs_pos = x_sigmoid\n        xs_neg = 1 - x_sigmoid\n\n        # Asymmetric Clipping\n        if self.clip is not None and self.clip > 0:\n            xs_neg = (xs_neg + self.clip).clamp(max=1)\n\n        # Basic CE\n        los_pos = y * torch.log(xs_pos.clamp(min=1e-8))\n        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=1e-8))\n        loss = los_pos + los_neg\n\n        # Asymmetric Focusing\n        if self.gamma_neg > 0 or self.gamma_pos > 0:\n            if self.disable_torch_grad_focal_loss:\n                torch.set_grad_enabled(False)\n            \n            pt0 = xs_pos * y\n            pt1 = xs_neg * (1 - y)\n            pt = pt0 + pt1\n            one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n            one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n            \n            if self.disable_torch_grad_focal_loss:\n                torch.set_grad_enabled(True)\n            \n            loss *= one_sided_w\n\n        return -loss.mean()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:07.875376Z","iopub.execute_input":"2025-11-10T14:59:07.875560Z","iopub.status.idle":"2025-11-10T14:59:07.889967Z","shell.execute_reply.started":"2025-11-10T14:59:07.875538Z","shell.execute_reply":"2025-11-10T14:59:07.889265Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def _find_optimal_thresholds(all_probs, all_labels, num_classes):\n    \"\"\"Improved threshold finding using multiple strategies\"\"\"\n    from sklearn.metrics import precision_recall_curve\n    \n    optimal_thresholds = []\n    per_class_f1 = []\n    \n    for i in range(num_classes):\n        probs = all_probs[:, i]\n        labels = all_labels[:, i]\n        \n        # Skip if no positive samples\n        if labels.sum() == 0:\n            optimal_thresholds.append(0.5)\n            per_class_f1.append(0.0)\n            continue\n            \n        # Strategy 1: Precision-Recall curve (more robust)\n        try:\n            precision, recall, thresholds = precision_recall_curve(labels, probs)\n            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n            best_idx = np.argmax(f1_scores[:-1])\n            thresh_pr = thresholds[best_idx]\n            f1_pr = f1_scores[best_idx]\n        except:\n            thresh_pr = 0.5\n            f1_pr = 0.0\n        \n        # Strategy 2: Adaptive grid search around PR threshold\n        mean_prob = probs.mean()\n        pos_ratio = labels.mean()\n        \n        # Smart search range based on class characteristics\n        if pos_ratio < 0.1:  # Rare class\n            search_min = max(0.01, thresh_pr - 0.1, mean_prob * 0.3)\n            search_max = min(0.3, thresh_pr + 0.1, mean_prob * 2.0)\n            n_points = 25\n        elif pos_ratio > 0.9:  # Common class\n            search_min = max(0.7, thresh_pr - 0.1, mean_prob * 0.8)\n            search_max = min(0.99, thresh_pr + 0.1, mean_prob * 1.2)\n            n_points = 25\n        else:\n            search_min = max(0.05, thresh_pr - 0.15, mean_prob * 0.5)\n            search_max = min(0.95, thresh_pr + 0.15, mean_prob * 1.5)\n            n_points = 20\n        \n        # Fine-tune with grid search\n        best_f1 = f1_pr\n        best_thresh = thresh_pr\n        \n        for thresh in np.linspace(search_min, search_max, n_points):\n            preds = (probs > thresh).astype(int)\n            f1 = f1_score(labels, preds, zero_division=0)\n            if f1 > best_f1:\n                best_f1 = f1\n                best_thresh = thresh\n        \n        # Ensure threshold is reasonable\n        best_thresh = np.clip(best_thresh, 0.05, 0.95)\n        \n        optimal_thresholds.append(best_thresh)\n        per_class_f1.append(best_f1)\n    \n    return optimal_thresholds, per_class_f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:07.890745Z","iopub.execute_input":"2025-11-10T14:59:07.891083Z","iopub.status.idle":"2025-11-10T14:59:07.909634Z","shell.execute_reply.started":"2025-11-10T14:59:07.891066Z","shell.execute_reply":"2025-11-10T14:59:07.909104Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def _ensure_shape_compatibility(y_true, logits, num_classes, device):\n    \"\"\"Ensure y_true has same shape as logits\"\"\"\n    if y_true.shape != logits.shape:\n        y_true = y_true.view(logits.shape[0], -1)\n        if y_true.shape[1] < num_classes:\n            pad = num_classes - y_true.shape[1]\n            y_true = torch.cat([\n                y_true,\n                torch.zeros(y_true.shape[0], pad, device=device)\n            ], dim=1)\n        elif y_true.shape[1] > num_classes:\n            y_true = y_true[:, :num_classes]\n    return y_true","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:00:20.167915Z","iopub.execute_input":"2025-11-10T15:00:20.168203Z","iopub.status.idle":"2025-11-10T15:00:20.173568Z","shell.execute_reply.started":"2025-11-10T15:00:20.168182Z","shell.execute_reply":"2025-11-10T15:00:20.172849Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def train_gat(model, classifier, train_loader, val_loader, device,\n                             num_classes=12, epochs=100, patience=20):\n    \"\"\"\n    Improved training function with better threshold optimization and error handling\n    \"\"\"\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üìä ANALYZING TRAINING DATA\")\n    print(\"=\"*80)\n    \n    # Calculate class statistics\n    all_labels = []\n    for batch in train_loader:\n        y_true = extract_graph_labels(batch, num_classes, device)\n        all_labels.append(y_true.cpu().numpy())\n    \n    all_labels = np.vstack(all_labels)\n    pos_counts = all_labels.sum(axis=0)\n    total_samples = len(all_labels)\n    \n    print(\"\\nüìà Class Distribution:\")\n    print(f\"{'Class':<8} {'Samples':<10} {'Ratio':<10} {'Imbalance'}\")\n    print(\"-\" * 80)\n    \n    for i in range(num_classes):\n        ratio = pos_counts[i] / total_samples\n        imbalance = (total_samples - pos_counts[i]) / (pos_counts[i] + 1e-5)\n        print(f\"Class {i:<2d} {int(pos_counts[i]):<10} {ratio*100:<9.2f}% {imbalance:<.2f}:1\")\n    \n    # Setup\n    model.to(device)\n    classifier.to(device)\n    \n    # Optimizer with different learning rates\n    optimizer = torch.optim.AdamW([\n        {'params': model.parameters(), 'lr': 0.001, 'weight_decay': 0.01},\n        {'params': classifier.parameters(), 'lr': 0.002, 'weight_decay': 0.005}\n    ])\n    \n    # Loss function\n    criterion = AsymmetricLossOptimized(\n        gamma_neg=4,\n        gamma_pos=1,\n        clip=0.05\n    )\n    \n    # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=8, min_lr=1e-6\n    )\n    \n    # Warmup scheduler\n    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n        optimizer, start_factor=0.1, total_iters=5\n    )\n    \n    best_f1 = 0.0\n    best_epoch = 0\n    patience_counter = 0\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üöÄ STARTING TRAINING\")\n    print(\"=\"*80)\n    \n    for epoch in range(epochs):\n        # ===== TRAINING =====\n        model.train()\n        classifier.train()\n        \n        total_loss = 0.0\n        train_probs_list = []\n        \n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            \n            # Forward\n            graph_embedding = model(batch.x, batch.edge_index, batch.batch)\n            logits = classifier(graph_embedding)\n            \n            # Get labels\n            y_true = extract_graph_labels(batch, num_classes, device)\n            \n            # Ensure shape compatibility\n            y_true = _ensure_shape_compatibility(y_true, logits, num_classes, device)\n            \n            # Loss & backward\n            loss = criterion(logits, y_true.float())\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(\n                list(model.parameters()) + list(classifier.parameters()),\n                max_norm=1.0\n            )\n            \n            optimizer.step()\n            total_loss += loss.item()\n            \n            # Monitor probabilities\n            with torch.no_grad():\n                train_probs_list.append(torch.sigmoid(logits).mean().item())\n        \n        # Warmup for first 5 epochs\n        if epoch < 5:\n            warmup_scheduler.step()\n        \n        avg_train_loss = total_loss / len(train_loader)\n        avg_train_prob = np.mean(train_probs_list)\n        \n        # ===== VALIDATION =====\n        model.eval()\n        classifier.eval()\n        \n        val_loss = 0.0\n        all_probs = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                batch = batch.to(device)\n                \n                graph_embedding = model(batch.x, batch.edge_index, batch.batch)\n                logits = classifier(graph_embedding)\n                probs = torch.sigmoid(logits)\n                \n                y_true = extract_graph_labels(batch, num_classes, device)\n                y_true = _ensure_shape_compatibility(y_true, logits, num_classes, device)\n                \n                loss = criterion(logits, y_true.float())\n                val_loss += loss.item()\n                \n                all_probs.append(probs.detach().cpu().numpy())\n                all_labels.append(y_true.detach().cpu().numpy())\n        \n        all_probs = np.vstack(all_probs)\n        all_labels = np.vstack(all_labels)\n        \n        # IMPROVED: Better threshold optimization\n        optimal_thresholds, per_class_f1 = _find_optimal_thresholds(\n            all_probs, all_labels, num_classes\n        )\n        \n        # Apply thresholds\n        all_preds = (all_probs > optimal_thresholds).astype(int)\n        \n        # Calculate metrics\n        micro_f1 = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n        micro_precision = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n        micro_recall = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n        hamming = hamming_loss(all_labels, all_preds)\n        \n        avg_val_loss = val_loss / len(val_loader)\n        mean_val_prob = all_probs.mean()\n        \n        # Print progress\n        if (epoch + 1) % 5 == 0 or epoch == 0:\n            print(f\"\\n{'='*80}\")\n            print(f\"Epoch {epoch+1}/{epochs}\")\n            print(f\"{'='*80}\")\n            print(f\"üìâ Loss:        Train={avg_train_loss:.4f} | Val={avg_val_loss:.4f}\")\n            print(f\"üìä Mean Prob:   Train={avg_train_prob:.4f} | Val={mean_val_prob:.4f}\")\n            print(f\"üéØ Micro F1:    {micro_f1:.4f} (P={micro_precision:.4f}, R={micro_recall:.4f})\")\n            print(f\"üéØ Macro F1:    {macro_f1:.4f}\")\n            print(f\"üéØ Hamming:     {hamming:.4f}\")\n            \n            # Show class statistics\n            low_f1_classes = np.argsort(per_class_f1)[:3]\n            print(f\"\\n‚ö†Ô∏è  Lowest F1 Classes:\")\n            for i in low_f1_classes:\n                pos_ratio = all_labels[:, i].mean()\n                print(f\"   Class {i}: F1={per_class_f1[i]:.3f}, \"\n                      f\"Thresh={optimal_thresholds[i]:.3f}, \"\n                      f\"PosRatio={pos_ratio:.3f}\")\n        \n        # LR scheduling\n        if epoch >= 5:\n            scheduler.step(micro_f1)\n            \n            # Print current learning rate\n            if (epoch + 1) % 10 == 0:\n                current_lr = optimizer.param_groups[0]['lr']\n                print(f\"   Current LR: {current_lr:.6f}\")\n        \n        # Save best model\n        if micro_f1 > best_f1:\n            best_f1 = micro_f1\n            best_epoch = epoch\n            patience_counter = 0\n            \n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'classifier_state_dict': classifier.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_f1': best_f1,\n                'optimal_thresholds': optimal_thresholds,\n                'mean_probability': mean_val_prob,\n                'per_class_f1': per_class_f1,\n                'all_probs_val': all_probs,  # For analysis\n                'all_labels_val': all_labels\n            }, 'best_model_improved.pt')\n            \n            print(f\"\\n‚úÖ Saved best model!\")\n            print(f\"   Micro F1: {best_f1:.4f}\")\n            print(f\"   Mean Prob: {mean_val_prob:.4f}\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"\\n‚ö†Ô∏è  Early stopping triggered at epoch {epoch+1}\")\n                print(f\"   Best F1 was {best_f1:.4f} at epoch {best_epoch+1}\")\n                break\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"‚ú® TRAINING COMPLETED\")\n    print(\"=\"*80)\n    print(f\"üèÜ Best Micro F1:     {best_f1:.4f}\")\n    print(f\"üìç Best Epoch:        {best_epoch+1}\")\n    print(f\"üìä Final Mean Prob:   {mean_val_prob:.4f}\")\n    print(\"=\"*80 + \"\\n\")\n    \n    # Load best model\n    checkpoint = torch.load('best_model_improved.pt', weights_only=False)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    classifier.load_state_dict(checkpoint['classifier_state_dict'])\n    \n    return model, classifier, checkpoint['optimal_thresholds']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:07.910397Z","iopub.execute_input":"2025-11-10T14:59:07.910633Z","iopub.status.idle":"2025-11-10T14:59:07.933170Z","shell.execute_reply.started":"2025-11-10T14:59:07.910613Z","shell.execute_reply":"2025-11-10T14:59:07.932599Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from sklearn.metrics import (\n    accuracy_score, f1_score, hamming_loss, jaccard_score,\n    precision_score, recall_score, classification_report\n)\ndef evaluate_multilabel(model, classifier, data_loader, device, \n                         num_classes=12, optimal_thresholds=None):\n    \"\"\"\n    Fixed evaluation function - no more RuntimeError!\n    \"\"\"\n    \n    model.eval()\n    classifier.eval()\n    \n    all_probs = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            batch = batch.to(device)\n            \n            graph_embedding = model(batch.x, batch.edge_index, batch.batch)\n            logits = classifier(graph_embedding)\n            probs = torch.sigmoid(logits)\n            \n            y_true = extract_graph_labels(batch, num_classes, device)\n            \n            if y_true.shape != logits.shape:\n                y_true = y_true.view(logits.shape[0], -1)\n                if y_true.shape[1] < num_classes:\n                    pad = num_classes - y_true.shape[1]\n                    y_true = torch.cat([\n                        y_true,\n                        torch.zeros(y_true.shape[0], pad, device=device)\n                    ], dim=1)\n                elif y_true.shape[1] > num_classes:\n                    y_true = y_true[:, :num_classes]\n            \n            # FIX: Add .detach()\n            all_probs.append(probs.detach().cpu().numpy())\n            all_labels.append(y_true.detach().cpu().numpy())\n    \n    all_probs = np.vstack(all_probs)\n    all_labels = np.vstack(all_labels)\n    \n    # Apply thresholds\n    if optimal_thresholds is None:\n        optimal_thresholds = [0.05] * num_classes\n    \n    all_preds = np.zeros_like(all_probs)\n    for i in range(num_classes):\n        all_preds[:, i] = (all_probs[:, i] > optimal_thresholds[i]).astype(int)\n    \n    # Metrics\n    from sklearn.metrics import accuracy_score\n    micro_f1 = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    micro_p = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n    micro_r = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n    hamming = hamming_loss(all_labels, all_preds)\n    subset_acc = accuracy_score(all_labels, all_preds)\n    \n    per_class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n    per_class_p = precision_score(all_labels, all_preds, average=None, zero_division=0)\n    per_class_r = recall_score(all_labels, all_preds, average=None, zero_division=0)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üìä FINAL EVALUATION RESULTS\")\n    print(\"=\"*80)\n    print(f\"\\nüéØ Overall Metrics:\")\n    print(f\"   Micro F1:        {micro_f1:.4f}\")\n    print(f\"   Macro F1:        {macro_f1:.4f}\")\n    print(f\"   Micro Precision: {micro_p:.4f}\")\n    print(f\"   Micro Recall:    {micro_r:.4f}\")\n    print(f\"   Hamming Loss:    {hamming:.4f}\")\n    print(f\"   Subset Accuracy: {subset_acc:.4f}\")\n    \n    print(f\"\\nüìà Per-Class Results:\")\n    print(f\"{'Class':<8} {'Threshold':<12} {'F1':<10} {'Precision':<12} {'Recall':<10} {'MeanProb':<10}\")\n    print(\"-\" * 80)\n    \n    class_names = [\n        \"NR-AR\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\",\n        \"NR-ER\", \"NR-ER-LBD\", \"NR-PPAR-Œ≥\",\n        \"SR-ARE\", \"SR-ATAD5\", \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n    ]\n    \n    for i in range(num_classes):\n        name = class_names[i] if i < len(class_names) else f\"Class{i}\"\n        print(f\"{name:<8} {optimal_thresholds[i]:<12.3f} \"\n              f\"{per_class_f1[i]:<10.3f} {per_class_p[i]:<12.3f} \"\n              f\"{per_class_r[i]:<10.3f} {all_probs[:, i].mean():<10.3f}\")\n    \n    print(\"=\"*80 + \"\\n\")\n    \n    return {\n        'micro_f1': micro_f1,\n        'macro_f1': macro_f1,\n        'predictions': all_preds,\n        'probabilities': all_probs,\n        'labels': all_labels\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:07.934045Z","iopub.execute_input":"2025-11-10T14:59:07.934307Z","iopub.status.idle":"2025-11-10T14:59:07.956035Z","shell.execute_reply.started":"2025-11-10T14:59:07.934291Z","shell.execute_reply":"2025-11-10T14:59:07.955334Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"# Embedding","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef extract_embeddings(model, classifier, data_loader, device, \n                       num_classes=12, save_path='embeddings.pt'):\n    \"\"\"\n    Extract graph embeddings t·ª´ GAT model v√† l∆∞u ra file .pt\n    \n    Args:\n        model: Trained GAT model\n        classifier: Trained classifier\n        data_loader: DataLoader (train/val/test)\n        device: torch device\n        num_classes: S·ªë l∆∞·ª£ng classes\n        save_path: ƒê∆∞·ªùng d·∫´n file output (.pt)\n    \n    Returns:\n        embeddings_dict: Dictionary ch·ª©a embeddings v√† metadata\n    \"\"\"\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üîç EXTRACTING GRAPH EMBEDDINGS\")\n    print(\"=\"*80)\n    \n    model.to(device)\n    classifier.to(device)\n    model.eval()\n    classifier.eval()\n    \n    all_embeddings = []\n    all_logits = []\n    all_probs = []\n    all_labels = []\n    all_graph_ids = []\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"Processing batches\")):\n            batch = batch.to(device)\n            \n            # Extract embeddings t·ª´ GAT\n            graph_embedding = model(batch.x, batch.edge_index, batch.batch)\n            \n            # Get logits v√† probabilities t·ª´ classifier\n            logits = classifier(graph_embedding)\n            probs = torch.sigmoid(logits)\n            \n            # Get labels\n            y_true = extract_graph_labels(batch, num_classes, device)\n            \n            # Ensure shape compatibility\n            if y_true.shape != logits.shape:\n                y_true = y_true.view(logits.shape[0], -1)\n                if y_true.shape[1] < num_classes:\n                    pad = num_classes - y_true.shape[1]\n                    y_true = torch.cat([\n                        y_true,\n                        torch.zeros(y_true.shape[0], pad, device=device)\n                    ], dim=1)\n                elif y_true.shape[1] > num_classes:\n                    y_true = y_true[:, :num_classes]\n            \n            # Store embeddings\n            all_embeddings.append(graph_embedding.cpu())\n            all_logits.append(logits.cpu())\n            all_probs.append(probs.cpu())\n            all_labels.append(y_true.cpu())\n            \n            # Store graph IDs n·∫øu c√≥\n            if hasattr(batch, 'graph_id'):\n                all_graph_ids.extend(batch.graph_id.cpu().tolist())\n            else:\n                # Generate sequential IDs\n                batch_size = graph_embedding.shape[0]\n                start_id = batch_idx * data_loader.batch_size\n                all_graph_ids.extend(range(start_id, start_id + batch_size))\n    \n    # Concatenate all batches\n    embeddings = torch.cat(all_embeddings, dim=0)\n    logits = torch.cat(all_logits, dim=0)\n    probs = torch.cat(all_probs, dim=0)\n    labels = torch.cat(all_labels, dim=0)\n    \n    print(f\"\\n‚úÖ Extracted {embeddings.shape[0]} graph embeddings\")\n    print(f\"   Embedding dimension: {embeddings.shape[1]}\")\n    print(f\"   Number of classes: {num_classes}\")\n    \n    # Create embeddings dictionary\n    embeddings_dict = {\n        'embeddings': embeddings,              # [N, embedding_dim]\n        'logits': logits,                      # [N, num_classes]\n        'probabilities': probs,                # [N, num_classes]\n        'labels': labels,                      # [N, num_classes]\n        'graph_ids': all_graph_ids,           # List of graph IDs\n        'embedding_dim': embeddings.shape[1],\n        'num_graphs': embeddings.shape[0],\n        'num_classes': num_classes,\n        'model_info': {\n            'model_type': model.__class__.__name__,\n            'classifier_type': classifier.__class__.__name__,\n        }\n    }\n    \n    # Save to file\n    torch.save(embeddings_dict, save_path)\n    print(f\"\\nüíæ Saved embeddings to: {save_path}\")\n    print(f\"   File size: {os.path.getsize(save_path) / (1024*1024):.2f} MB\")\n    \n    # Print statistics\n    print(f\"\\nüìä EMBEDDING STATISTICS:\")\n    print(f\"   Mean: {embeddings.mean().item():.4f}\")\n    print(f\"   Std:  {embeddings.std().item():.4f}\")\n    print(f\"   Min:  {embeddings.min().item():.4f}\")\n    print(f\"   Max:  {embeddings.max().item():.4f}\")\n    \n    print(f\"\\nüìä PROBABILITY STATISTICS:\")\n    print(f\"   Mean: {probs.mean().item():.4f}\")\n    print(f\"   Std:  {probs.std().item():.4f}\")\n    \n    print(\"=\"*80 + \"\\n\")\n    \n    return embeddings_dict\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:07.956881Z","iopub.execute_input":"2025-11-10T14:59:07.957062Z","iopub.status.idle":"2025-11-10T14:59:07.976819Z","shell.execute_reply.started":"2025-11-10T14:59:07.957041Z","shell.execute_reply":"2025-11-10T14:59:07.976050Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def extract_embeddings_by_split(model, classifier, train_loader, val_loader, \n                                test_loader, device, num_classes=12, \n                                output_dir='embeddings'):\n    \"\"\"\n    Extract embeddings cho t·∫•t c·∫£ c√°c splits (train/val/test) v√† l∆∞u ri√™ng\n    \n    Args:\n        model: Trained GAT model\n        classifier: Trained classifier\n        train_loader, val_loader, test_loader: DataLoaders\n        device: torch device\n        num_classes: S·ªë l∆∞·ª£ng classes\n        output_dir: Th∆∞ m·ª•c output\n    \n    Returns:\n        Dictionary ch·ª©a paths ƒë·∫øn c√°c embedding files\n    \"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n    \n    paths = {}\n    \n    # Extract train embeddings\n    if train_loader is not None:\n        print(\"\\nüîπ Extracting TRAIN embeddings...\")\n        train_path = os.path.join(output_dir, 'train_embeddings.pt')\n        extract_embeddings(model, classifier, train_loader, device, \n                          num_classes, train_path)\n        paths['train'] = train_path\n    \n    # Extract validation embeddings\n    if val_loader is not None:\n        print(\"\\nüîπ Extracting VALIDATION embeddings...\")\n        val_path = os.path.join(output_dir, 'val_embeddings.pt')\n        extract_embeddings(model, classifier, val_loader, device, \n                          num_classes, val_path)\n        paths['val'] = val_path\n    \n    # Extract test embeddings\n    if test_loader is not None:\n        print(\"\\nüîπ Extracting TEST embeddings...\")\n        test_path = os.path.join(output_dir, 'test_embeddings.pt')\n        extract_embeddings(model, classifier, test_loader, device, \n                          num_classes, test_path)\n        paths['test'] = test_path\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"‚ú® ALL EMBEDDINGS EXTRACTED SUCCESSFULLY\")\n    print(\"=\"*80)\n    for split, path in paths.items():\n        print(f\"   {split.upper()}: {path}\")\n    print(\"=\"*80 + \"\\n\")\n    \n    return paths\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:59:07.977618Z","iopub.execute_input":"2025-11-10T14:59:07.977834Z","iopub.status.idle":"2025-11-10T14:59:07.996500Z","shell.execute_reply.started":"2025-11-10T14:59:07.977812Z","shell.execute_reply":"2025-11-10T14:59:07.995745Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nprint(f\"‚úÖ Dataset: {len(dataset)} samples\")\nprint(f\"   Train: {len(train_dataset)} samples\")\nprint(f\"   Val:   {len(val_dataset)} samples\")\nprint(f\"   Test:  {len(test_dataset)} samples\\n\")\n\n# ============================================================================\n# SETUP DEVICE\n# ============================================================================\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üì± Device: {device}\\n\")\n\n# ============================================================================\n# INITIALIZE MODELS (FIX: X√≥a d√≤ng classifier = nn.Linear th·ª´a)\n# ============================================================================\nprint(\"üîß Initializing models...\")\n\n# 1. GAT Model\nmodel = GAT(\n    input_dim=7,          # s·ªë ƒë·∫∑c tr∆∞ng input c·ªßa node\n    hidden_dim=128,       # hidden dimension\n    num_heads=8,          # s·ªë attention heads\n    num_layers=3,         # s·ªë layer GAT\n    embedding_dim=512,    # k√≠ch th∆∞·ªõc embedding cu·ªëi\n    dropout=0.2\n)\n\n# 2. Classifier (FIX: Ch·ªâ kh·ªüi t·∫°o 1 l·∫ßn)\nclassifier = MultiLabelClassifier(\n    embedding_dim=512,\n    num_classes=12,\n    hidden_dim=256,\n    dropout=0.3,\n    temperature=1.0\n)\n\nprint(f\"‚úÖ GAT parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"‚úÖ Classifier parameters: {sum(p.numel() for p in classifier.parameters()):,}\\n\")\n\n# ============================================================================\n# TRAINING\n# ============================================================================\nprint(\"üöÄ Starting training...\\n\")\n\nmodel, classifier, optimal_thresholds = train_gat(\n    model=model,\n    classifier=classifier,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    device=device,\n    num_classes=12,\n    epochs=200,\n    patience=20\n)\n\n# ============================================================================\n# EVALUATION ON TEST SET\n# ============================================================================\nprint(\"\\nüìä Evaluating on test set...\")\n\ntest_results = evaluate_multilabel(\n    model=model,\n    classifier=classifier,\n    data_loader=test_loader,\n    device=device,\n    num_classes=12,\n    optimal_thresholds=optimal_thresholds\n)\n\n# Print final results\nprint(\"\\n\" + \"=\"*80)\nprint(\"üèÜ FINAL TEST RESULTS\")\nprint(\"=\"*80)\nprint(f\"Micro F1:       {test_results['micro_f1']:.4f}\")\nprint(f\"Macro F1:       {test_results['macro_f1']:.4f}\")\n\n\n\n\nprint(\"=\"*80 + \"\\n\")\n\n# ============================================================================\n# EXTRACT EMBEDDINGS (OPTIONAL)\n# ============================================================================\nprint(\"üîç Extracting embeddings from trained model...\")\n\nembeddings_paths = extract_embeddings_by_split(\n    model=model,\n    classifier=classifier,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    test_loader=test_loader,\n    device=device,\n    num_classes=12,\n    output_dir='embeddings'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:00:30.973782Z","iopub.execute_input":"2025-11-10T15:00:30.974059Z","iopub.status.idle":"2025-11-10T15:10:37.403460Z","shell.execute_reply.started":"2025-11-10T15:00:30.974037Z","shell.execute_reply":"2025-11-10T15:10:37.402666Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_48/588936053.py:1: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n/tmp/ipykernel_48/588936053.py:2: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n/tmp/ipykernel_48/588936053.py:3: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Dataset: 8014 samples\n   Train: 6411 samples\n   Val:   801 samples\n   Test:  802 samples\n\nüì± Device: cuda\n\nüîß Initializing models...\n‚úÖ GAT parameters: 4,686,208\n‚úÖ Classifier parameters: 166,541\n\nüöÄ Starting training...\n\n\n================================================================================\nüìä ANALYZING TRAINING DATA\n================================================================================\n\nüìà Class Distribution:\nClass    Samples    Ratio      Imbalance\n--------------------------------------------------------------------------------\nClass 0  255        3.98     % 24.14:1\nClass 1  188        2.93     % 33.10:1\nClass 2  643        10.03    % 8.97:1\nClass 3  223        3.48     % 27.75:1\nClass 4  644        10.05    % 8.95:1\nClass 5  281        4.38     % 21.81:1\nClass 6  154        2.40     % 40.63:1\nClass 7  774        12.07    % 7.28:1\nClass 8  228        3.56     % 27.12:1\nClass 9  298        4.65     % 20.51:1\nClass 10 755        11.78    % 7.49:1\nClass 11 345        5.38     % 17.58:1\n\n================================================================================\nüöÄ STARTING TRAINING\n================================================================================\n\n================================================================================\nEpoch 1/200\n================================================================================\nüìâ Loss:        Train=0.0886 | Val=0.0421\nüìä Mean Prob:   Train=0.4334 | Val=0.4450\nüéØ Micro F1:    0.2179 (P=0.1423, R=0.4655)\nüéØ Macro F1:    0.2248\nüéØ Hamming:     0.2068\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 8: F1=0.127, Thresh=0.498, PosRatio=0.022\n   Class 9: F1=0.135, Thresh=0.468, PosRatio=0.051\n   Class 5: F1=0.142, Thresh=0.452, PosRatio=0.049\n\n‚úÖ Saved best model!\n   Micro F1: 0.2179\n   Mean Prob: 0.4450\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.2292\n   Mean Prob: 0.4592\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.2560\n   Mean Prob: 0.4631\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.2880\n   Mean Prob: 0.4739\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 5/200\n================================================================================\nüìâ Loss:        Train=0.0418 | Val=0.0376\nüìä Mean Prob:   Train=0.4535 | Val=0.4643\nüéØ Micro F1:    0.3009 (P=0.2393, R=0.4050)\nüéØ Macro F1:    0.3027\nüéØ Hamming:     0.1165\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 6: F1=0.167, Thresh=0.511, PosRatio=0.022\n   Class 9: F1=0.168, Thresh=0.503, PosRatio=0.051\n   Class 3: F1=0.193, Thresh=0.481, PosRatio=0.051\n\n‚úÖ Saved best model!\n   Micro F1: 0.3009\n   Mean Prob: 0.4643\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.3119\n   Mean Prob: 0.4622\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.3287\n   Mean Prob: 0.4556\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 10/200\n================================================================================\nüìâ Loss:        Train=0.0375 | Val=0.0340\nüìä Mean Prob:   Train=0.4506 | Val=0.4304\nüéØ Micro F1:    0.3185 (P=0.2778, R=0.3731)\nüéØ Macro F1:    0.3201\nüéØ Hamming:     0.0988\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 6: F1=0.143, Thresh=0.486, PosRatio=0.022\n   Class 3: F1=0.222, Thresh=0.505, PosRatio=0.051\n   Class 11: F1=0.223, Thresh=0.506, PosRatio=0.060\n   Current LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.3511\n   Mean Prob: 0.4501\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 15/200\n================================================================================\nüìâ Loss:        Train=0.0367 | Val=0.0338\nüìä Mean Prob:   Train=0.4510 | Val=0.4418\nüéØ Micro F1:    0.3417 (P=0.2694, R=0.4672)\nüéØ Macro F1:    0.3544\nüéØ Hamming:     0.1114\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.189, Thresh=0.445, PosRatio=0.051\n   Class 6: F1=0.244, Thresh=0.498, PosRatio=0.022\n   Class 11: F1=0.277, Thresh=0.529, PosRatio=0.060\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.3779\n   Mean Prob: 0.4584\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.3827\n   Mean Prob: 0.4322\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 20/200\n================================================================================\nüìâ Loss:        Train=0.0356 | Val=0.0338\nüìä Mean Prob:   Train=0.4438 | Val=0.4471\nüéØ Micro F1:    0.3884 (P=0.3350, R=0.4622)\nüéØ Macro F1:    0.3961\nüéØ Hamming:     0.0901\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.221, Thresh=0.511, PosRatio=0.051\n   Class 6: F1=0.312, Thresh=0.547, PosRatio=0.022\n   Class 11: F1=0.312, Thresh=0.524, PosRatio=0.060\n   Current LR: 0.001000\n\n‚úÖ Saved best model!\n   Micro F1: 0.3884\n   Mean Prob: 0.4471\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.3906\n   Mean Prob: 0.4534\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 25/200\n================================================================================\nüìâ Loss:        Train=0.0352 | Val=0.0376\nüìä Mean Prob:   Train=0.4432 | Val=0.4652\nüéØ Micro F1:    0.3930 (P=0.3485, R=0.4504)\nüéØ Macro F1:    0.3975\nüéØ Hamming:     0.0861\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.235, Thresh=0.537, PosRatio=0.051\n   Class 9: F1=0.261, Thresh=0.547, PosRatio=0.051\n   Class 6: F1=0.291, Thresh=0.534, PosRatio=0.022\n\n‚úÖ Saved best model!\n   Micro F1: 0.3930\n   Mean Prob: 0.4652\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.4149\n   Mean Prob: 0.4483\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 30/200\n================================================================================\nüìâ Loss:        Train=0.0346 | Val=0.0307\nüìä Mean Prob:   Train=0.4415 | Val=0.4226\nüéØ Micro F1:    0.4030 (P=0.3669, R=0.4471)\nüéØ Macro F1:    0.3993\nüéØ Hamming:     0.0820\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 6: F1=0.219, Thresh=0.476, PosRatio=0.022\n   Class 3: F1=0.250, Thresh=0.517, PosRatio=0.051\n   Class 11: F1=0.307, Thresh=0.520, PosRatio=0.060\n   Current LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.4238\n   Mean Prob: 0.4428\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 35/200\n================================================================================\nüìâ Loss:        Train=0.0341 | Val=0.0367\nüìä Mean Prob:   Train=0.4367 | Val=0.4629\nüéØ Micro F1:    0.4009 (P=0.3704, R=0.4370)\nüéØ Macro F1:    0.3955\nüéØ Hamming:     0.0808\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.250, Thresh=0.525, PosRatio=0.051\n   Class 6: F1=0.261, Thresh=0.604, PosRatio=0.022\n   Class 8: F1=0.279, Thresh=0.589, PosRatio=0.022\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 40/200\n================================================================================\nüìâ Loss:        Train=0.0335 | Val=0.0362\nüìä Mean Prob:   Train=0.4342 | Val=0.4479\nüéØ Micro F1:    0.3944 (P=0.3369, R=0.4756)\nüéØ Macro F1:    0.4063\nüéØ Hamming:     0.0904\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.241, Thresh=0.494, PosRatio=0.051\n   Class 9: F1=0.289, Thresh=0.559, PosRatio=0.051\n   Class 6: F1=0.292, Thresh=0.529, PosRatio=0.022\n   Current LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.4287\n   Mean Prob: 0.4575\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 45/200\n================================================================================\nüìâ Loss:        Train=0.0321 | Val=0.0353\nüìä Mean Prob:   Train=0.4268 | Val=0.4477\nüéØ Micro F1:    0.4276 (P=0.3930, R=0.4689)\nüéØ Macro F1:    0.4463\nüéØ Hamming:     0.0777\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.263, Thresh=0.495, PosRatio=0.051\n   Class 9: F1=0.328, Thresh=0.530, PosRatio=0.051\n   Class 11: F1=0.350, Thresh=0.635, PosRatio=0.060\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.4365\n   Mean Prob: 0.4402\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.4563\n   Mean Prob: 0.4404\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 50/200\n================================================================================\nüìâ Loss:        Train=0.0316 | Val=0.0351\nüìä Mean Prob:   Train=0.4205 | Val=0.4460\nüéØ Micro F1:    0.4419 (P=0.4277, R=0.4571)\nüéØ Macro F1:    0.4500\nüéØ Hamming:     0.0715\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.262, Thresh=0.543, PosRatio=0.051\n   Class 9: F1=0.330, Thresh=0.552, PosRatio=0.051\n   Class 11: F1=0.341, Thresh=0.632, PosRatio=0.060\n   Current LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.4789\n   Mean Prob: 0.4339\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 55/200\n================================================================================\nüìâ Loss:        Train=0.0312 | Val=0.0318\nüìä Mean Prob:   Train=0.4177 | Val=0.4303\nüéØ Micro F1:    0.4583 (P=0.4112, R=0.5176)\nüéØ Macro F1:    0.4722\nüéØ Hamming:     0.0757\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 11: F1=0.327, Thresh=0.542, PosRatio=0.060\n   Class 9: F1=0.331, Thresh=0.509, PosRatio=0.051\n   Class 3: F1=0.349, Thresh=0.543, PosRatio=0.051\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 60/200\n================================================================================\nüìâ Loss:        Train=0.0306 | Val=0.0310\nüìä Mean Prob:   Train=0.4110 | Val=0.4168\nüéØ Micro F1:    0.4705 (P=0.4423, R=0.5025)\nüéØ Macro F1:    0.4743\nüéØ Hamming:     0.0700\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.310, Thresh=0.576, PosRatio=0.051\n   Class 11: F1=0.343, Thresh=0.550, PosRatio=0.060\n   Class 9: F1=0.366, Thresh=0.539, PosRatio=0.051\n   Current LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.4870\n   Mean Prob: 0.4197\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 65/200\n================================================================================\nüìâ Loss:        Train=0.0294 | Val=0.0321\nüìä Mean Prob:   Train=0.4068 | Val=0.4198\nüéØ Micro F1:    0.4801 (P=0.5056, R=0.4571)\nüéØ Macro F1:    0.4893\nüéØ Hamming:     0.0613\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 11: F1=0.343, Thresh=0.585, PosRatio=0.060\n   Class 3: F1=0.361, Thresh=0.595, PosRatio=0.051\n   Class 9: F1=0.391, Thresh=0.560, PosRatio=0.051\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 70/200\n================================================================================\nüìâ Loss:        Train=0.0292 | Val=0.0320\nüìä Mean Prob:   Train=0.4025 | Val=0.4208\nüéØ Micro F1:    0.4744 (P=0.4297, R=0.5294)\nüéØ Macro F1:    0.4908\nüéØ Hamming:     0.0726\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.337, Thresh=0.545, PosRatio=0.051\n   Class 11: F1=0.354, Thresh=0.566, PosRatio=0.060\n   Class 9: F1=0.380, Thresh=0.536, PosRatio=0.051\n   Current LR: 0.000250\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.4911\n   Mean Prob: 0.4103\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 75/200\n================================================================================\nüìâ Loss:        Train=0.0283 | Val=0.0320\nüìä Mean Prob:   Train=0.3944 | Val=0.4151\nüéØ Micro F1:    0.4742 (P=0.4558, R=0.4941)\nüéØ Macro F1:    0.4823\nüéØ Hamming:     0.0678\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.340, Thresh=0.542, PosRatio=0.051\n   Class 9: F1=0.348, Thresh=0.592, PosRatio=0.051\n   Class 11: F1=0.390, Thresh=0.597, PosRatio=0.060\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 80/200\n================================================================================\nüìâ Loss:        Train=0.0280 | Val=0.0301\nüìä Mean Prob:   Train=0.3917 | Val=0.3934\nüéØ Micro F1:    0.4869 (P=0.4622, R=0.5143)\nüéØ Macro F1:    0.4979\nüéØ Hamming:     0.0671\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.351, Thresh=0.536, PosRatio=0.051\n   Class 9: F1=0.387, Thresh=0.536, PosRatio=0.051\n   Class 11: F1=0.393, Thresh=0.582, PosRatio=0.060\n   Current LR: 0.000125\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.4926\n   Mean Prob: 0.3954\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 85/200\n================================================================================\nüìâ Loss:        Train=0.0276 | Val=0.0298\nüìä Mean Prob:   Train=0.3884 | Val=0.3953\nüéØ Micro F1:    0.4882 (P=0.4580, R=0.5227)\nüéØ Macro F1:    0.4971\nüéØ Hamming:     0.0678\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.374, Thresh=0.517, PosRatio=0.051\n   Class 9: F1=0.400, Thresh=0.549, PosRatio=0.051\n   Class 11: F1=0.403, Thresh=0.566, PosRatio=0.060\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 90/200\n================================================================================\nüìâ Loss:        Train=0.0275 | Val=0.0304\nüìä Mean Prob:   Train=0.3860 | Val=0.3949\nüéØ Micro F1:    0.4933 (P=0.4619, R=0.5294)\nüéØ Macro F1:    0.4986\nüéØ Hamming:     0.0673\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.365, Thresh=0.530, PosRatio=0.051\n   Class 11: F1=0.377, Thresh=0.565, PosRatio=0.060\n   Class 9: F1=0.395, Thresh=0.550, PosRatio=0.051\n   Current LR: 0.000125\n\n‚úÖ Saved best model!\n   Micro F1: 0.4933\n   Mean Prob: 0.3949\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 95/200\n================================================================================\nüìâ Loss:        Train=0.0272 | Val=0.0308\nüìä Mean Prob:   Train=0.3831 | Val=0.3936\nüéØ Micro F1:    0.4898 (P=0.4776, R=0.5025)\nüéØ Macro F1:    0.4931\nüéØ Hamming:     0.0648\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.330, Thresh=0.532, PosRatio=0.051\n   Class 9: F1=0.391, Thresh=0.552, PosRatio=0.051\n   Class 11: F1=0.406, Thresh=0.564, PosRatio=0.060\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 100/200\n================================================================================\nüìâ Loss:        Train=0.0268 | Val=0.0304\nüìä Mean Prob:   Train=0.3817 | Val=0.3906\nüéØ Micro F1:    0.4808 (P=0.4594, R=0.5042)\nüéØ Macro F1:    0.4931\nüéØ Hamming:     0.0674\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.337, Thresh=0.533, PosRatio=0.051\n   Class 9: F1=0.368, Thresh=0.551, PosRatio=0.051\n   Class 11: F1=0.373, Thresh=0.559, PosRatio=0.060\n   Current LR: 0.000063\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 105/200\n================================================================================\nüìâ Loss:        Train=0.0265 | Val=0.0305\nüìä Mean Prob:   Train=0.3776 | Val=0.3831\nüéØ Micro F1:    0.4804 (P=0.4574, R=0.5059)\nüéØ Macro F1:    0.4804\nüéØ Hamming:     0.0677\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.330, Thresh=0.533, PosRatio=0.051\n   Class 9: F1=0.348, Thresh=0.588, PosRatio=0.051\n   Class 11: F1=0.377, Thresh=0.557, PosRatio=0.060\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.4938\n   Mean Prob: 0.3833\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 110/200\n================================================================================\nüìâ Loss:        Train=0.0266 | Val=0.0307\nüìä Mean Prob:   Train=0.3757 | Val=0.3848\nüéØ Micro F1:    0.4848 (P=0.4532, R=0.5210)\nüéØ Macro F1:    0.4919\nüéØ Hamming:     0.0686\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.353, Thresh=0.529, PosRatio=0.051\n   Class 9: F1=0.391, Thresh=0.556, PosRatio=0.051\n   Class 11: F1=0.406, Thresh=0.560, PosRatio=0.060\n   Current LR: 0.000063\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 115/200\n================================================================================\nüìâ Loss:        Train=0.0266 | Val=0.0300\nüìä Mean Prob:   Train=0.3746 | Val=0.3841\nüéØ Micro F1:    0.4987 (P=0.4967, R=0.5008)\nüéØ Macro F1:    0.4956\nüéØ Hamming:     0.0623\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.328, Thresh=0.586, PosRatio=0.051\n   Class 9: F1=0.368, Thresh=0.538, PosRatio=0.051\n   Class 11: F1=0.417, Thresh=0.577, PosRatio=0.060\n\n‚úÖ Saved best model!\n   Micro F1: 0.4987\n   Mean Prob: 0.3841\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 120/200\n================================================================================\nüìâ Loss:        Train=0.0261 | Val=0.0299\nüìä Mean Prob:   Train=0.3715 | Val=0.3779\nüéØ Micro F1:    0.5000 (P=0.4835, R=0.5176)\nüéØ Macro F1:    0.5041\nüéØ Hamming:     0.0641\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.328, Thresh=0.586, PosRatio=0.051\n   Class 9: F1=0.384, Thresh=0.531, PosRatio=0.051\n   Class 11: F1=0.441, Thresh=0.554, PosRatio=0.060\n   Current LR: 0.000063\n\n‚úÖ Saved best model!\n   Micro F1: 0.5000\n   Mean Prob: 0.3779\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 125/200\n================================================================================\nüìâ Loss:        Train=0.0261 | Val=0.0304\nüìä Mean Prob:   Train=0.3698 | Val=0.3734\nüéØ Micro F1:    0.4919 (P=0.4743, R=0.5109)\nüéØ Macro F1:    0.4948\nüéØ Hamming:     0.0653\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.324, Thresh=0.575, PosRatio=0.051\n   Class 9: F1=0.382, Thresh=0.555, PosRatio=0.051\n   Class 11: F1=0.415, Thresh=0.562, PosRatio=0.060\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 130/200\n================================================================================\nüìâ Loss:        Train=0.0258 | Val=0.0306\nüìä Mean Prob:   Train=0.3690 | Val=0.3791\nüéØ Micro F1:    0.4874 (P=0.4457, R=0.5378)\nüéØ Macro F1:    0.4930\nüéØ Hamming:     0.0700\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.326, Thresh=0.506, PosRatio=0.051\n   Class 9: F1=0.358, Thresh=0.548, PosRatio=0.051\n   Class 11: F1=0.406, Thresh=0.567, PosRatio=0.060\n   Current LR: 0.000031\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Saved best model!\n   Micro F1: 0.5056\n   Mean Prob: 0.3706\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 135/200\n================================================================================\nüìâ Loss:        Train=0.0258 | Val=0.0307\nüìä Mean Prob:   Train=0.3679 | Val=0.3761\nüéØ Micro F1:    0.4863 (P=0.4559, R=0.5210)\nüéØ Macro F1:    0.4936\nüéØ Hamming:     0.0681\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.317, Thresh=0.513, PosRatio=0.051\n   Class 9: F1=0.400, Thresh=0.561, PosRatio=0.051\n   Class 11: F1=0.435, Thresh=0.577, PosRatio=0.060\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 140/200\n================================================================================\nüìâ Loss:        Train=0.0258 | Val=0.0303\nüìä Mean Prob:   Train=0.3672 | Val=0.3750\nüéØ Micro F1:    0.4947 (P=0.4781, R=0.5126)\nüéØ Macro F1:    0.4913\nüéØ Hamming:     0.0648\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.324, Thresh=0.590, PosRatio=0.051\n   Class 9: F1=0.383, Thresh=0.534, PosRatio=0.051\n   Class 11: F1=0.412, Thresh=0.557, PosRatio=0.060\n   Current LR: 0.000031\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 145/200\n================================================================================\nüìâ Loss:        Train=0.0256 | Val=0.0298\nüìä Mean Prob:   Train=0.3661 | Val=0.3721\nüéØ Micro F1:    0.4980 (P=0.4841, R=0.5126)\nüéØ Macro F1:    0.4944\nüéØ Hamming:     0.0640\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.333, Thresh=0.585, PosRatio=0.051\n   Class 9: F1=0.380, Thresh=0.518, PosRatio=0.051\n   Class 11: F1=0.443, Thresh=0.570, PosRatio=0.060\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 150/200\n================================================================================\nüìâ Loss:        Train=0.0254 | Val=0.0309\nüìä Mean Prob:   Train=0.3656 | Val=0.3769\nüéØ Micro F1:    0.4963 (P=0.4840, R=0.5092)\nüéØ Macro F1:    0.5024\nüéØ Hamming:     0.0640\n\n‚ö†Ô∏è  Lowest F1 Classes:\n   Class 3: F1=0.328, Thresh=0.522, PosRatio=0.051\n   Class 9: F1=0.400, Thresh=0.541, PosRatio=0.051\n   Class 11: F1=0.422, Thresh=0.577, PosRatio=0.060\n   Current LR: 0.000008\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n‚ö†Ô∏è  Early stopping triggered at epoch 152\n   Best F1 was 0.5056 at epoch 132\n\n================================================================================\n‚ú® TRAINING COMPLETED\n================================================================================\nüèÜ Best Micro F1:     0.5056\nüìç Best Epoch:        132\nüìä Final Mean Prob:   0.3840\n================================================================================\n\n\nüìä Evaluating on test set...\n\n================================================================================\nüìä FINAL EVALUATION RESULTS\n================================================================================\n\nüéØ Overall Metrics:\n   Micro F1:        0.4223\n   Macro F1:        0.3892\n   Micro Precision: 0.4038\n   Micro Recall:    0.4425\n   Hamming Loss:    0.0722\n   Subset Accuracy: 0.5835\n\nüìà Per-Class Results:\nClass    Threshold    F1         Precision    Recall     MeanProb  \n--------------------------------------------------------------------------------\nNR-AR    0.732        0.533      0.800        0.400      0.380     \nNR-AR-LBD 0.592        0.488      0.476        0.500      0.287     \nNR-AhR   0.606        0.518      0.440        0.629      0.408     \nNR-Aromatase 0.596        0.317      0.500        0.233      0.341     \nNR-ER    0.656        0.250      0.438        0.175      0.481     \nNR-ER-LBD 0.661        0.320      0.571        0.222      0.360     \nNR-PPAR-Œ≥ 0.585        0.222      0.179        0.294      0.294     \nSR-ARE   0.595        0.443      0.355        0.587      0.475     \nSR-ATAD5 0.663        0.216      0.250        0.190      0.286     \nSR-HSE   0.554        0.444      0.400        0.500      0.386     \nSR-MMP   0.652        0.554      0.551        0.557      0.410     \nSR-p53   0.566        0.365      0.261        0.605      0.365     \n================================================================================\n\n\n================================================================================\nüèÜ FINAL TEST RESULTS\n================================================================================\nMicro F1:       0.4223\nMacro F1:       0.3892\n================================================================================\n\nüîç Extracting embeddings from trained model...\n\nüîπ Extracting TRAIN embeddings...\n\n================================================================================\nüîç EXTRACTING GRAPH EMBEDDINGS\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [00:01<00:00, 147.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Extracted 6411 graph embeddings\n   Embedding dimension: 512\n   Number of classes: 12\n\nüíæ Saved embeddings to: embeddings/train_embeddings.pt\n   File size: 13.42 MB\n\nüìä EMBEDDING STATISTICS:\n   Mean: 0.2929\n   Std:  0.4950\n   Min:  0.0000\n   Max:  21.8663\n\nüìä PROBABILITY STATISTICS:\n   Mean: 0.3699\n   Std:  0.1538\n================================================================================\n\n\nüîπ Extracting VALIDATION embeddings...\n\n================================================================================\nüîç EXTRACTING GRAPH EMBEDDINGS\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:00<00:00, 149.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Extracted 801 graph embeddings\n   Embedding dimension: 512\n   Number of classes: 12\n\nüíæ Saved embeddings to: embeddings/val_embeddings.pt\n   File size: 1.68 MB\n\nüìä EMBEDDING STATISTICS:\n   Mean: 0.2857\n   Std:  0.4648\n   Min:  0.0000\n   Max:  9.7805\n\nüìä PROBABILITY STATISTICS:\n   Mean: 0.3706\n   Std:  0.1492\n================================================================================\n\n\nüîπ Extracting TEST embeddings...\n\n================================================================================\nüîç EXTRACTING GRAPH EMBEDDINGS\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:00<00:00, 153.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Extracted 802 graph embeddings\n   Embedding dimension: 512\n   Number of classes: 12\n\nüíæ Saved embeddings to: embeddings/test_embeddings.pt\n   File size: 1.68 MB\n\nüìä EMBEDDING STATISTICS:\n   Mean: 0.2921\n   Std:  0.4867\n   Min:  0.0000\n   Max:  11.2754\n\nüìä PROBABILITY STATISTICS:\n   Mean: 0.3727\n   Std:  0.1492\n================================================================================\n\n\n================================================================================\n‚ú® ALL EMBEDDINGS EXTRACTED SUCCESSFULLY\n================================================================================\n   TRAIN: embeddings/train_embeddings.pt\n   VAL: embeddings/val_embeddings.pt\n   TEST: embeddings/test_embeddings.pt\n================================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":33}]}