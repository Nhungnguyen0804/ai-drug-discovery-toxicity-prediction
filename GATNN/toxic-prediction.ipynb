{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13647884,"sourceType":"datasetVersion","datasetId":8676130},{"sourceId":13732017,"sourceType":"datasetVersion","datasetId":8736924},{"sourceId":13763876,"sourceType":"datasetVersion","datasetId":8759207}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:24:43.872578Z","iopub.execute_input":"2025-11-17T10:24:43.873298Z","iopub.status.idle":"2025-11-17T10:24:44.149067Z","shell.execute_reply.started":"2025-11-17T10:24:43.873273Z","shell.execute_reply":"2025-11-17T10:24:44.148447Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/stratified/train.pt\n/kaggle/input/stratified/val.pt\n/kaggle/input/stratified/test.pt\n/kaggle/input/random/train.pt\n/kaggle/input/random/val.pt\n/kaggle/input/random/test.pt\n/kaggle/input/tox21data/tox21.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch-geometric\n!pip install rdkit-pypi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:24:44.150223Z","iopub.execute_input":"2025-11-17T10:24:44.150979Z","iopub.status.idle":"2025-11-17T10:24:55.729576Z","shell.execute_reply.started":"2025-11-17T10:24:44.150959Z","shell.execute_reply":"2025-11-17T10:24:55.728822Z"}},"outputs":[{"name":"stdout","text":"Collecting torch-geometric\n  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.13.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.10.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.1.3)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.22.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.10.5)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nDownloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.7.0\nCollecting rdkit-pypi\n  Downloading rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi) (11.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit-pypi) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit-pypi) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit-pypi) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit-pypi) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit-pypi) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit-pypi) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit-pypi) (2024.2.0)\nDownloading rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rdkit-pypi\nSuccessfully installed rdkit-pypi-2022.9.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch_geometric.data import Data\nfrom torch.utils.data import DataLoader\nfrom torch_geometric.nn import GATConv, global_mean_pool, global_max_pool, global_add_pool\nfrom torch_geometric.explain import Explainer, GNNExplainer\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:24:55.730502Z","iopub.execute_input":"2025-11-17T10:24:55.730788Z","iopub.status.idle":"2025-11-17T10:25:05.302789Z","shell.execute_reply.started":"2025-11-17T10:24:55.730751Z","shell.execute_reply":"2025-11-17T10:25:05.301985Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from rdkit import Chem\n# import torch\n# from torch_geometric.data import Data\n# import random\n\n# # ============================================================================\n# # LOAD Dá»® LIá»†U\n# # ============================================================================\n\n# TOX21 = \"/kaggle/input/tox21data/tox21.csv\"\n# df = pd.read_csv(TOX21)\n\n# print('='*70)\n# print('ðŸ“Š LOAD DATA')\n# print('='*70)\n# print(df.head())\n\n# label_cols = [\n#     \"NR-AR\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\",\n#     \"NR-ER\", \"NR-ER-LBD\", \"NR-PPAR-gamma\",\n#     \"SR-ARE\", \"SR-ATAD5\", \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n# ]\n\n# smiles_df = df[\"smiles\"].tolist()\n# mol_ids_df = df[\"mol_id\"].tolist()\n# labels_df = df[label_cols].values  # (n_samples, 12)\n\n# print(f\"\\nâœ… SMILES: {len(smiles_df)} molecules\")\n# print(f\"âœ… Labels shape: {labels_df.shape}\")\n\n# # ============================================================================\n# # Táº O LABELS_CLEAN + MASK\n# # ============================================================================\n\n# labels_clean = np.nan_to_num(labels_df, nan=0.0)\n# mask = (~np.isnan(labels_df)).astype(np.float32)\n\n# print(f\"âœ… Labels_clean shape: {labels_clean.shape}\")\n# print(f\"âœ… Mask shape: {mask.shape}\")\n\n# # ============================================================================\n# # BUILD DATASET\n# # ============================================================================\n\n# dataset = []\n# failed = 0\n\n# print(f\"\\n{'='*70}\")\n# print(\"ðŸ—ï¸  BUILD DATASET\")\n# print(f\"{'='*70}\")\n\n# for i, smi in enumerate(smiles_df):\n#     # Convert SMILES â†’ Mol\n#     try:\n#         mol = Chem.MolFromSmiles(smi, sanitize=False)\n#         Chem.SanitizeMol(mol)\n#     except:\n#         failed += 1\n#         continue\n    \n#     if mol is None:\n#         failed += 1\n#         continue\n    \n#     # ===== NODE FEATURES =====\n#     atom_features = []\n#     periodic_table = Chem.GetPeriodicTable()\n    \n#     for atom in mol.GetAtoms():\n#         atomic_num = atom.GetAtomicNum()\n#         valence_electrons = periodic_table.GetNOuterElecs(atomic_num)\n#         features = [\n#             atomic_num,\n#             atom.GetTotalValence(),\n#             atom.GetTotalDegree(),\n#             int(atom.GetIsAromatic()),\n#             atom.GetFormalCharge(),\n#             valence_electrons,           \n#             atom.GetNumImplicitHs()      \n#         ]\n#         atom_features.append(features)\n    \n#     x = torch.tensor(atom_features, dtype=torch.float)\n    \n#     # ===== EDGE INDEX =====\n#     edge_index = []\n#     for bond in mol.GetBonds():\n#         a = bond.GetBeginAtomIdx()\n#         b = bond.GetEndAtomIdx()\n#         edge_index.append([a, b])\n#         edge_index.append([b, a])\n    \n#     if len(edge_index) == 0:\n#         edge_index = torch.zeros((2, 0), dtype=torch.long)\n#     else:\n#         edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    \n#     # ===== LABELS (GRAPH-LEVEL!) =====\n#     y = torch.tensor(labels_clean[i], dtype=torch.float)        # (12,) âœ…\n#     m = torch.tensor(mask[i], dtype=torch.float)                # (12,) âœ…\n    \n#     # ===== Táº O DATA OBJECT =====\n#     data = Data(\n#         x=x,\n#         edge_index=edge_index,\n#         y=y,\n#         mask=m,\n#         mol_id=mol_ids_df[i],\n#         num_nodes=x.shape[0]\n#     )\n    \n#     dataset.append(data)\n\n# print(f\"âœ… Dataset created: {len(dataset)} molecules\")\n# print(f\"âŒ Failed to parse: {failed} SMILES\")\n\n# # Kiá»ƒm tra sample\n# sample = dataset[0]\n# print(f\"\\nðŸ“Œ Sample Data Object:\")\n# print(f\"   x.shape: {sample.x.shape}\")\n# print(f\"   edge_index.shape: {sample.edge_index.shape}\")\n# print(f\"   y.shape: {sample.y.shape} âœ… (pháº£i lÃ  (12,))\")\n# print(f\"   mask.shape: {sample.mask.shape} âœ… (pháº£i lÃ  (12,))\")\n# print(f\"   num_nodes: {sample.num_nodes}\")\n# print(f\"   mol_id: {sample.mol_id}\")\n\n# # ============================================================================\n# # SPLIT DATASET\n# # ============================================================================\n\n# print(f\"\\n{'='*70}\")\n# print(\"ðŸ“Š SPLIT DATASET\")\n# print(f\"{'='*70}\")\n\n# random.seed(42)\n# random.shuffle(dataset)\n\n# total = len(dataset)\n# len_train = int(0.8 * total)\n# len_val = int(0.1 * total)\n# len_test = total - len_train - len_val\n\n# print(f\"Tá»•ng sá»‘ máº«u: {total}\")\n# print(f\"Train: {len_train} (80%)\")\n# print(f\"Val:   {len_val} (10%)\")\n# print(f\"Test:  {len_test} (10%)\")\n\n# train_dataset = dataset[:len_train]\n# val_dataset = dataset[len_train : len_train + len_val]\n# test_dataset = dataset[len_train + len_val :]\n\n# # ============================================================================\n# # VERIFY\n# # ============================================================================\n\n# print(f\"\\n{'='*70}\")\n# print(\"âœ… VERIFY DATASET\")\n# print(f\"{'='*70}\")\n\n# print(f\"\\nðŸ“Œ Train Sample:\")\n# print(f\"   x.shape: {train_dataset[0].x.shape}\")\n# print(f\"   y.shape: {train_dataset[0].y.shape} âœ…\")\n\n# print(f\"\\nðŸ“Œ Val Sample:\")\n# print(f\"   x.shape: {val_dataset[0].x.shape}\")\n# print(f\"   y.shape: {val_dataset[0].y.shape} âœ…\")\n\n# print(f\"\\nðŸ“Œ Test Sample:\")\n# print(f\"   x.shape: {test_dataset[0].x.shape}\")\n# print(f\"   y.shape: {test_dataset[0].y.shape} âœ…\")\n\n# # ============================================================================\n# # SAVE\n# # ============================================================================\n\n# print(f\"\\n{'='*70}\")\n# print(\"ðŸ’¾ SAVE DATASET\")\n# print(f\"{'='*70}\")\n\n# torch.save(train_dataset, \"train.pt\")\n# torch.save(val_dataset, \"val.pt\")\n# torch.save(test_dataset, \"test.pt\")\n\n# print(\"âœ… train.pt saved\")\n# print(\"âœ… val.pt saved\")\n# print(\"âœ… test.pt saved\")\n\n# print(f\"\\n{'='*70}\")\n# print(\"âœ¨ DONE!\")\n# print(f\"{'='*70}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:05.304337Z","iopub.execute_input":"2025-11-17T10:25:05.304714Z","iopub.status.idle":"2025-11-17T10:25:05.311157Z","shell.execute_reply.started":"2025-11-17T10:25:05.304695Z","shell.execute_reply":"2025-11-17T10:25:05.310393Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_dataset = torch.load(\"/kaggle/input/stratified/train.pt\", weights_only=False)\nval_dataset   = torch.load(\"/kaggle/input/stratified/val.pt\", weights_only=False)\ntest_dataset  = torch.load(\"/kaggle/input/stratified/test.pt\", weights_only=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:05.311847Z","iopub.execute_input":"2025-11-17T10:25:05.312019Z","iopub.status.idle":"2025-11-17T10:25:07.441249Z","shell.execute_reply.started":"2025-11-17T10:25:05.312005Z","shell.execute_reply":"2025-11-17T10:25:07.440412Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# def clean_dataset(ds, dim=9):\n#     clean = []\n#     for d in ds:\n#         if d.edge_attr is None:\n#             continue\n#         if d.edge_attr.size(1) != dim:\n#             continue\n#         clean.append(d)\n#     return clean\n\n# train_dataset = clean_dataset(train_dataset)\n# val_dataset   = clean_dataset(val_dataset)\n# test_dataset  = clean_dataset(test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:07.442104Z","iopub.execute_input":"2025-11-17T10:25:07.442379Z","iopub.status.idle":"2025-11-17T10:25:07.446091Z","shell.execute_reply.started":"2025-11-17T10:25:07.442352Z","shell.execute_reply":"2025-11-17T10:25:07.445326Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"data = train_dataset[0]\nprint(data)\nprint(\"Node feature matrix (x):\")\nprint(data.x)\nprint(\"x shape:\", data.x.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:07.446787Z","iopub.execute_input":"2025-11-17T10:25:07.446981Z","iopub.status.idle":"2025-11-17T10:25:07.503445Z","shell.execute_reply.started":"2025-11-17T10:25:07.446966Z","shell.execute_reply":"2025-11-17T10:25:07.502705Z"}},"outputs":[{"name":"stdout","text":"Data(x=[11, 41], edge_index=[2, 20], edge_attr=[20, 9], y=[12], mask=[12], mol_id='TOX9302', num_nodes=11)\nNode feature matrix (x):\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1201, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1201, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1201, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1201, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1201, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1201, 1.0000, 0.0000, 0.0000,\n         0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1201, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1600, 1.0000, 0.0000, 0.0000,\n         0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1201, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1201, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1201, 1.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 1.0000, 0.0000]])\nx shape: torch.Size([11, 41])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\ndef extract_graph_labels(batch, num_classes, device):\n    \"\"\"\n    Extract and reshape labels from batch\n    \"\"\"\n    y = batch.y\n    \n    # If already (batch_size, num_classes), return as is\n    if len(y.shape) == 2 and y.shape[1] == num_classes:\n        return y.to(device)\n    \n    # If (batch_size*num_classes,), reshape\n    if len(y.shape) == 1:\n        batch_size = y.shape[0] // num_classes\n        y = y.reshape(batch_size, num_classes)\n        return y.to(device)\n    \n    return y.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:07.504215Z","iopub.execute_input":"2025-11-17T10:25:07.504419Z","iopub.status.idle":"2025-11-17T10:25:07.511358Z","shell.execute_reply.started":"2025-11-17T10:25:07.504392Z","shell.execute_reply":"2025-11-17T10:25:07.510643Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## GAT MODEL","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch_geometric.nn import GATConv, GATv2Conv, global_mean_pool, global_max_pool, global_add_pool\nimport numpy as np\nfrom sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, accuracy_score\n\n\nclass GAT(nn.Module):\n    \"\"\"\n    State-of-the-art GAT cho multi-label classification vá»›i imbalanced data:\n    \n    Features:\n    - GATv2Conv (better than GAT)\n    - Multiple pooling strategies (mean + max + sum)\n    - Batch Normalization\n    - Residual connections at graph level\n    - Dropout regularization\n    - Better weight initialization\n    \"\"\"\n    def __init__(self, input_dim=41, hidden_dim=512, num_heads=8,\n                 num_layers=3, embedding_dim=1024, dropout=0.2, \n                 edge_dim=None, add_self_loops=True):\n        super().__init__()\n        \n        self.num_layers = num_layers\n        self.dropout = dropout\n        \n        # GAT Layers - using GATv2 (more expressive)\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n        \n        # Input layer\n        self.convs.append(\n            GATv2Conv(\n                input_dim, \n                hidden_dim, \n                heads=num_heads,\n                dropout=dropout,\n                concat=True,\n                add_self_loops=add_self_loops,\n                edge_dim=edge_dim\n            )\n        )\n        self.batch_norms.append(nn.BatchNorm1d(hidden_dim * num_heads))\n        \n        # Hidden layers\n        for i in range(num_layers - 2):\n            self.convs.append(\n                GATv2Conv(\n                    hidden_dim * num_heads,\n                    hidden_dim,\n                    heads=num_heads,\n                    dropout=dropout,\n                    concat=True,\n                    add_self_loops=add_self_loops,\n                    edge_dim=edge_dim\n                )\n            )\n            self.batch_norms.append(nn.BatchNorm1d(hidden_dim * num_heads))\n        \n        # Final layer - average attention heads\n        self.convs.append(\n            GATv2Conv(\n                hidden_dim * num_heads,\n                hidden_dim,\n                heads=num_heads,\n                dropout=dropout,\n                concat=False,  # Average instead of concat\n                add_self_loops=add_self_loops,\n                edge_dim=edge_dim\n            )\n        )\n        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n        \n        # Graph-level pooling projection\n        # Concat mean + max + sum = 3 * hidden_dim\n        pooling_dim = hidden_dim * 3\n        \n        # MLP for graph embedding\n        self.graph_mlp = nn.Sequential(\n            nn.Linear(pooling_dim, embedding_dim),\n            nn.BatchNorm1d(embedding_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            \n            nn.Linear(embedding_dim, embedding_dim),\n            nn.BatchNorm1d(embedding_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout)\n        )\n        \n        # Initialize weights\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        \"\"\"Xavier initialization for better gradient flow\"\"\"\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.BatchNorm1d):\n            nn.init.ones_(module.weight)\n            nn.init.zeros_(module.bias)\n    \n    def forward(self, x, edge_index, batch, edge_attr=None):\n        \"\"\"\n        Args:\n            x: Node features [num_nodes, input_dim]\n            edge_index: Edge indices [2, num_edges]\n            batch: Batch assignment [num_nodes]\n            edge_attr: Edge features [num_edges, edge_dim] (optional)\n        \"\"\"\n        \n        # Message passing\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index, edge_attr=edge_attr)\n            x = self.batch_norms[i](x)\n            x = torch.relu(x)\n            x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n        \n        # Multiple pooling strategies\n        x_mean = global_mean_pool(x, batch)\n        x_max = global_max_pool(x, batch)\n        x_sum = global_add_pool(x, batch)\n        \n        # Concatenate poolings\n        graph_repr = torch.cat([x_mean, x_max, x_sum], dim=-1)\n        \n        # Graph-level MLP\n        graph_embedding = self.graph_mlp(graph_repr)\n        \n        return graph_embedding\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:07.512108Z","iopub.execute_input":"2025-11-17T10:25:07.512288Z","iopub.status.idle":"2025-11-17T10:25:08.026324Z","shell.execute_reply.started":"2025-11-17T10:25:07.512275Z","shell.execute_reply":"2025-11-17T10:25:08.025783Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class MultiLabelClassifier(nn.Module):\n    \"\"\"\n    Advanced classifier vá»›i temperature scaling vÃ  multiple hidden layers\n    \"\"\"\n    def __init__(self, embedding_dim=512, num_classes=12, \n                 hidden_dim=256, dropout=0.3, temperature=1.0):\n        super().__init__()\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(embedding_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            \n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.BatchNorm1d(hidden_dim // 2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            \n            nn.Linear(hidden_dim // 2, num_classes)\n        )\n        \n        # Temperature parameter for calibration\n        self.temperature = nn.Parameter(torch.ones(1) * temperature)\n        \n        # Initialize\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n    \n    def forward(self, x):\n        logits = self.classifier(x)\n        # Apply temperature scaling\n        scaled_logits = logits / torch.clamp(self.temperature, min=0.5, max=2.0)\n        return scaled_logits\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:08.028231Z","iopub.execute_input":"2025-11-17T10:25:08.028666Z","iopub.status.idle":"2025-11-17T10:25:08.035191Z","shell.execute_reply.started":"2025-11-17T10:25:08.028646Z","shell.execute_reply":"2025-11-17T10:25:08.034438Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\n\nclass AsymmetricLossOptimized(nn.Module):\n    \"\"\"\n    Asymmetric Loss specifically designed for imbalanced multi-label\n   \n    \"\"\"\n    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, \n                 disable_torch_grad_focal_loss=True):\n        super().__init__()\n        self.gamma_neg = gamma_neg\n        self.gamma_pos = gamma_pos\n        self.clip = clip\n        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n    \n    def forward(self, x, y):\n        \"\"\"\n        Args:\n            x: logits [batch_size, num_classes]\n            y: targets [batch_size, num_classes] (0 or 1)\n        \"\"\"\n        # Probabilities\n        x_sigmoid = torch.sigmoid(x)\n        xs_pos = x_sigmoid\n        xs_neg = 1 - x_sigmoid\n\n        # Asymmetric Clipping\n        if self.clip is not None and self.clip > 0:\n            xs_neg = (xs_neg + self.clip).clamp(max=1)\n\n        # Basic CE\n        los_pos = y * torch.log(xs_pos.clamp(min=1e-8))\n        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=1e-8))\n        loss = los_pos + los_neg\n\n        # Asymmetric Focusing\n        if self.gamma_neg > 0 or self.gamma_pos > 0:\n            if self.disable_torch_grad_focal_loss:\n                torch.set_grad_enabled(False)\n            \n            pt0 = xs_pos * y\n            pt1 = xs_neg * (1 - y)\n            pt = pt0 + pt1\n            one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n            one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n            \n            if self.disable_torch_grad_focal_loss:\n                torch.set_grad_enabled(True)\n            \n            loss *= one_sided_w\n\n        return -loss.mean()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:08.035903Z","iopub.execute_input":"2025-11-17T10:25:08.036135Z","iopub.status.idle":"2025-11-17T10:25:08.047602Z","shell.execute_reply.started":"2025-11-17T10:25:08.036117Z","shell.execute_reply":"2025-11-17T10:25:08.046948Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def _find_optimal_thresholds(all_probs, all_labels, num_classes):\n    \"\"\"Improved threshold finding using multiple strategies\"\"\"\n    from sklearn.metrics import precision_recall_curve\n    \n    optimal_thresholds = []\n    per_class_f1 = []\n    \n    for i in range(num_classes):\n        probs = all_probs[:, i]\n        labels = all_labels[:, i]\n        \n        # Skip if no positive samples\n        if labels.sum() == 0:\n            optimal_thresholds.append(0.5)\n            per_class_f1.append(0.0)\n            continue\n            \n        # Strategy 1: Precision-Recall curve (more robust)\n        try:\n            precision, recall, thresholds = precision_recall_curve(labels, probs)\n            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n            best_idx = np.argmax(f1_scores[:-1])\n            thresh_pr = thresholds[best_idx]\n            f1_pr = f1_scores[best_idx]\n        except:\n            thresh_pr = 0.5\n            f1_pr = 0.0\n        \n        # Strategy 2: Adaptive grid search around PR threshold\n        mean_prob = probs.mean()\n        pos_ratio = labels.mean()\n        \n        # Smart search range based on class characteristics\n        if pos_ratio < 0.1:  # Rare class\n            search_min = max(0.01, thresh_pr - 0.1, mean_prob * 0.3)\n            search_max = min(0.3, thresh_pr + 0.1, mean_prob * 2.0)\n            n_points = 25\n        elif pos_ratio > 0.9:  # Common class\n            search_min = max(0.7, thresh_pr - 0.1, mean_prob * 0.8)\n            search_max = min(0.99, thresh_pr + 0.1, mean_prob * 1.2)\n            n_points = 25\n        else:\n            search_min = max(0.05, thresh_pr - 0.15, mean_prob * 0.5)\n            search_max = min(0.95, thresh_pr + 0.15, mean_prob * 1.5)\n            n_points = 20\n        \n        # Fine-tune with grid search\n        best_f1 = f1_pr\n        best_thresh = thresh_pr\n        \n        for thresh in np.linspace(search_min, search_max, n_points):\n            preds = (probs > thresh).astype(int)\n            f1 = f1_score(labels, preds, zero_division=0)\n            if f1 > best_f1:\n                best_f1 = f1\n                best_thresh = thresh\n        \n        # Ensure threshold is reasonable\n        best_thresh = np.clip(best_thresh, 0.05, 0.95)\n        \n        optimal_thresholds.append(best_thresh)\n        per_class_f1.append(best_f1)\n    \n    return optimal_thresholds, per_class_f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:08.048319Z","iopub.execute_input":"2025-11-17T10:25:08.048586Z","iopub.status.idle":"2025-11-17T10:25:08.062311Z","shell.execute_reply.started":"2025-11-17T10:25:08.048563Z","shell.execute_reply":"2025-11-17T10:25:08.061729Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def _ensure_shape_compatibility(y_true, logits, num_classes, device):\n    \"\"\"\n    Ensure y_true and logits have compatible shapes\n    \"\"\"\n    # y_true shape: (batch_size, num_classes)\n    # logits shape: (batch_size, num_classes)\n    \n    if y_true.shape != logits.shape:\n        # Try to reshape y_true to match logits\n        if y_true.numel() == logits.numel():\n            y_true = y_true.reshape(logits.shape)\n        else:\n            raise ValueError(\n                f\"Shape mismatch: y_true {y_true.shape} vs logits {logits.shape}\"\n            )\n    \n    return y_true","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:08.063114Z","iopub.execute_input":"2025-11-17T10:25:08.063341Z","iopub.status.idle":"2025-11-17T10:25:08.076781Z","shell.execute_reply.started":"2025-11-17T10:25:08.063322Z","shell.execute_reply":"2025-11-17T10:25:08.076037Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train_gat(model, classifier, train_loader, val_loader, device,\n              num_classes=12, epochs=100, patience=20):\n    \"\"\"\n    Improved training function with better threshold optimization and error handling\n    \"\"\"\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"ANALYZING TRAINING DATA\")\n    print(\"=\"*80)\n    \n    # Calculate class statistics\n    all_labels = []\n    for batch in train_loader:\n        batch = batch.to(device)\n        y_true = extract_graph_labels(batch, num_classes, device)\n        all_labels.append(y_true.cpu().numpy())\n    \n    all_labels = np.vstack(all_labels)\n    pos_counts = all_labels.sum(axis=0)\n    total_samples = len(all_labels)\n    \n    print(\"\\nClass Distribution:\")\n    print(f\"{'Class':<8} {'Samples':<10} {'Ratio':<10} {'Imbalance'}\")\n    print(\"-\" * 80)\n    \n    for i in range(num_classes):\n        ratio = pos_counts[i] / total_samples\n        imbalance = (total_samples - pos_counts[i]) / (pos_counts[i] + 1e-5)\n        print(f\"Class {i:<2d} {int(pos_counts[i]):<10} {ratio*100:<9.2f}% {imbalance:<.2f}:1\")\n    \n    # Setup\n    model.to(device)\n    classifier.to(device)\n    \n    # Optimizer with different learning rates\n    optimizer = torch.optim.AdamW([\n        {'params': model.parameters(), 'lr': 0.001, 'weight_decay': 0.01},\n        {'params': classifier.parameters(), 'lr': 0.002, 'weight_decay': 0.005}\n    ])\n    \n    # Loss function\n    criterion = AsymmetricLossOptimized(\n        gamma_neg=4,\n        gamma_pos=1,\n        clip=0.05\n    )\n    \n    # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=8, min_lr=1e-6\n    )\n    \n    # Warmup scheduler\n    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n        optimizer, start_factor=0.1, total_iters=5\n    )\n    \n    best_f1 = 0.0\n    best_epoch = 0\n    patience_counter = 0\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"STARTING TRAINING\")\n    print(\"=\"*80)\n    \n    for epoch in range(epochs):\n        # ===== TRAINING =====\n        model.train()\n        classifier.train()\n        \n        total_loss = 0.0\n        train_probs_list = []\n        \n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            \n            # Forward\n            graph_embedding = model(batch.x, batch.edge_index, batch.batch)\n            logits = classifier(graph_embedding)\n            \n            # Get labels\n            y_true = extract_graph_labels(batch, num_classes, device)\n            \n            # Ensure shape compatibility\n            y_true = _ensure_shape_compatibility(y_true, logits, num_classes, device)\n            \n            # Loss & backward\n            loss = criterion(logits, y_true.float())\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(\n                list(model.parameters()) + list(classifier.parameters()),\n                max_norm=1.0\n            )\n            \n            optimizer.step()\n            total_loss += loss.item()\n            \n            # Monitor probabilities\n            with torch.no_grad():\n                train_probs_list.append(torch.sigmoid(logits).mean().item())\n        \n        # Warmup for first 5 epochs\n        if epoch < 5:\n            warmup_scheduler.step()\n        \n        avg_train_loss = total_loss / len(train_loader)\n        avg_train_prob = np.mean(train_probs_list)\n        \n        # ===== VALIDATION =====\n        model.eval()\n        classifier.eval()\n        \n        val_loss = 0.0\n        all_probs = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                batch = batch.to(device)\n                \n                graph_embedding = model(batch.x, batch.edge_index, batch.batch)\n                logits = classifier(graph_embedding)\n                probs = torch.sigmoid(logits)\n                \n                y_true = extract_graph_labels(batch, num_classes, device)\n                y_true = _ensure_shape_compatibility(y_true, logits, num_classes, device)\n                \n                loss = criterion(logits, y_true.float())\n                val_loss += loss.item()\n                \n                all_probs.append(probs.detach().cpu().numpy())\n                all_labels.append(y_true.detach().cpu().numpy())\n        \n        all_probs = np.vstack(all_probs)\n        all_labels = np.vstack(all_labels)\n        \n        # Find optimal thresholds\n        optimal_thresholds, per_class_f1 = _find_optimal_thresholds(\n            all_probs, all_labels, num_classes\n        )\n        \n        # Apply thresholds\n        all_preds = (all_probs > optimal_thresholds).astype(int)\n        \n        # Calculate metrics\n        from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss\n        \n        micro_f1 = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n        micro_precision = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n        micro_recall = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n        hamming = hamming_loss(all_labels, all_preds)\n        \n        avg_val_loss = val_loss / len(val_loader)\n        mean_val_prob = all_probs.mean()\n        \n        # Print progress\n        if (epoch + 1) % 5 == 0 or epoch == 0:\n            print(f\"\\n{'='*80}\")\n            print(f\"Epoch {epoch+1}/{epochs}\")\n            print(f\"{'='*80}\")\n            print(f\"Loss:        Train={avg_train_loss:.4f} | Val={avg_val_loss:.4f}\")\n            print(f\" Mean Prob:   Train={avg_train_prob:.4f} | Val={mean_val_prob:.4f}\")\n            print(f\" Micro F1:    {micro_f1:.4f} (P={micro_precision:.4f}, R={micro_recall:.4f})\")\n            print(f\" Macro F1:    {macro_f1:.4f}\")\n            print(f\" Hamming:     {hamming:.4f}\")\n            \n            # Show class statistics\n            low_f1_classes = np.argsort(per_class_f1)[:3]\n            print(f\"\\n Lowest F1 Classes:\")\n            for i in low_f1_classes:\n                pos_ratio = all_labels[:, i].mean()\n                print(f\"   Class {i}: F1={per_class_f1[i]:.3f}, \"\n                      f\"Thresh={optimal_thresholds[i]:.3f}, \"\n                      f\"PosRatio={pos_ratio:.3f}\")\n        \n        # LR scheduling\n        if epoch >= 5:\n            scheduler.step(micro_f1)\n            \n            if (epoch + 1) % 10 == 0:\n                current_lr = optimizer.param_groups[0]['lr']\n                print(f\"   Current LR: {current_lr:.6f}\")\n        \n        # Save best model\n        if micro_f1 > best_f1:\n            best_f1 = micro_f1\n            best_epoch = epoch\n            patience_counter = 0\n            \n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'classifier_state_dict': classifier.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_f1': best_f1,\n                'optimal_thresholds': optimal_thresholds,\n                'mean_probability': mean_val_prob,\n                'per_class_f1': per_class_f1,\n                'all_probs_val': all_probs,\n                'all_labels_val': all_labels\n            }, 'best_model_improved.pt')\n            \n            print(f\"\\n Saved best model!\")\n            print(f\"   Micro F1: {best_f1:.4f}\")\n            print(f\"   Mean Prob: {mean_val_prob:.4f}\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"\\n  Early stopping triggered at epoch {epoch+1}\")\n                print(f\"   Best F1 was {best_f1:.4f} at epoch {best_epoch+1}\")\n                break\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\" TRAINING COMPLETED\")\n    print(\"=\"*80)\n    print(f\" Best Micro F1:     {best_f1:.4f}\")\n    print(f\" Best Epoch:        {best_epoch+1}\")\n    print(f\" Final Mean Prob:   {mean_val_prob:.4f}\")\n    print(\"=\"*80 + \"\\n\")\n    \n    # Load best model\n    checkpoint = torch.load('best_model_improved.pt', weights_only=False)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    classifier.load_state_dict(checkpoint['classifier_state_dict'])\n    \n    return model, classifier, checkpoint['optimal_thresholds']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:08.077517Z","iopub.execute_input":"2025-11-17T10:25:08.077762Z","iopub.status.idle":"2025-11-17T10:25:08.099172Z","shell.execute_reply.started":"2025-11-17T10:25:08.077737Z","shell.execute_reply":"2025-11-17T10:25:08.098556Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from sklearn.metrics import (\n    accuracy_score, f1_score, hamming_loss, jaccard_score,\n    precision_score, recall_score, classification_report\n)\ndef evaluate_multilabel(model, classifier, data_loader, device, \n                         num_classes=12, optimal_thresholds=None):\n    \"\"\"\n    Fixed evaluation function - no more RuntimeError!\n    \"\"\"\n    \n    model.eval()\n    classifier.eval()\n    \n    all_probs = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            batch = batch.to(device)\n            \n            graph_embedding = model(batch.x, batch.edge_index, batch.batch)\n            logits = classifier(graph_embedding)\n            probs = torch.sigmoid(logits)\n            \n            y_true = extract_graph_labels(batch, num_classes, device)\n            \n            if y_true.shape != logits.shape:\n                y_true = y_true.view(logits.shape[0], -1)\n                if y_true.shape[1] < num_classes:\n                    pad = num_classes - y_true.shape[1]\n                    y_true = torch.cat([\n                        y_true,\n                        torch.zeros(y_true.shape[0], pad, device=device)\n                    ], dim=1)\n                elif y_true.shape[1] > num_classes:\n                    y_true = y_true[:, :num_classes]\n            \n            # FIX: Add .detach()\n            all_probs.append(probs.detach().cpu().numpy())\n            all_labels.append(y_true.detach().cpu().numpy())\n    \n    all_probs = np.vstack(all_probs)\n    all_labels = np.vstack(all_labels)\n    \n    # Apply thresholds\n    if optimal_thresholds is None:\n        optimal_thresholds = [0.05] * num_classes\n    \n    all_preds = np.zeros_like(all_probs)\n    for i in range(num_classes):\n        all_preds[:, i] = (all_probs[:, i] > optimal_thresholds[i]).astype(int)\n    \n    # Metrics\n    from sklearn.metrics import accuracy_score\n    micro_f1 = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    micro_p = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n    micro_r = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n    hamming = hamming_loss(all_labels, all_preds)\n    subset_acc = accuracy_score(all_labels, all_preds)\n    \n    per_class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n    per_class_p = precision_score(all_labels, all_preds, average=None, zero_division=0)\n    per_class_r = recall_score(all_labels, all_preds, average=None, zero_division=0)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"FINAL EVALUATION RESULTS\")\n    print(\"=\"*80)\n    print(f\"\\n Overall Metrics:\")\n    print(f\"   Micro F1:        {micro_f1:.4f}\")\n    print(f\"   Macro F1:        {macro_f1:.4f}\")\n    print(f\"   Micro Precision: {micro_p:.4f}\")\n    print(f\"   Micro Recall:    {micro_r:.4f}\")\n    print(f\"   Hamming Loss:    {hamming:.4f}\")\n    print(f\"   Subset Accuracy: {subset_acc:.4f}\")\n    \n    print(f\"\\n Per-Class Results:\")\n    print(f\"{'Class':<8} {'Threshold':<12} {'F1':<10} {'Precision':<12} {'Recall':<10} {'MeanProb':<10}\")\n    print(\"-\" * 80)\n    \n    class_names = [\n        \"NR-AR\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\",\n        \"NR-ER\", \"NR-ER-LBD\", \"NR-PPAR-Î³\",\n        \"SR-ARE\", \"SR-ATAD5\", \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n    ]\n    \n    for i in range(num_classes):\n        name = class_names[i] if i < len(class_names) else f\"Class{i}\"\n        print(f\"{name:<8} {optimal_thresholds[i]:<12.3f} \"\n              f\"{per_class_f1[i]:<10.3f} {per_class_p[i]:<12.3f} \"\n              f\"{per_class_r[i]:<10.3f} {all_probs[:, i].mean():<10.3f}\")\n    \n    print(\"=\"*80 + \"\\n\")\n    \n    return {\n        'micro_f1': micro_f1,\n        'macro_f1': macro_f1,\n        'predictions': all_preds,\n        'probabilities': all_probs,\n        'labels': all_labels\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:08.099964Z","iopub.execute_input":"2025-11-17T10:25:08.100743Z","iopub.status.idle":"2025-11-17T10:25:08.114902Z","shell.execute_reply.started":"2025-11-17T10:25:08.100725Z","shell.execute_reply":"2025-11-17T10:25:08.114328Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Embedding","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nimport os\n\ndef extract_embeddings(model, classifier, data_loader, device, \n                       num_classes=12, save_path='embeddings.pt'):\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXTRACTING GRAPH EMBEDDINGS (with mol_id)\")\n    print(\"=\"*80)\n\n    model.to(device).eval()\n    classifier.to(device).eval()\n\n    all_embeddings = []\n    all_logits = []\n    all_probs = []\n    all_labels = []\n    all_mol_ids = []   # <-- lÆ°u mol_id tháº­t\n\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(tqdm(data_loader)):\n\n            batch = batch.to(device)\n\n            # ==== (1) Kiá»ƒm tra mol_id cÃ³ tá»“n táº¡i khÃ´ng ====\n            if not hasattr(batch, \"mol_id\"):\n                raise ValueError(\n                    \"batch khÃ´ng cÃ³ mol_id! \"\n                    \"HÃ£y cháº¯c cháº¯n báº¡n cÃ³ Data(mol_id=...) trong dataset.\"\n                )\n\n            # ==== (2) Xá»­ lÃ½ mol_id sau batching ====\n            # PyG chuyá»ƒn thÃ nh list khi batch\n            if isinstance(batch.mol_id, list):\n                batch_mol_ids = batch.mol_id\n            else:\n                # náº¿u tensor â†’ convert sang list\n                if torch.is_tensor(batch.mol_id):\n                    batch_mol_ids = batch.mol_id.cpu().tolist()\n                else:\n                    # string/dÆ¡n item\n                    batch_mol_ids = [batch.mol_id]\n\n            all_mol_ids.extend(batch_mol_ids)\n\n            # ==== (3) Láº¥y graph embedding ====\n            graph_embedding = model(batch.x, batch.edge_index, batch.batch)\n\n            # ==== (4) Classifier ====\n            logits = classifier(graph_embedding)\n            probs = torch.sigmoid(logits)\n\n            # ==== (5) Labels ====\n            y_true = extract_graph_labels(batch, num_classes, device)\n\n            # reshape náº¿u mismatch\n            if y_true.shape != logits.shape:\n                y_true = y_true.view(logits.shape[0], -1)\n\n            # ==== (6) LÆ°u vÃ o list ====\n            all_embeddings.append(graph_embedding.cpu())\n            all_logits.append(logits.cpu())\n            all_probs.append(probs.cpu())\n            all_labels.append(y_true.cpu())\n\n    # ==== CONCAT ====\n    embeddings = torch.cat(all_embeddings, dim=0)\n    logits      = torch.cat(all_logits, dim=0)\n    probs       = torch.cat(all_probs, dim=0)\n    labels      = torch.cat(all_labels, dim=0)\n\n    print(f\"\\n Extracted {embeddings.shape[0]} graph embeddings\")\n    print(f\" Embedding dim: {embeddings.shape[1]}\")\n\n    # ==== SAVE ====\n    output = {\n        \"embeddings\": embeddings,\n        \"logits\": logits,\n        \"probabilities\": probs,\n        \"labels\": labels,\n        \"mol_ids\": all_mol_ids,     # <-- lÆ°u mol_id tháº­t\n        \"num_graphs\": embeddings.shape[0],\n        \"embedding_dim\": embeddings.shape[1],\n        \"num_classes\": num_classes,\n    }\n\n    torch.save(output, save_path)\n\n    print(f\"\\n Saved to {save_path}\")\n    print(f\" File size: {os.path.getsize(save_path)/(1024*1024):.2f} MB\")\n    print(\"=\"*80)\n\n    return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:08.115685Z","iopub.execute_input":"2025-11-17T10:25:08.116237Z","iopub.status.idle":"2025-11-17T10:25:08.128981Z","shell.execute_reply.started":"2025-11-17T10:25:08.116216Z","shell.execute_reply":"2025-11-17T10:25:08.128328Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def extract_embeddings_by_split(model, classifier, train_loader, val_loader, \n                                test_loader, device, num_classes=12, \n                                output_dir='embeddings'):\n    \"\"\"\n    Extract embeddings cho táº¥t cáº£ cÃ¡c splits (train/val/test) vÃ  lÆ°u riÃªng\n    \n    Args:\n        model: Trained GAT model\n        classifier: Trained classifier\n        train_loader, val_loader, test_loader: DataLoaders\n        device: torch device\n        num_classes: Sá»‘ lÆ°á»£ng classes\n        output_dir: ThÆ° má»¥c output\n    \n    Returns:\n        Dictionary chá»©a paths Ä‘áº¿n cÃ¡c embedding files\n    \"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n    \n    paths = {}\n    \n    # Extract train embeddings\n    if train_loader is not None:\n        print(\"\\nðŸ”¹ Extracting TRAIN embeddings...\")\n        train_path = os.path.join(output_dir, 'train_embeddings.pt')\n        extract_embeddings(model, classifier, train_loader, device, \n                          num_classes, train_path)\n        paths['train'] = train_path\n    \n    # Extract validation embeddings\n    if val_loader is not None:\n        print(\"\\nðŸ”¹ Extracting VALIDATION embeddings...\")\n        val_path = os.path.join(output_dir, 'val_embeddings.pt')\n        extract_embeddings(model, classifier, val_loader, device, \n                          num_classes, val_path)\n        paths['val'] = val_path\n    \n    # Extract test embeddings\n    if test_loader is not None:\n        print(\"\\nðŸ”¹ Extracting TEST embeddings...\")\n        test_path = os.path.join(output_dir, 'test_embeddings.pt')\n        extract_embeddings(model, classifier, test_loader, device, \n                          num_classes, test_path)\n        paths['test'] = test_path\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\" ALL EMBEDDINGS EXTRACTED SUCCESSFULLY\")\n    print(\"=\"*80)\n    for split, path in paths.items():\n        print(f\"   {split.upper()}: {path}\")\n    print(\"=\"*80 + \"\\n\")\n    \n    return paths\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:25:08.129635Z","iopub.execute_input":"2025-11-17T10:25:08.129839Z","iopub.status.idle":"2025-11-17T10:25:08.143307Z","shell.execute_reply.started":"2025-11-17T10:25:08.129819Z","shell.execute_reply":"2025-11-17T10:25:08.142736Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# ============================================================================\n# INSPECT ALL ATTRIBUTES IN DATASET\n# ============================================================================\n\nprint(\"=\"*80)\nprint(\"INSPECTING DATA ATTRIBUTES\")\nprint(\"=\"*80)\n\nsample = train_dataset[0]\nprint(f\"\\nFirst sample attributes and types:\")\n\nfor key in sample.keys():\n    value = sample[key]\n    if isinstance(value, torch.Tensor):\n        print(f\"  {key}: Tensor {value.shape} dtype={value.dtype}\")\n    elif isinstance(value, (int, float, str)):\n        print(f\"  {key}: {type(value).__name__} = {value}\")\n    else:\n        print(f\"  {key}: {type(value).__name__}\")\n\n# Check a few more samples to see if attributes vary\nprint(f\"\\nChecking attribute consistency across 5 samples:\")\nfor i in range(min(5, len(train_dataset))):\n    data = train_dataset[i]\n    print(f\"\\nSample {i}:\")\n    for key in data.keys():\n        value = data[key]\n        if isinstance(value, torch.Tensor):\n            print(f\"  {key}: {value.shape}\")\n        else:\n            print(f\"  {key}: {value}\")\n\n# ============================================================================\n# CLEAN DATASET - REMOVE/STANDARDIZE PROBLEMATIC ATTRIBUTES\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CLEANING DATASET\")\nprint(\"=\"*80)\n\ndef clean_dataset(dataset, name):\n    \"\"\"Remove problematic attributes from dataset\"\"\"\n    print(f\"\\nCleaning {name}...\")\n    \n    for i, data in enumerate(dataset):\n        # Keep only essential attributes\n        keys_to_keep = ['x', 'edge_index', 'y', 'mask', 'mol_id']\n        keys_to_remove = [k for k in data.keys() if k not in keys_to_keep]\n        \n        for key in keys_to_remove:\n            delattr(data, key)\n    \n    print(f\"   Kept only: {keys_to_keep}\")\n    print(f\"   Removed: {keys_to_remove}\")\n\nclean_dataset(train_dataset, \"train_dataset\")\nclean_dataset(val_dataset, \"val_dataset\")\nclean_dataset(test_dataset, \"test_dataset\")\n\nprint(\"\\n Dataset cleaned!\")\n\n# Verify\nsample = train_dataset[0]\nprint(f\"\\nAfter cleaning, attributes: {list(sample.keys())}\")\n\n# ============================================================================\n# SIMPLE COLLATE FUNCTION\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CREATING SIMPLE COLLATE FUNCTION\")\nprint(\"=\"*80)\n\nfrom torch.utils.data import DataLoader\nfrom torch_geometric.data import Batch\nimport torch\n\ndef simple_collate_fn(data_list):\n    \"\"\"\n    Simple collate - only concatenate x, edge_index\n    Reshape y, mask manually\n    \"\"\"\n    batch = Batch.from_data_list(data_list)\n    \n    batch_size = len(data_list)\n    num_classes = 12\n    \n    # Reshape y: (batch_size * 12,) â†’ (batch_size, 12)\n    if batch.y is not None:\n        batch.y = batch.y.reshape(batch_size, num_classes)\n    \n    # Reshape mask: (batch_size * 12,) â†’ (batch_size, 12)\n    if batch.mask is not None:\n        batch.mask = batch.mask.reshape(batch_size, num_classes)\n    \n    return batch\n\n\n# ============================================================================\n# CREATE DATALOADERS\n# ============================================================================\n\nprint(\"\\nCreating dataloaders...\\n\")\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    shuffle=True,\n    collate_fn=simple_collate_fn\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=32,\n    shuffle=False,\n    collate_fn=simple_collate_fn\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=32,\n    shuffle=False,\n    collate_fn=simple_collate_fn\n)\n\nprint(f\" train_loader: {len(train_loader)} batches\")\nprint(f\" val_loader: {len(val_loader)} batches\")\nprint(f\" test_loader: {len(test_loader)} batches\")\n\n# ============================================================================\n# VERIFY DATALOADERS\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"VERIFYING DATALOADERS\")\nprint(\"=\"*80)\n\nsuccess = True\ntry:\n    for batch_idx, batch in enumerate(train_loader):\n        print(f\"\\nBatch {batch_idx}:\")\n        print(f\"  x shape:         {batch.x.shape}\")\n        print(f\"  edge_index shape: {batch.edge_index.shape}\")\n        print(f\"  y shape:         {batch.y.shape}\")\n        print(f\"  mask shape:      {batch.mask.shape}\")\n        \n        # Verify shapes\n        assert batch.y.shape == (32, 12), f\"y shape wrong: {batch.y.shape}\"\n        assert batch.mask.shape == (32, 12), f\"mask shape wrong: {batch.mask.shape}\"\n        \n        if batch_idx >= 2:\n            break\n    \n    print(f\"\\n\" + \"=\"*80)\n    print(f\" SUCCESS! DATALOADERS READY FOR TRAINING!\")\n    print(f\"=\"*80 + \"\\n\")\n    \nexcept Exception as e:\n    print(f\"\\n ERROR: {e}\")\n    import traceback\n    traceback.print_exc()\n    success = False\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:31:33.914014Z","iopub.execute_input":"2025-11-17T10:31:33.914692Z","iopub.status.idle":"2025-11-17T10:31:33.964350Z","shell.execute_reply.started":"2025-11-17T10:31:33.914667Z","shell.execute_reply":"2025-11-17T10:31:33.963579Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nINSPECTING DATA ATTRIBUTES\n================================================================================\n\nFirst sample attributes and types:\n  mol_id: str = TOX9302\n  mask: Tensor torch.Size([12]) dtype=torch.float32\n  x: Tensor torch.Size([11, 41]) dtype=torch.float32\n  edge_index: Tensor torch.Size([2, 20]) dtype=torch.int64\n  y: Tensor torch.Size([12]) dtype=torch.float32\n\nChecking attribute consistency across 5 samples:\n\nSample 0:\n  mol_id: TOX9302\n  mask: torch.Size([12])\n  x: torch.Size([11, 41])\n  edge_index: torch.Size([2, 20])\n  y: torch.Size([12])\n\nSample 1:\n  mol_id: TOX25748\n  mask: torch.Size([12])\n  x: torch.Size([20, 41])\n  edge_index: torch.Size([2, 40])\n  y: torch.Size([12])\n\nSample 2:\n  mol_id: TOX4625\n  mask: torch.Size([12])\n  x: torch.Size([16, 41])\n  edge_index: torch.Size([2, 34])\n  y: torch.Size([12])\n\nSample 3:\n  mol_id: TOX22180\n  mask: torch.Size([12])\n  x: torch.Size([13, 41])\n  edge_index: torch.Size([2, 26])\n  y: torch.Size([12])\n\nSample 4:\n  mol_id: TOX4559\n  mask: torch.Size([12])\n  x: torch.Size([16, 41])\n  edge_index: torch.Size([2, 34])\n  y: torch.Size([12])\n\n================================================================================\nCLEANING DATASET\n================================================================================\n\nCleaning train_dataset...\n   Kept only: ['x', 'edge_index', 'y', 'mask', 'mol_id']\n   Removed: []\n\nCleaning val_dataset...\n   Kept only: ['x', 'edge_index', 'y', 'mask', 'mol_id']\n   Removed: []\n\nCleaning test_dataset...\n   Kept only: ['x', 'edge_index', 'y', 'mask', 'mol_id']\n   Removed: []\n\n Dataset cleaned!\n\nAfter cleaning, attributes: ['mol_id', 'mask', 'x', 'edge_index', 'y']\n\n================================================================================\nCREATING SIMPLE COLLATE FUNCTION\n================================================================================\n\nCreating dataloaders...\n\n train_loader: 201 batches\n val_loader: 26 batches\n test_loader: 26 batches\n\n================================================================================\nVERIFYING DATALOADERS\n================================================================================\n\nBatch 0:\n  x shape:         torch.Size([554, 41])\n  edge_index shape: torch.Size([2, 1158])\n  y shape:         torch.Size([32, 12])\n  mask shape:      torch.Size([32, 12])\n\nBatch 1:\n  x shape:         torch.Size([587, 41])\n  edge_index shape: torch.Size([2, 1204])\n  y shape:         torch.Size([32, 12])\n  mask shape:      torch.Size([32, 12])\n\nBatch 2:\n  x shape:         torch.Size([654, 41])\n  edge_index shape: torch.Size([2, 1368])\n  y shape:         torch.Size([32, 12])\n  mask shape:      torch.Size([32, 12])\n\n================================================================================\n SUCCESS! DATALOADERS READY FOR TRAINING!\n================================================================================\n\nNext: Run training!\n\nmodel, classifier, optimal_thresholds = train_gat(\n    model=model,\n    classifier=classifier,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    device=device,\n    num_classes=12,\n    epochs=100\n)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"\nprint(f\"   Train: {len(train_dataset)} samples\")\nprint(f\"   Val:   {len(val_dataset)} samples\")\nprint(f\"   Test:  {len(test_dataset)} samples\\n\")\n\n# ============================================================================\n# SETUP DEVICE\n# ============================================================================\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\" Device: {device}\\n\")\n\n\nmodel = GAT(\n    input_dim=41,          # sá»‘ Ä‘áº·c trÆ°ng input cá»§a node\n    hidden_dim=128,       # hidden dimension\n    num_heads=4,          # sá»‘ attention heads\n    num_layers=3,         # sá»‘ layer GAT\n    embedding_dim=512,    # kÃ­ch thÆ°á»›c embedding cuá»‘i\n    dropout=0.2\n)\n\n# 2. Classifier (FIX: Chá»‰ khá»Ÿi táº¡o 1 láº§n)\nclassifier = MultiLabelClassifier(\n    embedding_dim=512,\n    num_classes=12,\n    hidden_dim=256,\n    dropout=0.3,\n    temperature=1.0\n)\n\nprint(f\" GAT parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\" Classifier parameters: {sum(p.numel() for p in classifier.parameters()):,}\\n\")\n\n# ============================================================================\n# TRAINING\n# ============================================================================\nprint(\" Starting training...\\n\")\n\nmodel, classifier, optimal_thresholds = train_gat(\n    model=model,\n    classifier=classifier,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    device=device,\n    num_classes=12,\n    epochs=200,\n    patience=20\n)\n\n# ============================================================================\n# EVALUATION ON TEST SET\n# ============================================================================\nprint(\"\\n Evaluating on test set...\")\n\ntest_results = evaluate_multilabel(\n    model=model,\n    classifier=classifier,\n    data_loader=test_loader,\n    device=device,\n    num_classes=12,\n    optimal_thresholds=optimal_thresholds\n)\n\n# Print final results\nprint(\"\\n\" + \"=\"*80)\nprint(\" FINAL TEST RESULTS\")\nprint(\"=\"*80)\nprint(f\"Micro F1:       {test_results['micro_f1']:.4f}\")\nprint(f\"Macro F1:       {test_results['macro_f1']:.4f}\")\n\n\n\n\nprint(\"=\"*80 + \"\\n\")\n\n# ============================================================================\n# EXTRACT EMBEDDINGS (OPTIONAL)\n# ============================================================================\nprint(\" Extracting embeddings from trained model...\")\n\nembeddings_paths = extract_embeddings_by_split(\n    model=model,\n    classifier=classifier,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    test_loader=test_loader,\n    device=device,\n    num_classes=12,\n    output_dir='embeddings'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:31:33.965513Z","iopub.execute_input":"2025-11-17T10:31:33.965752Z","iopub.status.idle":"2025-11-17T10:36:30.343545Z","shell.execute_reply.started":"2025-11-17T10:31:33.965735Z","shell.execute_reply":"2025-11-17T10:36:30.342870Z"}},"outputs":[{"name":"stdout","text":"   Train: 6404 samples\n   Val:   801 samples\n   Test:  801 samples\n\n Device: cuda\n\n GAT parameters: 1,560,448\n Classifier parameters: 166,541\n\n Starting training...\n\n\n================================================================================\nANALYZING TRAINING DATA\n================================================================================\n\nClass Distribution:\nClass    Samples    Ratio      Imbalance\n--------------------------------------------------------------------------------\nClass 0  254        3.97     % 24.21:1\nClass 1  202        3.15     % 30.70:1\nClass 2  626        9.78     % 9.23:1\nClass 3  237        3.70     % 26.02:1\nClass 4  633        9.88     % 9.12:1\nClass 5  295        4.61     % 20.71:1\nClass 6  154        2.40     % 40.58:1\nClass 7  763        11.91    % 7.39:1\nClass 8  215        3.36     % 28.79:1\nClass 9  301        4.70     % 20.28:1\nClass 10 754        11.77    % 7.49:1\nClass 11 356        5.56     % 16.99:1\n\n================================================================================\nSTARTING TRAINING\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 1/200\n================================================================================\nLoss:        Train=0.0999 | Val=0.0508\n Mean Prob:   Train=0.4419 | Val=0.4186\n Micro F1:    0.2241 (P=0.1417, R=0.5356)\n Macro F1:    0.2264\n Hamming:     0.2385\n\n Lowest F1 Classes:\n   Class 6: F1=0.111, Thresh=0.507, PosRatio=0.021\n   Class 9: F1=0.117, Thresh=0.455, PosRatio=0.056\n   Class 3: F1=0.121, Thresh=0.518, PosRatio=0.037\n\n Saved best model!\n   Micro F1: 0.2241\n   Mean Prob: 0.4186\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.3072\n   Mean Prob: 0.4055\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.3355\n   Mean Prob: 0.4194\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 5/200\n================================================================================\nLoss:        Train=0.0401 | Val=0.0390\n Mean Prob:   Train=0.4374 | Val=0.4258\n Micro F1:    0.3300 (P=0.2521, R=0.4773)\n Macro F1:    0.3310\n Hamming:     0.1246\n\n Lowest F1 Classes:\n   Class 6: F1=0.196, Thresh=0.492, PosRatio=0.021\n   Class 11: F1=0.203, Thresh=0.491, PosRatio=0.049\n   Class 3: F1=0.208, Thresh=0.491, PosRatio=0.037\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.3614\n   Mean Prob: 0.4309\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.3769\n   Mean Prob: 0.4299\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.3906\n   Mean Prob: 0.4504\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.4017\n   Mean Prob: 0.4405\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 10/200\n================================================================================\nLoss:        Train=0.0358 | Val=0.0420\n Mean Prob:   Train=0.4409 | Val=0.4534\n Micro F1:    0.3900 (P=0.3414, R=0.4547)\n Macro F1:    0.3730\n Hamming:     0.0914\n\n Lowest F1 Classes:\n   Class 6: F1=0.235, Thresh=0.498, PosRatio=0.021\n   Class 3: F1=0.246, Thresh=0.547, PosRatio=0.037\n   Class 11: F1=0.297, Thresh=0.593, PosRatio=0.049\n   Current LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.4078\n   Mean Prob: 0.4479\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.4185\n   Mean Prob: 0.4425\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 15/200\n================================================================================\nLoss:        Train=0.0348 | Val=0.0404\n Mean Prob:   Train=0.4367 | Val=0.4467\n Micro F1:    0.4081 (P=0.3433, R=0.5032)\n Macro F1:    0.4049\n Hamming:     0.0938\n\n Lowest F1 Classes:\n   Class 11: F1=0.270, Thresh=0.526, PosRatio=0.049\n   Class 3: F1=0.277, Thresh=0.526, PosRatio=0.037\n   Class 5: F1=0.337, Thresh=0.550, PosRatio=0.040\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.4373\n   Mean Prob: 0.4293\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 20/200\n================================================================================\nLoss:        Train=0.0344 | Val=0.0405\n Mean Prob:   Train=0.4357 | Val=0.4437\n Micro F1:    0.4217 (P=0.3819, R=0.4709)\n Macro F1:    0.4013\n Hamming:     0.0830\n\n Lowest F1 Classes:\n   Class 3: F1=0.259, Thresh=0.562, PosRatio=0.037\n   Class 11: F1=0.286, Thresh=0.609, PosRatio=0.049\n   Class 6: F1=0.323, Thresh=0.528, PosRatio=0.021\n   Current LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 25/200\n================================================================================\nLoss:        Train=0.0335 | Val=0.0389\n Mean Prob:   Train=0.4305 | Val=0.4387\n Micro F1:    0.4232 (P=0.3660, R=0.5016)\n Macro F1:    0.4108\n Hamming:     0.0879\n\n Lowest F1 Classes:\n   Class 3: F1=0.213, Thresh=0.548, PosRatio=0.037\n   Class 11: F1=0.270, Thresh=0.562, PosRatio=0.049\n   Class 6: F1=0.275, Thresh=0.524, PosRatio=0.021\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 30/200\n================================================================================\nLoss:        Train=0.0318 | Val=0.0380\n Mean Prob:   Train=0.4210 | Val=0.4367\n Micro F1:    0.4181 (P=0.3423, R=0.5372)\n Macro F1:    0.4114\n Hamming:     0.0961\n\n Lowest F1 Classes:\n   Class 6: F1=0.232, Thresh=0.530, PosRatio=0.021\n   Class 3: F1=0.235, Thresh=0.508, PosRatio=0.037\n   Class 11: F1=0.315, Thresh=0.617, PosRatio=0.049\n   Current LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.4388\n   Mean Prob: 0.4115\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 35/200\n================================================================================\nLoss:        Train=0.0314 | Val=0.0369\n Mean Prob:   Train=0.4174 | Val=0.4239\n Micro F1:    0.4357 (P=0.3804, R=0.5097)\n Macro F1:    0.4152\n Hamming:     0.0849\n\n Lowest F1 Classes:\n   Class 3: F1=0.250, Thresh=0.577, PosRatio=0.037\n   Class 6: F1=0.293, Thresh=0.538, PosRatio=0.021\n   Class 11: F1=0.316, Thresh=0.617, PosRatio=0.049\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 40/200\n================================================================================\nLoss:        Train=0.0313 | Val=0.0360\n Mean Prob:   Train=0.4149 | Val=0.4198\n Micro F1:    0.4395 (P=0.3961, R=0.4935)\n Macro F1:    0.4125\n Hamming:     0.0809\n\n Lowest F1 Classes:\n   Class 6: F1=0.190, Thresh=0.628, PosRatio=0.021\n   Class 3: F1=0.255, Thresh=0.569, PosRatio=0.037\n   Class 11: F1=0.313, Thresh=0.571, PosRatio=0.049\n   Current LR: 0.000500\n\n Saved best model!\n   Micro F1: 0.4395\n   Mean Prob: 0.4198\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.4476\n   Mean Prob: 0.4045\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 45/200\n================================================================================\nLoss:        Train=0.0309 | Val=0.0384\n Mean Prob:   Train=0.4134 | Val=0.4265\n Micro F1:    0.4368 (P=0.3859, R=0.5032)\n Macro F1:    0.4213\n Hamming:     0.0834\n\n Lowest F1 Classes:\n   Class 6: F1=0.222, Thresh=0.510, PosRatio=0.021\n   Class 3: F1=0.259, Thresh=0.586, PosRatio=0.037\n   Class 11: F1=0.330, Thresh=0.629, PosRatio=0.049\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 50/200\n================================================================================\nLoss:        Train=0.0301 | Val=0.0376\n Mean Prob:   Train=0.4061 | Val=0.4196\n Micro F1:    0.4386 (P=0.3762, R=0.5259)\n Macro F1:    0.4213\n Hamming:     0.0866\n\n Lowest F1 Classes:\n   Class 6: F1=0.207, Thresh=0.553, PosRatio=0.021\n   Class 3: F1=0.256, Thresh=0.536, PosRatio=0.037\n   Class 11: F1=0.362, Thresh=0.610, PosRatio=0.049\n   Current LR: 0.000250\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.4561\n   Mean Prob: 0.4043\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 55/200\n================================================================================\nLoss:        Train=0.0288 | Val=0.0382\n Mean Prob:   Train=0.3967 | Val=0.4057\n Micro F1:    0.4458 (P=0.4098, R=0.4887)\n Macro F1:    0.4201\n Hamming:     0.0781\n\n Lowest F1 Classes:\n   Class 3: F1=0.233, Thresh=0.586, PosRatio=0.037\n   Class 6: F1=0.240, Thresh=0.566, PosRatio=0.021\n   Class 11: F1=0.320, Thresh=0.672, PosRatio=0.049\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.4583\n   Mean Prob: 0.3955\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 60/200\n================================================================================\nLoss:        Train=0.0283 | Val=0.0366\n Mean Prob:   Train=0.3879 | Val=0.3935\n Micro F1:    0.4547 (P=0.4064, R=0.5162)\n Macro F1:    0.4218\n Hamming:     0.0796\n\n Lowest F1 Classes:\n   Class 6: F1=0.250, Thresh=0.646, PosRatio=0.021\n   Class 3: F1=0.273, Thresh=0.605, PosRatio=0.037\n   Class 11: F1=0.348, Thresh=0.555, PosRatio=0.049\n   Current LR: 0.000250\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.4606\n   Mean Prob: 0.3956\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 65/200\n================================================================================\nLoss:        Train=0.0279 | Val=0.0371\n Mean Prob:   Train=0.3851 | Val=0.3870\n Micro F1:    0.4545 (P=0.4051, R=0.5178)\n Macro F1:    0.4288\n Hamming:     0.0799\n\n Lowest F1 Classes:\n   Class 6: F1=0.256, Thresh=0.600, PosRatio=0.021\n   Class 3: F1=0.306, Thresh=0.541, PosRatio=0.037\n   Class 11: F1=0.360, Thresh=0.561, PosRatio=0.049\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n Saved best model!\n   Micro F1: 0.4662\n   Mean Prob: 0.3774\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 70/200\n================================================================================\nLoss:        Train=0.0275 | Val=0.0399\n Mean Prob:   Train=0.3840 | Val=0.3894\n Micro F1:    0.4733 (P=0.4271, R=0.5307)\n Macro F1:    0.4443\n Hamming:     0.0759\n\n Lowest F1 Classes:\n   Class 3: F1=0.291, Thresh=0.577, PosRatio=0.037\n   Class 6: F1=0.303, Thresh=0.625, PosRatio=0.021\n   Class 11: F1=0.349, Thresh=0.598, PosRatio=0.049\n   Current LR: 0.000250\n\n Saved best model!\n   Micro F1: 0.4733\n   Mean Prob: 0.3894\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 75/200\n================================================================================\nLoss:        Train=0.0270 | Val=0.0413\n Mean Prob:   Train=0.3745 | Val=0.3837\n Micro F1:    0.4532 (P=0.4111, R=0.5049)\n Macro F1:    0.4199\n Hamming:     0.0783\n\n Lowest F1 Classes:\n   Class 3: F1=0.269, Thresh=0.544, PosRatio=0.037\n   Class 6: F1=0.286, Thresh=0.620, PosRatio=0.021\n   Class 11: F1=0.333, Thresh=0.645, PosRatio=0.049\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 80/200\n================================================================================\nLoss:        Train=0.0263 | Val=0.0403\n Mean Prob:   Train=0.3699 | Val=0.3742\n Micro F1:    0.4584 (P=0.4342, R=0.4854)\n Macro F1:    0.4369\n Hamming:     0.0738\n\n Lowest F1 Classes:\n   Class 3: F1=0.277, Thresh=0.557, PosRatio=0.037\n   Class 6: F1=0.286, Thresh=0.612, PosRatio=0.021\n   Class 11: F1=0.365, Thresh=0.563, PosRatio=0.049\n   Current LR: 0.000125\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 85/200\n================================================================================\nLoss:        Train=0.0259 | Val=0.0427\n Mean Prob:   Train=0.3613 | Val=0.3733\n Micro F1:    0.4537 (P=0.4211, R=0.4919)\n Macro F1:    0.4331\n Hamming:     0.0762\n\n Lowest F1 Classes:\n   Class 6: F1=0.303, Thresh=0.630, PosRatio=0.021\n   Class 3: F1=0.304, Thresh=0.617, PosRatio=0.037\n   Class 11: F1=0.353, Thresh=0.589, PosRatio=0.049\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 90/200\n================================================================================\nLoss:        Train=0.0253 | Val=0.0418\n Mean Prob:   Train=0.3568 | Val=0.3648\n Micro F1:    0.4644 (P=0.4817, R=0.4482)\n Macro F1:    0.4366\n Hamming:     0.0665\n\n Lowest F1 Classes:\n   Class 6: F1=0.267, Thresh=0.660, PosRatio=0.021\n   Class 3: F1=0.340, Thresh=0.611, PosRatio=0.037\n   Class 11: F1=0.368, Thresh=0.573, PosRatio=0.049\n   Current LR: 0.000063\n\nâš ï¸  Early stopping triggered at epoch 90\n   Best F1 was 0.4733 at epoch 70\n\n================================================================================\n TRAINING COMPLETED\n================================================================================\n Best Micro F1:     0.4733\n Best Epoch:        70\n Final Mean Prob:   0.3648\n================================================================================\n\n\n Evaluating on test set...\n\n================================================================================\nFINAL EVALUATION RESULTS\n================================================================================\n\n Overall Metrics:\n   Micro F1:        0.4282\n   Macro F1:        0.4083\n   Micro Precision: 0.4029\n   Micro Recall:    0.4569\n   Hamming Loss:    0.0692\n   Subset Accuracy: 0.5718\n\n Per-Class Results:\nClass    Threshold    F1         Precision    Recall     MeanProb  \n--------------------------------------------------------------------------------\nNR-AR    0.824        0.514      1.000        0.346      0.399     \nNR-AR-LBD 0.788        0.636      0.778        0.538      0.326     \nNR-AhR   0.604        0.472      0.357        0.697      0.439     \nNR-Aromatase 0.577        0.348      0.414        0.300      0.348     \nNR-ER    0.608        0.364      0.337        0.394      0.508     \nNR-ER-LBD 0.724        0.410      0.727        0.286      0.376     \nNR-PPAR-Î³ 0.625        0.182      0.200        0.167      0.276     \nSR-ARE   0.611        0.410      0.381        0.443      0.488     \nSR-ATAD5 0.639        0.264      0.233        0.304      0.308     \nSR-HSE   0.555        0.320      0.286        0.364      0.393     \nSR-MMP   0.621        0.595      0.574        0.617      0.421     \nSR-p53   0.598        0.386      0.340        0.444      0.352     \n================================================================================\n\n\n================================================================================\n FINAL TEST RESULTS\n================================================================================\nMicro F1:       0.4282\nMacro F1:       0.4083\n================================================================================\n\n Extracting embeddings from trained model...\n\nðŸ”¹ Extracting TRAIN embeddings...\n\n================================================================================\nEXTRACTING GRAPH EMBEDDINGS (with mol_id)\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:01<00:00, 152.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n Extracted 6404 graph embeddings\n Embedding dim: 512\n\n Saved to embeddings/train_embeddings.pt\n File size: 13.50 MB\n================================================================================\n\nðŸ”¹ Extracting VALIDATION embeddings...\n\n================================================================================\nEXTRACTING GRAPH EMBEDDINGS (with mol_id)\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 158.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n Extracted 801 graph embeddings\n Embedding dim: 512\n\n Saved to embeddings/val_embeddings.pt\n File size: 1.69 MB\n================================================================================\n\nðŸ”¹ Extracting TEST embeddings...\n\n================================================================================\nEXTRACTING GRAPH EMBEDDINGS (with mol_id)\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 158.59it/s]","output_type":"stream"},{"name":"stdout","text":"\n Extracted 801 graph embeddings\n Embedding dim: 512\n\n Saved to embeddings/test_embeddings.pt\n File size: 1.69 MB\n================================================================================\n\n================================================================================\n ALL EMBEDDINGS EXTRACTED SUCCESSFULLY\n================================================================================\n   TRAIN: embeddings/train_embeddings.pt\n   VAL: embeddings/val_embeddings.pt\n   TEST: embeddings/test_embeddings.pt\n================================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"path = \"/kaggle/working/embeddings/train_embeddings.pt\"\n\ndata = torch.load(path)\nprint(data.keys())\nprint(data['mol_ids'][:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:36:30.344393Z","iopub.execute_input":"2025-11-17T10:36:30.344704Z","iopub.status.idle":"2025-11-17T10:36:30.378116Z","shell.execute_reply.started":"2025-11-17T10:36:30.344685Z","shell.execute_reply":"2025-11-17T10:36:30.377247Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['embeddings', 'logits', 'probabilities', 'labels', 'mol_ids', 'num_graphs', 'embedding_dim', 'num_classes'])\n['TOX27328', 'TOX29127', 'TOX31604', 'TOX4368', 'TOX5599', 'TOX25380', 'TOX27453', 'TOX23895', 'TOX2659', 'TOX1892']\n","output_type":"stream"}],"execution_count":22}]}