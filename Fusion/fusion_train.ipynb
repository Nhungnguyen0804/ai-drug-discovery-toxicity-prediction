{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de80b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49201d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constant\n",
    "TEST_EMBEDDING_FILE = \"test_embeddings.pt\"\n",
    "TRAIN_EMBEDDING_FILE = \"train_embeddings.pt\"\n",
    "VAL_EMBEDDING_FILE = \"val_embeddings.pt\"\n",
    "\n",
    "GNN_TEST_EMBEDDING = f\"../GATNN/embeddings/{TEST_EMBEDDING_FILE}\"\n",
    "GNN_TRAIN_EMBEDDING = f\"../GATNN/embeddings/{TRAIN_EMBEDDING_FILE}\"\n",
    "GNN_VAL_EMBEDDING = f\"../GATNN/embeddings/{VAL_EMBEDDING_FILE}\"\n",
    "BERT_TEST_EMBEDDING = f\"../BERT/embedding/{TEST_EMBEDDING_FILE}\"\n",
    "BERT_TRAIN_EMBEDDING = f\"../BERT/embedding/{TRAIN_EMBEDDING_FILE}\"\n",
    "BERT_VAL_EMBEDDING = f\"../BERT/embedding/{VAL_EMBEDDING_FILE}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b29f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNN_FILE = \"gat_embeddings.pt\"\n",
    "BERT_FILE = \"all_embeddings.pt\"\n",
    "GNN_EMBEDDING = f\"../GATNN/embeddings/{GNN_FILE}\"\n",
    "BERT_EMBEDDING = f\"../BERT/embedding/{BERT_FILE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df0c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_file(path):\n",
    "    \"\"\"\n",
    "    Hàm load file embedding từ đường dẫn `path`.\n",
    "    Tự động nhận dạng nhiều kiểu dữ liệu khác nhau:\n",
    "    - dict có key 'embeddings' hoặc 'emb', 'vectors', 'features'\n",
    "    - trực tiếp là Tensor\n",
    "    - list/tuple (chuyển sang Tensor)\n",
    "    \"\"\"\n",
    "    # ---- Bỏ chặn numpy pickle cổ ----\n",
    "    try:\n",
    "        torch.serialization.add_safe_globals([np.core.multiarray._reconstruct])\n",
    "    except Exception:\n",
    "        pass  # không sao\n",
    "\n",
    "    # Load thử với weights_only=False ----\n",
    "    try:\n",
    "        data = torch.load(path, weights_only=False)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LỖI load file embedding: {e}\")\n",
    "    \n",
    "    # data = torch.load(path, map_location='cpu')\n",
    "\n",
    "    #Chuẩn hóa về tensor ----\n",
    "    # Nếu là tensor trực tiếp\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return {'embeddings': data}\n",
    "    \n",
    "\n",
    "    # Nếu là numpy\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return {'embeddings': torch.tensor(data)}\n",
    "    \n",
    "\n",
    "    # Trường hợp 1: File lưu dạng dict\n",
    "    if isinstance(data, dict):\n",
    "\n",
    "        # TH1.1: Dict có key phổ biến 'embeddings'\n",
    "        if 'embeddings' in data:\n",
    "            return {\n",
    "                'embeddings': torch.as_tensor(data['embeddings']),\n",
    "                'ids': data.get('mol_ids', None)  or data.get('mol_id',None) # có thể không có\n",
    "            }\n",
    "\n",
    "        # TH1.2: Một số file embedding dùng key khác\n",
    "        for key in ['emb', 'vectors', 'features']:\n",
    "            if key in data:\n",
    "                return {\n",
    "                    'embeddings': torch.as_tensor(data[key]),\n",
    "                    'ids': data.get('mol_ids', None) or data.get('mol_id',None)\n",
    "                }\n",
    "\n",
    "        # TH1.3: Dict không rõ cấu trúc → thử convert cả dict sang tensor\n",
    "        try:\n",
    "            return {'embeddings': torch.as_tensor(data)}\n",
    "        except Exception:\n",
    "            raise ValueError(\n",
    "                f\"Lỗi: Dict trong file {path} có cấu trúc không hỗ trợ để chuyển sang Tensor\"\n",
    "            )\n",
    "\n",
    "    \n",
    "\n",
    "    # Trường hợp 3: File là list hoặc tuple → convert sang Tensor\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        try:\n",
    "            return {'embeddings': torch.as_tensor(data)}\n",
    "        except Exception:\n",
    "            raise ValueError(\n",
    "                f\"Lỗi: Không thể chuyển list/tuple trong file {path} sang Tensor\"\n",
    "            )\n",
    "\n",
    "    # Trường hợp không thuộc loại nào\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Lỗi: Định dạng dữ liệu trong file {path} không được hỗ trợ\"\n",
    "            f\"Loại: {type(data)}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73042a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KIỂM TRA BERT BRANCH==================================\n",
      "KIỂM TRA FILE: ../BERT/embedding/test_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (802, 768)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../BERT/embedding/train_embeddings.pt ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16284\\1766450209.py:11: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  torch.serialization.add_safe_globals([np.core.multiarray._reconstruct])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅File hợp lệ!\n",
      "Kích thước: (6411, 768)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../BERT/embedding/val_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (801, 768)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../BERT/embedding/all_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (8014, 768)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA GNN BRANCH====================================\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/test_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (801, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/train_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (6404, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/val_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (801, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/gat_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (8006, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "✅File hợp lệ!\n",
      "Kích thước: (8014, 768)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA GNN BRANCH====================================\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/test_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (801, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/train_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (6404, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/val_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (801, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/gat_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (8006, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_embedding_file(path):\n",
    "    \"\"\"Kiểm tra 1 file embedding và trả về True/False + in thông tin chi tiết.\"\"\"\n",
    "\n",
    "    print(f\"KIỂM TRA FILE: {path} ======\")\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Lỗi: File không tồn tại.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        result = load_embedding_file(path)\n",
    "        emb = result[\"embeddings\"]\n",
    "\n",
    "        # --- Kiểm tra embedding có phải Tensor ---\n",
    "        if not isinstance(emb, torch.Tensor):\n",
    "            print(\"Lỗi: embeddings không phải Torch.Tensor.\")\n",
    "            return False\n",
    "\n",
    "        # --- Kiểm tra số chiều ---\n",
    "        if emb.ndim < 2:\n",
    "            print(f\"Lỗi: embeddings phải >= 2 chiều, hiện tại: {emb.ndim}\")\n",
    "            return False\n",
    "\n",
    "        # --- In thông tin hợp lệ ---\n",
    "        print(\"✅File hợp lệ!\")\n",
    "        print(f\"Kích thước: {tuple(emb.shape)}___Kiểu dữ liệu: {emb.dtype}___Có IDs không? { 'Có' if result.get('ids') is not None else 'Không' }\")\n",
    "       \n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi load file: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_all_embeddings():\n",
    "    \"\"\"Test toàn bộ file BERT + GNN\"\"\"\n",
    "\n",
    "    bert_files = [\n",
    "        BERT_TEST_EMBEDDING,\n",
    "        BERT_TRAIN_EMBEDDING,\n",
    "        BERT_VAL_EMBEDDING,\n",
    "        BERT_EMBEDDING\n",
    "    ]\n",
    "\n",
    "    gnn_files = [\n",
    "        GNN_TEST_EMBEDDING,\n",
    "        GNN_TRAIN_EMBEDDING,\n",
    "        GNN_VAL_EMBEDDING,\n",
    "        GNN_EMBEDDING\n",
    "    ]\n",
    "\n",
    "   \n",
    "    print(\"KIỂM TRA BERT BRANCH==================================\")\n",
    "  \n",
    "\n",
    "    for f in bert_files:\n",
    "        check_embedding_file(f)\n",
    "\n",
    "    \n",
    "    print(\"KIỂM TRA GNN BRANCH====================================\")\n",
    "\n",
    "    for f in gnn_files:\n",
    "        check_embedding_file(f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_all_embeddings()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d87694bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load BERT embeddings\n",
    "bert_train = torch.load(BERT_TRAIN_EMBEDDING, weights_only=False)\n",
    "bert_val = torch.load(BERT_VAL_EMBEDDING, weights_only=False)\n",
    "bert_test = torch.load(BERT_TEST_EMBEDDING, weights_only=False)\n",
    "\n",
    "# Load GAT embeddings\n",
    "gat_train = torch.load(GNN_TRAIN_EMBEDDING, weights_only=False)\n",
    "gat_val = torch.load(GNN_VAL_EMBEDDING, weights_only=False)\n",
    "gat_test = torch.load(GNN_TEST_EMBEDDING, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c187148",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_all = torch.load(BERT_EMBEDDING, weights_only=False)\n",
    "gat_all = torch.load(GNN_EMBEDDING, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24793905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: Số lượng không khớp! BERT=6411, GAT=6404\n",
      "Train: Mol IDs KHÔNG khớp, cần align lại!\n",
      "Val: Mol IDs KHÔNG khớp, cần align lại!\n",
      " Test: Số lượng không khớp! BERT=802, GAT=801\n",
      "Test: Mol IDs KHÔNG khớp, cần align lại!\n",
      " All Data: Số lượng không khớp! BERT=8014, GAT=8006\n",
      "All Data: Mol IDs KHÔNG khớp, cần align lại!\n"
     ]
    }
   ],
   "source": [
    "def check_alignment(bert_data, gat_data, split_name):\n",
    "    \"\"\"Kiểm tra xem mol_ids có khớp nhau không\"\"\"\n",
    "    bert_ids = bert_data['mol_id']  # hoặc 'mol_ids'\n",
    "    gat_ids = gat_data['mol_ids']\n",
    "    \n",
    "    # So sánh\n",
    "    if len(bert_ids) != len(gat_ids):\n",
    "        print(f\" {split_name}: Số lượng không khớp! BERT={len(bert_ids)}, GAT={len(gat_ids)}\")\n",
    "    \n",
    "    # Kiểm tra thứ tự\n",
    "    if isinstance(bert_ids[0], str) and isinstance(gat_ids[0], str):\n",
    "        match = all(t == g for t, g in zip(bert_ids, gat_ids))\n",
    "    else:\n",
    "        match = torch.equal(bert_ids, gat_ids)\n",
    "    \n",
    "    if match:\n",
    "        print(f\" {split_name}: Mol IDs khớp hoàn toàn!\")\n",
    "    else:\n",
    "        print(f\"{split_name}: Mol IDs KHÔNG khớp, cần align lại!\")\n",
    "    \n",
    "    return match\n",
    "\n",
    "# Check tất cả splits\n",
    "train_match = check_alignment(bert_train, gat_train, \"Train\")\n",
    "val_match = check_alignment(bert_val, gat_val, \"Val\")\n",
    "test_match = check_alignment(bert_test, gat_test, \"Test\")\n",
    "all_match = check_alignment(bert_all, gat_all, \"All Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f891e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, bert_data, gat_data):\n",
    "        # Embeddings\n",
    "        self.text_embs = bert_data['embeddings']\n",
    "        self.graph_embs = gat_data['embeddings']\n",
    "        \n",
    "        # Labels (lấy từ 1 trong 2, giả sử giống nhau)\n",
    "        self.labels = bert_data['labels']\n",
    "        \n",
    "        # Optional: mol_ids để track\n",
    "        self.mol_ids = bert_data['mol_id']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text_emb': self.text_embs[idx],\n",
    "            'graph_emb': self.graph_embs[idx],\n",
    "            'label': self.labels[idx],\n",
    "            'mol_id': self.mol_ids[idx]\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecbee277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT có 8014 mẫu\n",
      "GAT  có 8006 mẫu\n",
      "Trùng mol_id: 8006 mẫu\n"
     ]
    }
   ],
   "source": [
    "def align_datasets(bert_data, gat_data):\n",
    "    \"\"\"Align BERT và GAT theo mol_id (không kiểm tra labels).\"\"\"\n",
    "\n",
    "    bert_ids = bert_data['mol_id']\n",
    "    gat_ids  = gat_data['mol_ids']\n",
    "\n",
    "    # Tạo dictionary để tìm index nhanh\n",
    "    bert_dict = {str(m): i for i, m in enumerate(bert_ids)}\n",
    "    gat_dict  = {str(m): i for i, m in enumerate(gat_ids)}\n",
    "\n",
    "    # Lấy phần giao của mol_id\n",
    "    common_ids = sorted(set(bert_dict.keys()) & set(gat_dict.keys()))\n",
    "\n",
    "    print(f\"BERT có {len(bert_ids)} mẫu\")\n",
    "    print(f\"GAT  có {len(gat_ids)} mẫu\")\n",
    "    print(f\"Trùng mol_id: {len(common_ids)} mẫu\")\n",
    "\n",
    "    bert_indices = [bert_dict[m] for m in common_ids]\n",
    "    gat_indices  = [gat_dict[m] for m in common_ids]\n",
    "\n",
    "    # Embeddings\n",
    "    bert_embs = bert_data[\"embeddings\"][bert_indices]\n",
    "    gat_embs  = gat_data[\"embeddings\"][gat_indices]\n",
    "\n",
    "    # Labels\n",
    "    bert_labels = bert_data[\"labels\"][bert_indices]\n",
    "    gat_labels  = gat_data[\"labels\"][gat_indices]\n",
    "\n",
    "    aligned_bert = {\n",
    "        \"embeddings\": bert_embs,\n",
    "        \"labels\": bert_labels,\n",
    "        \"mol_id\": common_ids\n",
    "    }\n",
    "\n",
    "    aligned_gat = {\n",
    "        \"embeddings\": gat_embs,\n",
    "        \"labels\": gat_labels,\n",
    "        \"mol_ids\": common_ids\n",
    "    }\n",
    "\n",
    "    return aligned_bert, aligned_gat\n",
    "\n",
    "\n",
    "# Align từng split\n",
    "\n",
    "all_bert_aligned, all_gat_aligned = align_datasets(bert_all, gat_all)\n",
    "# Tạo datasets\n",
    "\n",
    "\n",
    "# print(f\"\\n Datasets created:\")\n",
    "# print(f\"Train: {len(train_dataset)} samples\")\n",
    "# print(f\"Val: {len(val_dataset)} samples\")\n",
    "# print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b14fea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total aligned samples: 8006\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N = len(all_bert_aligned[\"mol_id\"])\n",
    "print(\"Total aligned samples:\", N)\n",
    "indices = np.arange(N)\n",
    "\n",
    "print(type(all_bert_aligned[\"labels\"]))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c4dee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Datasets created:\n",
      "Train: 6404 samples\n",
      "Val: 801 samples\n",
      "Test: 801 samples\n"
     ]
    }
   ],
   "source": [
    "def split_data(aligned_data, idx, mol_id):\n",
    "    return {\n",
    "        \"embeddings\": aligned_data[\"embeddings\"][idx],\n",
    "        \"labels\": aligned_data[\"labels\"][idx],\n",
    "        \"mol_id\": [aligned_data[mol_id][i] for i in idx]\n",
    "    }\n",
    "train_bert_aligned = split_data(all_bert_aligned, train_idx, \"mol_id\")\n",
    "val_bert_aligned = split_data(all_bert_aligned, val_idx, \"mol_id\")\n",
    "test_bert_aligned = split_data(all_bert_aligned, test_idx, \"mol_id\")\n",
    "\n",
    "train_dataset = FusionDataset(train_bert_aligned, split_data(all_gat_aligned, train_idx, \"mol_ids\"))\n",
    "val_dataset = FusionDataset(val_bert_aligned, split_data(all_gat_aligned, val_idx, \"mol_ids\"))\n",
    "test_dataset = FusionDataset(test_bert_aligned, split_data(all_gat_aligned, test_idx, \"mol_ids\"))\n",
    "print(f\"\\n Datasets created:\")\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8920b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFusionMultiLabel(nn.Module):\n",
    "    \"\"\"Balanced fusion: force GAT to ~50% contribution\"\"\"\n",
    "    def __init__(self, text_dim=768, graph_dim=512, num_labels=12,\n",
    "                 gat_target_ratio=0.5, strategy='balanced'):\n",
    "        \"\"\"\n",
    "        strategy options:\n",
    "        - 'balanced': Equal capacity for both, independent classifiers\n",
    "        - 'gat_priority': GAT gets stronger projections & separate head\n",
    "        - 'force_ratio': Strict 50-50 split\n",
    "        - 'competitive': Both models compete equally\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gat_target_ratio = gat_target_ratio\n",
    "        self.strategy = strategy\n",
    "        \n",
    "        hidden_dim = 256\n",
    "        \n",
    "        # ============ Strategy 1: Separate pathways ============\n",
    "        if strategy in ['balanced', 'gat_priority', 'competitive']:\n",
    "            # BERT pathway\n",
    "            self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
    "            self.text_classifier = nn.Sequential(\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.Linear(hidden_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(128, num_labels)\n",
    "            )\n",
    "            \n",
    "            # GAT pathway (matched capacity)\n",
    "            self.graph_proj = nn.Linear(graph_dim, hidden_dim)\n",
    "            self.graph_classifier = nn.Sequential(\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.Linear(hidden_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(128, num_labels)\n",
    "            )\n",
    "            \n",
    "            # Mixture of Experts: learned weighting\n",
    "            self.router = nn.Sequential(\n",
    "                nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, 2)\n",
    "            )\n",
    "            \n",
    "            # NEW: Temperature for sharpening weights\n",
    "            self.temperature = nn.Parameter(torch.tensor(1.0))\n",
    "            \n",
    "            # NEW: Bias to prefer GAT during init\n",
    "            self.preference_bias = nn.Parameter(torch.tensor([0.0, 0.5]))\n",
    "\n",
    "        # ============ Strategy 2: Force ratio ============\n",
    "        elif strategy == 'force_ratio':\n",
    "            self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
    "            self.graph_proj = nn.Linear(graph_dim, hidden_dim)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.Linear(hidden_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(128, num_labels)\n",
    "            )\n",
    "\n",
    "    def forward(self, text_emb, graph_emb):\n",
    "        B = text_emb.size(0)\n",
    "        \n",
    "        # ============ Separate pathways strategy ============\n",
    "        if self.strategy in ['balanced', 'gat_priority', 'competitive']:\n",
    "            t = self.text_proj(text_emb)   # [B, H]\n",
    "            g = self.graph_proj(graph_emb) # [B, H]\n",
    "            \n",
    "            # Get logits from both pathways\n",
    "            t_logits = self.text_classifier(t)  # [B, num_labels]\n",
    "            g_logits = self.graph_classifier(g) # [B, num_labels]\n",
    "            \n",
    "            # ---- Router: Learn mixture weights ----\n",
    "            router_input = torch.cat([t, g], dim=1)  # [B, H*2]\n",
    "            router_logits = self.router(router_input)  # [B, 2]\n",
    "            \n",
    "            # Add preference bias (favor GAT)\n",
    "            router_logits = router_logits + self.preference_bias\n",
    "            \n",
    "            # Temperature-scaled softmax for sharper decisions\n",
    "            temp = torch.sigmoid(self.temperature)  # [0.5, 1.0]\n",
    "            weights = torch.softmax(router_logits / (temp + 0.1), dim=1)  # [B, 2]\n",
    "            \n",
    "            # Combine logits using weights\n",
    "            final_logits = weights[:, 0].unsqueeze(-1) * t_logits + \\\n",
    "                          weights[:, 1].unsqueeze(-1) * g_logits\n",
    "            \n",
    "            return final_logits, weights.unsqueeze(1)\n",
    "        \n",
    "        # ============ Force ratio strategy ============\n",
    "        elif self.strategy == 'force_ratio':\n",
    "            t = self.text_proj(text_emb)\n",
    "            g = self.graph_proj(graph_emb)\n",
    "            \n",
    "            # Strict 50-50 or custom ratio\n",
    "            w_text = (1 - self.gat_target_ratio)\n",
    "            w_gat = self.gat_target_ratio\n",
    "            \n",
    "            fused = w_text * t + w_gat * g\n",
    "            logits = self.classifier(fused)\n",
    "            \n",
    "            weights = torch.full((B, 2), 0.5, device=text_emb.device)\n",
    "            weights[:, 0] = w_text\n",
    "            weights[:, 1] = w_gat\n",
    "            \n",
    "            return logits, weights.unsqueeze(1)\n",
    "\n",
    "\n",
    "\n",
    "class BalancedTrainer:\n",
    "    \"\"\"Helper to ensure balanced training\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def balanced_loss(logits, weights, targets, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Penalize models if one dominates too much.\n",
    "        Encourage balanced contributions.\n",
    "        \"\"\"\n",
    "        # Standard classification loss\n",
    "        ce_loss = nn.functional.binary_cross_entropy_with_logits(logits, targets)\n",
    "        \n",
    "        # Regularization: penalize weight imbalance\n",
    "        # weights shape: [B, 1, 2]\n",
    "        w = weights.squeeze(1)  # [B, 2]\n",
    "        \n",
    "        # KL divergence from uniform (0.5, 0.5)\n",
    "        uniform = torch.ones_like(w) * 0.5\n",
    "        balance_loss = nn.functional.kl_div(\n",
    "            torch.log(w + 1e-8), \n",
    "            uniform, \n",
    "            reduction='mean'\n",
    "        )\n",
    "        \n",
    "        return ce_loss + alpha * balance_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def entropy_regularization(weights, target_entropy=0.693):\n",
    "        \"\"\"\n",
    "        target_entropy ≈ 0.693 = entropy of (0.5, 0.5)\n",
    "        Encourage weights to stay balanced\n",
    "        \"\"\"\n",
    "        w = weights.squeeze(1)  # [B, 2]\n",
    "        entropy = -(w * torch.log(w + 1e-8)).sum(dim=1).mean()\n",
    "        \n",
    "        # Loss: penalize deviation from target entropy\n",
    "        entropy_loss = (entropy - target_entropy).abs()\n",
    "        return entropy_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc0b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6404 samples\n",
      "Val: 801 samples\n",
      "Test: 801 samples\n"
     ]
    }
   ],
   "source": [
    "# Tạo DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8b10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "=== Configuration ===\n",
      "Text dim: 768\n",
      "Graph dim: 512\n",
      "Num labels (organs): 12\n",
      "=== Batch Debug ===\n",
      "text_emb: torch.Size([32, 768]), dtype: torch.float32\n",
      "graph_emb: torch.Size([32, 512]), dtype: torch.float32\n",
      "label: torch.Size([32, 12]), dtype: torch.float32\n",
      "label values: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get dimensions\n",
    "sample_batch = next(iter(train_loader))\n",
    "text_dim = sample_batch['text_emb'].shape[1]  # 768\n",
    "graph_dim = sample_batch['graph_emb'].shape[1]  # 512\n",
    "num_labels = sample_batch['label'].shape[1]  # 12\n",
    "\n",
    "print(f\"\\n=== Configuration ===\")\n",
    "print(f\"Text dim: {text_dim}\")\n",
    "print(f\"Graph dim: {graph_dim}\")\n",
    "print(f\"Num labels (organs): {num_labels}\")\n",
    "\n",
    "\n",
    "# Initialize model (you can pass init_gate_pref to bias initial gate, and force_ratio to force global ratio)\n",
    "model = AttentionFusionMultiLabel(\n",
    "    gat_target_ratio=0.5,\n",
    "    strategy= 'gat_priority',\n",
    "    \n",
    "    )\n",
    "model = model.to(device)\n",
    "\n",
    "# Chạy này trước khi train để kiểm tra\n",
    "batch = next(iter(train_loader))\n",
    "print(\"=== Batch Debug ===\")\n",
    "print(f\"text_emb: {batch['text_emb'].shape}, dtype: {batch['text_emb'].dtype}\")\n",
    "print(f\"graph_emb: {batch['graph_emb'].shape}, dtype: {batch['graph_emb'].dtype}\")\n",
    "print(f\"label: {batch['label'].shape}, dtype: {batch['label'].dtype}\")\n",
    "print(f\"label values: {batch['label'][:5]}\")  # First 5 labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d502292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label frequencies (train):\n",
      "  Organ 0: 236 samples (3.7%)\n",
      "  Organ 1: 178 samples (2.8%)\n",
      "  Organ 2: 638 samples (10.0%)\n",
      "  Organ 3: 250 samples (3.9%)\n",
      "  Organ 4: 626 samples (9.8%)\n",
      "  Organ 5: 279 samples (4.4%)\n",
      "  Organ 6: 141 samples (2.2%)\n",
      "  Organ 7: 765 samples (11.9%)\n",
      "  Organ 8: 208 samples (3.2%)\n",
      "  Organ 9: 295 samples (4.6%)\n",
      "  Organ 10: 763 samples (11.9%)\n",
      "  Organ 11: 340 samples (5.3%)\n",
      "\n",
      "Total parameters: 529,949\n",
      "\n",
      "================================================================================\n",
      "=== Training Started (Variance-Aware) ===\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "=== Training Started (Variance-Aware) ===\n",
      "================================================================================\n",
      "Epoch   1 | Loss: 0.5391 | F1-M: 0.8382 | BERT: 0.999±0.002 | GAT: 0.001±0.002 | Var: 0.2488 | α=0.000\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.002\n",
      "    Best F1: 0.8382 | GAT: 0.1%\n",
      "Epoch   1 | Loss: 0.5391 | F1-M: 0.8382 | BERT: 0.999±0.002 | GAT: 0.001±0.002 | Var: 0.2488 | α=0.000\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.002\n",
      "    Best F1: 0.8382 | GAT: 0.1%\n",
      "Epoch   2 | Loss: 0.1501 | F1-M: 0.8977 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2498 | α=0.002\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.004\n",
      "    Best F1: 0.8977 | GAT: 0.0%\n",
      "Epoch   2 | Loss: 0.1501 | F1-M: 0.8977 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2498 | α=0.002\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.004\n",
      "    Best F1: 0.8977 | GAT: 0.0%\n",
      "Epoch   3 | Loss: 0.0898 | F1-M: 0.9339 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2499 | α=0.004\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.006\n",
      "    Best F1: 0.9339 | GAT: 0.0%\n",
      "Epoch   3 | Loss: 0.0898 | F1-M: 0.9339 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2499 | α=0.004\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.006\n",
      "    Best F1: 0.9339 | GAT: 0.0%\n",
      "Epoch   4 | Loss: 0.0658 | F1-M: 0.9473 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2499 | α=0.006\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.008\n",
      "    Best F1: 0.9473 | GAT: 0.0%\n",
      "Epoch   4 | Loss: 0.0658 | F1-M: 0.9473 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2499 | α=0.006\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.008\n",
      "    Best F1: 0.9473 | GAT: 0.0%\n",
      "Epoch   5 | Loss: 0.0508 | F1-M: 0.9536 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.008\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.010\n",
      "    Best F1: 0.9536 | GAT: 0.0%\n",
      "Epoch   5 | Loss: 0.0508 | F1-M: 0.9536 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.008\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.010\n",
      "    Best F1: 0.9536 | GAT: 0.0%\n",
      "Epoch   6 | Loss: 0.0422 | F1-M: 0.9609 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.010\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.012\n",
      "    Best F1: 0.9609 | GAT: 0.0%\n",
      "Epoch   6 | Loss: 0.0422 | F1-M: 0.9609 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.010\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.012\n",
      "    Best F1: 0.9609 | GAT: 0.0%\n",
      "Epoch   7 | Loss: 0.0366 | F1-M: 0.9611 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.012\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.014\n",
      "    Best F1: 0.9611 | GAT: 0.0%\n",
      "Epoch   7 | Loss: 0.0366 | F1-M: 0.9611 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.012\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.014\n",
      "    Best F1: 0.9611 | GAT: 0.0%\n",
      "Epoch   8 | Loss: 0.0327 | F1-M: 0.9707 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.014\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.016\n",
      "    Best F1: 0.9707 | GAT: 0.0%\n",
      "Epoch   8 | Loss: 0.0327 | F1-M: 0.9707 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.014\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.016\n",
      "    Best F1: 0.9707 | GAT: 0.0%\n",
      "Epoch   9 | Loss: 0.0284 | F1-M: 0.9705 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.016\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.018\n",
      "Epoch   9 | Loss: 0.0284 | F1-M: 0.9705 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.016\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.018\n",
      "Epoch  10 | Loss: 0.0286 | F1-M: 0.9728 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.018\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.020\n",
      "    Best F1: 0.9728 | GAT: 0.0%\n",
      "Epoch  10 | Loss: 0.0286 | F1-M: 0.9728 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.018\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.020\n",
      "    Best F1: 0.9728 | GAT: 0.0%\n",
      "Epoch  11 | Loss: 0.0253 | F1-M: 0.9736 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.020\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.022\n",
      "    Best F1: 0.9736 | GAT: 0.0%\n",
      "Epoch  11 | Loss: 0.0253 | F1-M: 0.9736 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.020\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.022\n",
      "    Best F1: 0.9736 | GAT: 0.0%\n",
      "Epoch  12 | Loss: 0.0225 | F1-M: 0.9758 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.022\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.024\n",
      "    Best F1: 0.9758 | GAT: 0.0%\n",
      "Epoch  12 | Loss: 0.0225 | F1-M: 0.9758 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.022\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.024\n",
      "    Best F1: 0.9758 | GAT: 0.0%\n",
      "Epoch  13 | Loss: 0.0198 | F1-M: 0.9822 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.024\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.026\n",
      "    Best F1: 0.9822 | GAT: 0.0%\n",
      "Epoch  13 | Loss: 0.0198 | F1-M: 0.9822 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.024\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.026\n",
      "    Best F1: 0.9822 | GAT: 0.0%\n",
      "Epoch  14 | Loss: 0.0183 | F1-M: 0.9832 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.026\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.028\n",
      "    Best F1: 0.9832 | GAT: 0.0%\n",
      "Epoch  14 | Loss: 0.0183 | F1-M: 0.9832 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.026\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.028\n",
      "    Best F1: 0.9832 | GAT: 0.0%\n",
      "Epoch  15 | Loss: 0.0187 | F1-M: 0.9772 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.028\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  15 | Loss: 0.0187 | F1-M: 0.9772 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.028\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  16 | Loss: 0.0172 | F1-M: 0.9779 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  16 | Loss: 0.0172 | F1-M: 0.9779 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  17 | Loss: 0.0157 | F1-M: 0.9852 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "    Best F1: 0.9852 | GAT: 0.0%\n",
      "Epoch  17 | Loss: 0.0157 | F1-M: 0.9852 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "    Best F1: 0.9852 | GAT: 0.0%\n",
      "Epoch  18 | Loss: 0.0143 | F1-M: 0.9728 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  18 | Loss: 0.0143 | F1-M: 0.9728 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  19 | Loss: 0.0157 | F1-M: 0.9873 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "    Best F1: 0.9873 | GAT: 0.0%\n",
      "Epoch  19 | Loss: 0.0157 | F1-M: 0.9873 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "    Best F1: 0.9873 | GAT: 0.0%\n",
      "Epoch  20 | Loss: 0.0150 | F1-M: 0.9866 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  20 | Loss: 0.0150 | F1-M: 0.9866 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  21 | Loss: 0.0140 | F1-M: 0.9840 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  21 | Loss: 0.0140 | F1-M: 0.9840 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  22 | Loss: 0.0146 | F1-M: 0.9805 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  22 | Loss: 0.0146 | F1-M: 0.9805 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  23 | Loss: 0.0134 | F1-M: 0.9880 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "    Best F1: 0.9880 | GAT: 0.0%\n",
      "Epoch  23 | Loss: 0.0134 | F1-M: 0.9880 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "    Best F1: 0.9880 | GAT: 0.0%\n",
      "Epoch  24 | Loss: 0.0117 | F1-M: 0.9842 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  24 | Loss: 0.0117 | F1-M: 0.9842 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  25 | Loss: 0.0119 | F1-M: 0.9815 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  25 | Loss: 0.0119 | F1-M: 0.9815 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  26 | Loss: 0.0130 | F1-M: 0.9847 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  26 | Loss: 0.0130 | F1-M: 0.9847 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  27 | Loss: 0.0120 | F1-M: 0.9837 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  27 | Loss: 0.0120 | F1-M: 0.9837 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  28 | Loss: 0.0119 | F1-M: 0.9879 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  28 | Loss: 0.0119 | F1-M: 0.9879 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  29 | Loss: 0.0123 | F1-M: 0.9802 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  29 | Loss: 0.0123 | F1-M: 0.9802 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  30 | Loss: 0.0109 | F1-M: 0.9844 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  30 | Loss: 0.0109 | F1-M: 0.9844 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  31 | Loss: 0.0091 | F1-M: 0.9873 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  31 | Loss: 0.0091 | F1-M: 0.9873 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  32 | Loss: 0.0101 | F1-M: 0.9824 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  32 | Loss: 0.0101 | F1-M: 0.9824 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2500 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  33 | Loss: 0.0113 | F1-M: 0.9845 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2499 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  33 | Loss: 0.0113 | F1-M: 0.9845 | BERT: 1.000±0.000 | GAT: 0.000±0.000 | Var: 0.2499 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  34 | Loss: 0.0097 | F1-M: 0.9856 | BERT: 0.664±0.171 | GAT: 0.336±0.171 | Var: 0.0561 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  34 | Loss: 0.0097 | F1-M: 0.9856 | BERT: 0.664±0.171 | GAT: 0.336±0.171 | Var: 0.0561 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  35 | Loss: 0.0110 | F1-M: 0.9873 | BERT: 0.754±0.129 | GAT: 0.246±0.129 | Var: 0.0811 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  35 | Loss: 0.0110 | F1-M: 0.9873 | BERT: 0.754±0.129 | GAT: 0.246±0.129 | Var: 0.0811 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  36 | Loss: 0.0093 | F1-M: 0.9871 | BERT: 0.641±0.171 | GAT: 0.359±0.171 | Var: 0.0491 | α=0.030\n",
      "Epoch  36 | Loss: 0.0093 | F1-M: 0.9871 | BERT: 0.641±0.171 | GAT: 0.359±0.171 | Var: 0.0491 | α=0.030\n",
      "Epoch  37 | Loss: 0.0108 | F1-M: 0.9913 | BERT: 0.619±0.194 | GAT: 0.381±0.194 | Var: 0.0518 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "    Best F1: 0.9913 | GAT: 38.1%\n",
      "Epoch  37 | Loss: 0.0108 | F1-M: 0.9913 | BERT: 0.619±0.194 | GAT: 0.381±0.194 | Var: 0.0518 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "    Best F1: 0.9913 | GAT: 38.1%\n",
      "Epoch  38 | Loss: 0.0104 | F1-M: 0.9882 | BERT: 0.602±0.186 | GAT: 0.398±0.186 | Var: 0.0449 | α=0.030\n",
      "Epoch  38 | Loss: 0.0104 | F1-M: 0.9882 | BERT: 0.602±0.186 | GAT: 0.398±0.186 | Var: 0.0449 | α=0.030\n",
      "Epoch  39 | Loss: 0.0095 | F1-M: 0.9818 | BERT: 0.611±0.180 | GAT: 0.389±0.180 | Var: 0.0448 | α=0.030\n",
      "Epoch  39 | Loss: 0.0095 | F1-M: 0.9818 | BERT: 0.611±0.180 | GAT: 0.389±0.180 | Var: 0.0448 | α=0.030\n",
      "Epoch  40 | Loss: 0.0103 | F1-M: 0.9894 | BERT: 0.597±0.187 | GAT: 0.403±0.187 | Var: 0.0444 | α=0.030\n",
      "Epoch  40 | Loss: 0.0103 | F1-M: 0.9894 | BERT: 0.597±0.187 | GAT: 0.403±0.187 | Var: 0.0444 | α=0.030\n",
      "Epoch  41 | Loss: 0.0083 | F1-M: 0.9893 | BERT: 0.682±0.153 | GAT: 0.318±0.153 | Var: 0.0563 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  41 | Loss: 0.0083 | F1-M: 0.9893 | BERT: 0.682±0.153 | GAT: 0.318±0.153 | Var: 0.0563 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  42 | Loss: 0.0087 | F1-M: 0.9859 | BERT: 0.667±0.168 | GAT: 0.333±0.168 | Var: 0.0562 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  42 | Loss: 0.0087 | F1-M: 0.9859 | BERT: 0.667±0.168 | GAT: 0.333±0.168 | Var: 0.0562 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  43 | Loss: 0.0083 | F1-M: 0.9868 | BERT: 0.676±0.154 | GAT: 0.324±0.154 | Var: 0.0545 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  43 | Loss: 0.0083 | F1-M: 0.9868 | BERT: 0.676±0.154 | GAT: 0.324±0.154 | Var: 0.0545 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  44 | Loss: 0.0074 | F1-M: 0.9845 | BERT: 0.595±0.184 | GAT: 0.405±0.184 | Var: 0.0428 | α=0.030\n",
      "Epoch  44 | Loss: 0.0074 | F1-M: 0.9845 | BERT: 0.595±0.184 | GAT: 0.405±0.184 | Var: 0.0428 | α=0.030\n",
      "Epoch  45 | Loss: 0.0077 | F1-M: 0.9885 | BERT: 0.601±0.175 | GAT: 0.399±0.175 | Var: 0.0406 | α=0.030\n",
      "Epoch  45 | Loss: 0.0077 | F1-M: 0.9885 | BERT: 0.601±0.175 | GAT: 0.399±0.175 | Var: 0.0406 | α=0.030\n",
      "Epoch  46 | Loss: 0.0076 | F1-M: 0.9786 | BERT: 0.659±0.156 | GAT: 0.341±0.156 | Var: 0.0497 | α=0.030\n",
      "Epoch  46 | Loss: 0.0076 | F1-M: 0.9786 | BERT: 0.659±0.156 | GAT: 0.341±0.156 | Var: 0.0497 | α=0.030\n",
      "Epoch  47 | Loss: 0.0092 | F1-M: 0.9819 | BERT: 0.609±0.183 | GAT: 0.391±0.183 | Var: 0.0453 | α=0.030\n",
      "Epoch  47 | Loss: 0.0092 | F1-M: 0.9819 | BERT: 0.609±0.183 | GAT: 0.391±0.183 | Var: 0.0453 | α=0.030\n",
      "Epoch  48 | Loss: 0.0093 | F1-M: 0.9832 | BERT: 0.583±0.189 | GAT: 0.417±0.189 | Var: 0.0425 | α=0.030\n",
      "Epoch  48 | Loss: 0.0093 | F1-M: 0.9832 | BERT: 0.583±0.189 | GAT: 0.417±0.189 | Var: 0.0425 | α=0.030\n",
      "Epoch  49 | Loss: 0.0070 | F1-M: 0.9872 | BERT: 0.612±0.170 | GAT: 0.388±0.170 | Var: 0.0415 | α=0.030\n",
      "Epoch  49 | Loss: 0.0070 | F1-M: 0.9872 | BERT: 0.612±0.170 | GAT: 0.388±0.170 | Var: 0.0415 | α=0.030\n",
      "Epoch  50 | Loss: 0.0073 | F1-M: 0.9871 | BERT: 0.612±0.162 | GAT: 0.388±0.162 | Var: 0.0389 | α=0.030\n",
      "Epoch  50 | Loss: 0.0073 | F1-M: 0.9871 | BERT: 0.612±0.162 | GAT: 0.388±0.162 | Var: 0.0389 | α=0.030\n",
      "Epoch  51 | Loss: 0.0062 | F1-M: 0.9913 | BERT: 0.589±0.168 | GAT: 0.411±0.168 | Var: 0.0359 | α=0.030\n",
      "    Best F1: 0.9913 | GAT: 41.1%\n",
      "Epoch  51 | Loss: 0.0062 | F1-M: 0.9913 | BERT: 0.589±0.168 | GAT: 0.411±0.168 | Var: 0.0359 | α=0.030\n",
      "    Best F1: 0.9913 | GAT: 41.1%\n",
      "Epoch  52 | Loss: 0.0072 | F1-M: 0.9827 | BERT: 0.617±0.166 | GAT: 0.383±0.166 | Var: 0.0412 | α=0.030\n",
      "Epoch  52 | Loss: 0.0072 | F1-M: 0.9827 | BERT: 0.617±0.166 | GAT: 0.383±0.166 | Var: 0.0412 | α=0.030\n",
      "Epoch  53 | Loss: 0.0086 | F1-M: 0.9863 | BERT: 0.588±0.173 | GAT: 0.412±0.173 | Var: 0.0377 | α=0.030\n",
      "Epoch  53 | Loss: 0.0086 | F1-M: 0.9863 | BERT: 0.588±0.173 | GAT: 0.412±0.173 | Var: 0.0377 | α=0.030\n",
      "Epoch  54 | Loss: 0.0077 | F1-M: 0.9771 | BERT: 0.604±0.170 | GAT: 0.396±0.170 | Var: 0.0398 | α=0.030\n",
      "Epoch  54 | Loss: 0.0077 | F1-M: 0.9771 | BERT: 0.604±0.170 | GAT: 0.396±0.170 | Var: 0.0398 | α=0.030\n",
      "Epoch  55 | Loss: 0.0068 | F1-M: 0.9896 | BERT: 0.605±0.166 | GAT: 0.395±0.166 | Var: 0.0384 | α=0.030\n",
      "Epoch  55 | Loss: 0.0068 | F1-M: 0.9896 | BERT: 0.605±0.166 | GAT: 0.395±0.166 | Var: 0.0384 | α=0.030\n",
      "Epoch  56 | Loss: 0.0064 | F1-M: 0.9871 | BERT: 0.618±0.160 | GAT: 0.382±0.160 | Var: 0.0395 | α=0.030\n",
      "Epoch  56 | Loss: 0.0064 | F1-M: 0.9871 | BERT: 0.618±0.160 | GAT: 0.382±0.160 | Var: 0.0395 | α=0.030\n",
      "Epoch  57 | Loss: 0.0065 | F1-M: 0.9894 | BERT: 0.598±0.170 | GAT: 0.402±0.170 | Var: 0.0384 | α=0.030\n",
      "Epoch  57 | Loss: 0.0065 | F1-M: 0.9894 | BERT: 0.598±0.170 | GAT: 0.402±0.170 | Var: 0.0384 | α=0.030\n",
      "Epoch  58 | Loss: 0.0059 | F1-M: 0.9887 | BERT: 0.620±0.163 | GAT: 0.380±0.163 | Var: 0.0409 | α=0.030\n",
      "Epoch  58 | Loss: 0.0059 | F1-M: 0.9887 | BERT: 0.620±0.163 | GAT: 0.380±0.163 | Var: 0.0409 | α=0.030\n",
      "Epoch  59 | Loss: 0.0072 | F1-M: 0.9828 | BERT: 0.601±0.166 | GAT: 0.399±0.166 | Var: 0.0376 | α=0.030\n",
      "Epoch  59 | Loss: 0.0072 | F1-M: 0.9828 | BERT: 0.601±0.166 | GAT: 0.399±0.166 | Var: 0.0376 | α=0.030\n",
      "Epoch  60 | Loss: 0.0050 | F1-M: 0.9873 | BERT: 0.623±0.160 | GAT: 0.377±0.160 | Var: 0.0405 | α=0.030\n",
      "Epoch  60 | Loss: 0.0050 | F1-M: 0.9873 | BERT: 0.623±0.160 | GAT: 0.377±0.160 | Var: 0.0405 | α=0.030\n",
      "Epoch  61 | Loss: 0.0072 | F1-M: 0.9832 | BERT: 0.626±0.174 | GAT: 0.374±0.174 | Var: 0.0460 | α=0.030\n",
      "Epoch  61 | Loss: 0.0072 | F1-M: 0.9832 | BERT: 0.626±0.174 | GAT: 0.374±0.174 | Var: 0.0460 | α=0.030\n",
      "Epoch  62 | Loss: 0.0077 | F1-M: 0.9794 | BERT: 0.700±0.125 | GAT: 0.300±0.125 | Var: 0.0557 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  62 | Loss: 0.0077 | F1-M: 0.9794 | BERT: 0.700±0.125 | GAT: 0.300±0.125 | Var: 0.0557 | α=0.030\n",
      "   ℹ GAT low but variance good. Slight boost: α=0.030\n",
      "Epoch  63 | Loss: 0.0069 | F1-M: 0.9852 | BERT: 0.603±0.158 | GAT: 0.397±0.158 | Var: 0.0354 | α=0.030\n",
      "Epoch  63 | Loss: 0.0069 | F1-M: 0.9852 | BERT: 0.603±0.158 | GAT: 0.397±0.158 | Var: 0.0354 | α=0.030\n",
      "Epoch  64 | Loss: 0.0069 | F1-M: 0.9885 | BERT: 0.598±0.161 | GAT: 0.402±0.161 | Var: 0.0355 | α=0.030\n",
      "Epoch  64 | Loss: 0.0069 | F1-M: 0.9885 | BERT: 0.598±0.161 | GAT: 0.402±0.161 | Var: 0.0355 | α=0.030\n",
      "Epoch  65 | Loss: 0.0053 | F1-M: 0.9903 | BERT: 0.595±0.161 | GAT: 0.405±0.161 | Var: 0.0347 | α=0.030\n",
      "Epoch  65 | Loss: 0.0053 | F1-M: 0.9903 | BERT: 0.595±0.161 | GAT: 0.405±0.161 | Var: 0.0347 | α=0.030\n",
      "Epoch  66 | Loss: 0.0049 | F1-M: 0.9877 | BERT: 0.605±0.158 | GAT: 0.395±0.158 | Var: 0.0359 | α=0.030\n",
      "Epoch  66 | Loss: 0.0049 | F1-M: 0.9877 | BERT: 0.605±0.158 | GAT: 0.395±0.158 | Var: 0.0359 | α=0.030\n",
      "Epoch  67 | Loss: 0.0053 | F1-M: 0.9904 | BERT: 0.627±0.155 | GAT: 0.373±0.155 | Var: 0.0403 | α=0.030\n",
      "Epoch  67 | Loss: 0.0053 | F1-M: 0.9904 | BERT: 0.627±0.155 | GAT: 0.373±0.155 | Var: 0.0403 | α=0.030\n",
      "Epoch  68 | Loss: 0.0050 | F1-M: 0.9887 | BERT: 0.605±0.167 | GAT: 0.395±0.167 | Var: 0.0390 | α=0.030\n",
      "Epoch  68 | Loss: 0.0050 | F1-M: 0.9887 | BERT: 0.605±0.167 | GAT: 0.395±0.167 | Var: 0.0390 | α=0.030\n",
      "Epoch  69 | Loss: 0.0060 | F1-M: 0.9878 | BERT: 0.593±0.167 | GAT: 0.407±0.167 | Var: 0.0365 | α=0.030\n",
      "Epoch  69 | Loss: 0.0060 | F1-M: 0.9878 | BERT: 0.593±0.167 | GAT: 0.407±0.167 | Var: 0.0365 | α=0.030\n",
      "Epoch  70 | Loss: 0.0063 | F1-M: 0.9856 | BERT: 0.593±0.166 | GAT: 0.407±0.166 | Var: 0.0363 | α=0.030\n",
      "Epoch  70 | Loss: 0.0063 | F1-M: 0.9856 | BERT: 0.593±0.166 | GAT: 0.407±0.166 | Var: 0.0363 | α=0.030\n",
      "Epoch  71 | Loss: 0.0059 | F1-M: 0.9857 | BERT: 0.597±0.167 | GAT: 0.403±0.167 | Var: 0.0374 | α=0.030\n",
      "\n",
      "Early stopping at epoch 71\n",
      "\n",
      "================================================================================\n",
      "=== Final Test Results ===\n",
      "================================================================================\n",
      "\n",
      "Classification Metrics:\n",
      "   Exact Match:      0.9875\n",
      "   F1-Macro:         0.9895\n",
      "   F1-Micro:         0.9907\n",
      "   AUC-Macro:        1.0000\n",
      "\n",
      "Weight Distribution (= healthy):\n",
      "   BERT: 0.597 ± 0.1723\n",
      "   GAT:  0.403 ± 0.1723\n",
      "   Mean Variance: 0.0391 ✓\n",
      "\n",
      "================================================================================\n",
      " GAT at 40.3% - increase balance_alpha slightly\n",
      "Epoch  71 | Loss: 0.0059 | F1-M: 0.9857 | BERT: 0.597±0.167 | GAT: 0.403±0.167 | Var: 0.0374 | α=0.030\n",
      "\n",
      "Early stopping at epoch 71\n",
      "\n",
      "================================================================================\n",
      "=== Final Test Results ===\n",
      "================================================================================\n",
      "\n",
      "Classification Metrics:\n",
      "   Exact Match:      0.9875\n",
      "   F1-Macro:         0.9895\n",
      "   F1-Micro:         0.9907\n",
      "   AUC-Macro:        1.0000\n",
      "\n",
      "Weight Distribution (= healthy):\n",
      "   BERT: 0.597 ± 0.1723\n",
      "   GAT:  0.403 ± 0.1723\n",
      "   Mean Variance: 0.0391 ✓\n",
      "\n",
      "================================================================================\n",
      " GAT at 40.3% - increase balance_alpha slightly\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, hamming_loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device, balance_alpha=0.0):\n",
    "    \"\"\"\n",
    "    Train WITHOUT over-regularization - let router learn naturally\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_balance_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        text_emb = batch['text_emb'].to(device)\n",
    "        graph_emb = batch['graph_emb'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits, weights = model(text_emb, graph_emb)  # [batch, 12], [batch, 1, 2]\n",
    "        \n",
    "        # Classification loss (MAIN)\n",
    "        ce_loss = criterion(logits, labels)\n",
    "        \n",
    "        # ===== LIGHT balance regularization =====\n",
    "        # Only gently encourage toward 50-50, don't force it\n",
    "        balance_loss = torch.tensor(0.0, device=device)\n",
    "        if balance_alpha > 0:\n",
    "            w = weights.squeeze(1)  # [batch, 2]\n",
    "            # Target: weights should have variance > 0\n",
    "            # But also: don't stray too far from 50-50\n",
    "            target = torch.ones_like(w) * 0.5\n",
    "            balance_loss = torch.mean((w - target) ** 2)  # MSE, not KL\n",
    "        \n",
    "        total_weighted_loss = ce_loss + balance_alpha * balance_loss\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        total_weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += ce_loss.item()\n",
    "        total_balance_loss += balance_loss.item()\n",
    "    \n",
    "    return total_loss / len(loader), total_balance_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate_with_variance(model, loader, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate + compute weight statistics including variance\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_weights = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            text_emb = batch['text_emb'].to(device)\n",
    "            graph_emb = batch['graph_emb'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits, weights = model(text_emb, graph_emb)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > threshold).float()\n",
    "            \n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_weights.append(weights.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    all_probs = torch.cat(all_probs, dim=0).numpy()\n",
    "    all_weights = torch.cat(all_weights, dim=0)  # [N, 1, 2]\n",
    "    \n",
    "    # Classification metrics\n",
    "    exact_match = accuracy_score(all_labels, all_preds)\n",
    "    hamming = hamming_loss(all_labels, all_preds)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        auc_macro = roc_auc_score(all_labels, all_probs, average='macro')\n",
    "    except:\n",
    "        auc_macro = 0.0\n",
    "    \n",
    "    # ===== Weight statistics =====\n",
    "    w = all_weights.squeeze(1)  # [N, 2]\n",
    "    \n",
    "    # Per-sample variance (should be > 0 if router is learning)\n",
    "    sample_variance = ((w - 0.5) ** 2).mean(dim=1)  # [N]\n",
    "    mean_variance = sample_variance.mean().item()\n",
    "    \n",
    "    # Weight statistics\n",
    "    bert_weights = w[:, 0]\n",
    "    gat_weights = w[:, 1]\n",
    "    \n",
    "    return {\n",
    "        'exact_match': exact_match,\n",
    "        'hamming_loss': hamming,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'auc_macro': auc_macro,\n",
    "        'avg_bert_weight': bert_weights.mean().item(),\n",
    "        'avg_gat_weight': gat_weights.mean().item(),\n",
    "        'bert_std': bert_weights.std().item(),\n",
    "        'gat_std': gat_weights.std().item(),\n",
    "        'mean_variance': mean_variance,  # ← KEY METRIC\n",
    "        'weights_tensor': w\n",
    "    }\n",
    "\n",
    "\n",
    "# ============ Setup ============\n",
    "\n",
    "all_labels = torch.cat([batch['label'] for batch in train_loader], dim=0)\n",
    "label_freq = all_labels.sum(dim=0)\n",
    "print(f\"\\nLabel frequencies (train):\")\n",
    "for i, freq in enumerate(label_freq):\n",
    "    print(f\"  Organ {i}: {freq.item():.0f} samples ({freq.item()/len(all_labels)*100:.1f}%)\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "pos_weight = (len(all_labels) - label_freq) / (label_freq + 1e-6)\n",
    "pos_weight = pos_weight.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# ============ Training (FIXED) ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== Training Started (Variance-Aware) ===\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_val_f1 = 0\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "\n",
    "# Start with NO balance loss - let router learn first\n",
    "# Gradually introduce balance loss if weights collapse\n",
    "balance_alpha = 0.0\n",
    "min_variance_threshold = 0.001  # If variance drops below this, increase alpha\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss, train_balance = train_epoch(\n",
    "        model, train_loader, optimizer, criterion, device,\n",
    "        balance_alpha=balance_alpha\n",
    "    )\n",
    "    val_metrics = evaluate_with_variance(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:3d} | \"\n",
    "          f\"Loss: {train_loss:.4f} | \"\n",
    "          f\"F1-M: {val_metrics['f1_macro']:.4f} | \"\n",
    "          f\"BERT: {val_metrics['avg_bert_weight']:.3f}±{val_metrics['bert_std']:.3f} | \"\n",
    "          f\"GAT: {val_metrics['avg_gat_weight']:.3f}±{val_metrics['gat_std']:.3f} | \"\n",
    "          f\"Var: {val_metrics['mean_variance']:.4f} | \"\n",
    "          f\"α={balance_alpha:.3f}\")\n",
    "    \n",
    "    # ===== Dynamic balance alpha adjustment =====\n",
    "    # If variance is TOO LOW, increase regularization\n",
    "    if val_metrics['mean_variance'] < min_variance_threshold and epoch > 5:\n",
    "        balance_alpha = min(balance_alpha + 0.01, 0.05)\n",
    "        print(f\"    Variance too low! Increasing α to {balance_alpha:.3f}\")\n",
    "    \n",
    "    # If variance is HIGH and GAT < 45%, increase balance gently\n",
    "    elif val_metrics['mean_variance'] > 0.05 and val_metrics['avg_gat_weight'] < 0.45:\n",
    "        balance_alpha = min(balance_alpha + 0.002, 0.03)\n",
    "        print(f\"   ℹ GAT low but variance good. Slight boost: α={balance_alpha:.3f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_metrics['f1_macro'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['f1_macro']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_metrics': val_metrics,\n",
    "        }, 'best_fusion_multilabel_model.pt')\n",
    "        patience_counter = 0\n",
    "        print(f\"    Best F1: {best_val_f1:.4f} | GAT: {val_metrics['avg_gat_weight']:.1%}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "# ============ Test ============\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== Final Test Results ===\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checkpoint = torch.load('best_fusion_multilabel_model.pt', map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "test_metrics = evaluate_with_variance(model, test_loader, device)\n",
    "\n",
    "print(f\"\\nClassification Metrics:\")\n",
    "print(f\"   Exact Match:      {test_metrics['exact_match']:.4f}\")\n",
    "print(f\"   F1-Macro:         {test_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   F1-Micro:         {test_metrics['f1_micro']:.4f}\")\n",
    "print(f\"   AUC-Macro:        {test_metrics['auc_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\nWeight Distribution (= healthy):\")\n",
    "print(f\"   BERT: {test_metrics['avg_bert_weight']:.3f} ± {test_metrics['bert_std']:.4f}\")\n",
    "print(f\"   GAT:  {test_metrics['avg_gat_weight']:.3f} ± {test_metrics['gat_std']:.4f}\")\n",
    "print(f\"   Mean Variance: {test_metrics['mean_variance']:.4f} {'✓' if test_metrics['mean_variance'] > 0.001 else '✗'}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "if test_metrics['avg_gat_weight'] >= 0.45 and test_metrics['mean_variance'] > 0.001:\n",
    "    print(f\" SUCCESS: GAT {test_metrics['avg_gat_weight']:.1%} with healthy variance!\")\n",
    "elif test_metrics['mean_variance'] < 0.001:\n",
    "    print(f\" ISSUE: Variance too low - router not learning to differentiate\")\n",
    "    print(f\"  Solutions:\")\n",
    "    print(f\"    1. Reduce balance_alpha initial value\")\n",
    "    print(f\"    2. Use force_ratio=0.5 strategy for hard constraint\")\n",
    "    print(f\"    3. Check if router gradients are flowing properly\")\n",
    "else:\n",
    "    print(f\" GAT at {test_metrics['avg_gat_weight']:.1%} - increase balance_alpha slightly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0d570d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BERT contribution: 0.5885030627250671\n",
      " GAT contribution : 0.41149693727493286\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_weights = []\n",
    "\n",
    "for batch in val_loader:\n",
    "    text_emb = batch['text_emb'].to(device)\n",
    "    graph_emb = batch['graph_emb'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, attn = model(text_emb, graph_emb)\n",
    "    \n",
    "    # attn shape = [batch, 1, 2]  => squeeze thành [batch, 2]\n",
    "    all_weights.append(attn.squeeze(1).cpu())\n",
    "\n",
    "all_weights = torch.cat(all_weights, dim=0)\n",
    "\n",
    "bert_contrib = all_weights[:, 0].mean().item()\n",
    "gat_contrib  = all_weights[:, 1].mean().item()\n",
    "\n",
    "print(\" BERT contribution:\", bert_contrib)\n",
    "print(\" GAT contribution :\", gat_contrib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b84edfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance: tensor([0.0281, 0.0281])\n"
     ]
    }
   ],
   "source": [
    "print(\"Variance:\", all_weights.var(dim=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HD_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
