{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de80b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49201d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constant\n",
    "TEST_EMBEDDING_FILE = \"test_embeddings.pt\"\n",
    "TRAIN_EMBEDDING_FILE = \"train_embeddings.pt\"\n",
    "VAL_EMBEDDING_FILE = \"val_embeddings.pt\"\n",
    "\n",
    "GNN_TEST_EMBEDDING = f\"../GATNN/embeddings/{TEST_EMBEDDING_FILE}\"\n",
    "GNN_TRAIN_EMBEDDING = f\"../GATNN/embeddings/{TRAIN_EMBEDDING_FILE}\"\n",
    "GNN_VAL_EMBEDDING = f\"../GATNN/embeddings/{VAL_EMBEDDING_FILE}\"\n",
    "BERT_TEST_EMBEDDING = f\"../BERT/embedding/{TEST_EMBEDDING_FILE}\"\n",
    "BERT_TRAIN_EMBEDDING = f\"../BERT/embedding/{TRAIN_EMBEDDING_FILE}\"\n",
    "BERT_VAL_EMBEDDING = f\"../BERT/embedding/{VAL_EMBEDDING_FILE}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b29f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNN_FILE = \"gat_embeddings.pt\"\n",
    "BERT_FILE = \"all_embeddings.pt\"\n",
    "GNN_EMBEDDING = f\"../GATNN/embeddings/{GNN_FILE}\"\n",
    "BERT_EMBEDDING = f\"../BERT/embedding/{BERT_FILE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df0c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_file(path):\n",
    "    \"\"\"\n",
    "    Hàm load file embedding từ đường dẫn `path`.\n",
    "    Tự động nhận dạng nhiều kiểu dữ liệu khác nhau:\n",
    "    - dict có key 'embeddings' hoặc 'emb', 'vectors', 'features'\n",
    "    - trực tiếp là Tensor\n",
    "    - list/tuple (chuyển sang Tensor)\n",
    "    \"\"\"\n",
    "    # ---- Bỏ chặn numpy pickle cổ ----\n",
    "    try:\n",
    "        torch.serialization.add_safe_globals([np.core.multiarray._reconstruct])\n",
    "    except Exception:\n",
    "        pass  # không sao\n",
    "\n",
    "    # Load thử với weights_only=False ----\n",
    "    try:\n",
    "        data = torch.load(path, weights_only=False)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LỖI load file embedding: {e}\")\n",
    "    \n",
    "    # data = torch.load(path, map_location='cpu')\n",
    "\n",
    "    #Chuẩn hóa về tensor ----\n",
    "    # Nếu là tensor trực tiếp\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return {'embeddings': data}\n",
    "    \n",
    "\n",
    "    # Nếu là numpy\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return {'embeddings': torch.tensor(data)}\n",
    "    \n",
    "\n",
    "    # Trường hợp 1: File lưu dạng dict\n",
    "    if isinstance(data, dict):\n",
    "\n",
    "        # TH1.1: Dict có key phổ biến 'embeddings'\n",
    "        if 'embeddings' in data:\n",
    "            return {\n",
    "                'embeddings': torch.as_tensor(data['embeddings']),\n",
    "                'ids': data.get('mol_ids', None)  or data.get('mol_id',None) # có thể không có\n",
    "            }\n",
    "\n",
    "        # TH1.2: Một số file embedding dùng key khác\n",
    "        for key in ['emb', 'vectors', 'features']:\n",
    "            if key in data:\n",
    "                return {\n",
    "                    'embeddings': torch.as_tensor(data[key]),\n",
    "                    'ids': data.get('mol_ids', None) or data.get('mol_id',None)\n",
    "                }\n",
    "\n",
    "        # TH1.3: Dict không rõ cấu trúc → thử convert cả dict sang tensor\n",
    "        try:\n",
    "            return {'embeddings': torch.as_tensor(data)}\n",
    "        except Exception:\n",
    "            raise ValueError(\n",
    "                f\"Lỗi: Dict trong file {path} có cấu trúc không hỗ trợ để chuyển sang Tensor\"\n",
    "            )\n",
    "\n",
    "    \n",
    "\n",
    "    # Trường hợp 3: File là list hoặc tuple → convert sang Tensor\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        try:\n",
    "            return {'embeddings': torch.as_tensor(data)}\n",
    "        except Exception:\n",
    "            raise ValueError(\n",
    "                f\"Lỗi: Không thể chuyển list/tuple trong file {path} sang Tensor\"\n",
    "            )\n",
    "\n",
    "    # Trường hợp không thuộc loại nào\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Lỗi: Định dạng dữ liệu trong file {path} không được hỗ trợ\"\n",
    "            f\"Loại: {type(data)}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73042a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KIỂM TRA BERT BRANCH==================================\n",
      "KIỂM TRA FILE: ../BERT/embedding/test_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (802, 768)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../BERT/embedding/train_embeddings.pt ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10648\\1766450209.py:11: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  torch.serialization.add_safe_globals([np.core.multiarray._reconstruct])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅File hợp lệ!\n",
      "Kích thước: (6411, 768)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../BERT/embedding/val_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (801, 768)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../BERT/embedding/all_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (8014, 768)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA GNN BRANCH====================================\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/test_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (801, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/train_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (6404, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/val_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (801, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/gat_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (8006, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_embedding_file(path):\n",
    "    \"\"\"Kiểm tra 1 file embedding và trả về True/False + in thông tin chi tiết.\"\"\"\n",
    "\n",
    "    print(f\"KIỂM TRA FILE: {path} ======\")\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Lỗi: File không tồn tại.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        result = load_embedding_file(path)\n",
    "        emb = result[\"embeddings\"]\n",
    "\n",
    "        # --- Kiểm tra embedding có phải Tensor ---\n",
    "        if not isinstance(emb, torch.Tensor):\n",
    "            print(\"Lỗi: embeddings không phải Torch.Tensor.\")\n",
    "            return False\n",
    "\n",
    "        # --- Kiểm tra số chiều ---\n",
    "        if emb.ndim < 2:\n",
    "            print(f\"Lỗi: embeddings phải >= 2 chiều, hiện tại: {emb.ndim}\")\n",
    "            return False\n",
    "\n",
    "        # --- In thông tin hợp lệ ---\n",
    "        print(\"✅File hợp lệ!\")\n",
    "        print(f\"Kích thước: {tuple(emb.shape)}___Kiểu dữ liệu: {emb.dtype}___Có IDs không? { 'Có' if result.get('ids') is not None else 'Không' }\")\n",
    "       \n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi load file: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_all_embeddings():\n",
    "    \"\"\"Test toàn bộ file BERT + GNN\"\"\"\n",
    "\n",
    "    bert_files = [\n",
    "        BERT_TEST_EMBEDDING,\n",
    "        BERT_TRAIN_EMBEDDING,\n",
    "        BERT_VAL_EMBEDDING,\n",
    "        BERT_EMBEDDING\n",
    "    ]\n",
    "\n",
    "    gnn_files = [\n",
    "        GNN_TEST_EMBEDDING,\n",
    "        GNN_TRAIN_EMBEDDING,\n",
    "        GNN_VAL_EMBEDDING,\n",
    "        GNN_EMBEDDING\n",
    "    ]\n",
    "\n",
    "   \n",
    "    print(\"KIỂM TRA BERT BRANCH==================================\")\n",
    "  \n",
    "\n",
    "    for f in bert_files:\n",
    "        check_embedding_file(f)\n",
    "\n",
    "    \n",
    "    print(\"KIỂM TRA GNN BRANCH====================================\")\n",
    "\n",
    "    for f in gnn_files:\n",
    "        check_embedding_file(f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_all_embeddings()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d87694bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load BERT embeddings\n",
    "bert_train = torch.load(BERT_TRAIN_EMBEDDING, weights_only=False)\n",
    "bert_val = torch.load(BERT_VAL_EMBEDDING, weights_only=False)\n",
    "bert_test = torch.load(BERT_TEST_EMBEDDING, weights_only=False)\n",
    "\n",
    "# Load GAT embeddings\n",
    "gat_train = torch.load(GNN_TRAIN_EMBEDDING, weights_only=False)\n",
    "gat_val = torch.load(GNN_VAL_EMBEDDING, weights_only=False)\n",
    "gat_test = torch.load(GNN_TEST_EMBEDDING, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c187148",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_all = torch.load(BERT_EMBEDDING, weights_only=False)\n",
    "gat_all = torch.load(GNN_EMBEDDING, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24793905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: Số lượng không khớp! BERT=6411, GAT=6404\n",
      "Train: Mol IDs KHÔNG khớp, cần align lại!\n",
      "Val: Mol IDs KHÔNG khớp, cần align lại!\n",
      " Test: Số lượng không khớp! BERT=802, GAT=801\n",
      "Test: Mol IDs KHÔNG khớp, cần align lại!\n",
      " All Data: Số lượng không khớp! BERT=8014, GAT=8006\n",
      "All Data: Mol IDs KHÔNG khớp, cần align lại!\n"
     ]
    }
   ],
   "source": [
    "def check_alignment(bert_data, gat_data, split_name):\n",
    "    \"\"\"Kiểm tra xem mol_ids có khớp nhau không\"\"\"\n",
    "    bert_ids = bert_data['mol_id']  # hoặc 'mol_ids'\n",
    "    gat_ids = gat_data['mol_ids']\n",
    "    \n",
    "    # So sánh\n",
    "    if len(bert_ids) != len(gat_ids):\n",
    "        print(f\" {split_name}: Số lượng không khớp! BERT={len(bert_ids)}, GAT={len(gat_ids)}\")\n",
    "    \n",
    "    # Kiểm tra thứ tự\n",
    "    if isinstance(bert_ids[0], str) and isinstance(gat_ids[0], str):\n",
    "        match = all(t == g for t, g in zip(bert_ids, gat_ids))\n",
    "    else:\n",
    "        match = torch.equal(bert_ids, gat_ids)\n",
    "    \n",
    "    if match:\n",
    "        print(f\" {split_name}: Mol IDs khớp hoàn toàn!\")\n",
    "    else:\n",
    "        print(f\"{split_name}: Mol IDs KHÔNG khớp, cần align lại!\")\n",
    "    \n",
    "    return match\n",
    "\n",
    "# Check tất cả splits\n",
    "train_match = check_alignment(bert_train, gat_train, \"Train\")\n",
    "val_match = check_alignment(bert_val, gat_val, \"Val\")\n",
    "test_match = check_alignment(bert_test, gat_test, \"Test\")\n",
    "all_match = check_alignment(bert_all, gat_all, \"All Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f891e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, bert_data, gat_data):\n",
    "        # Embeddings\n",
    "        self.text_embs = bert_data['embeddings']\n",
    "        self.graph_embs = gat_data['embeddings']\n",
    "        \n",
    "        # Labels (lấy từ 1 trong 2, giả sử giống nhau)\n",
    "        self.labels = bert_data['labels']\n",
    "        \n",
    "        # Optional: mol_ids để track\n",
    "        self.mol_ids = bert_data['mol_id']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text_emb': self.text_embs[idx],\n",
    "            'graph_emb': self.graph_embs[idx],\n",
    "            'label': self.labels[idx],\n",
    "            'mol_id': self.mol_ids[idx]\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecbee277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT có 8014 mẫu\n",
      "GAT  có 8006 mẫu\n",
      "Trùng mol_id: 8006 mẫu\n"
     ]
    }
   ],
   "source": [
    "def align_datasets(bert_data, gat_data):\n",
    "    \"\"\"Align BERT và GAT theo mol_id (không kiểm tra labels).\"\"\"\n",
    "\n",
    "    bert_ids = bert_data['mol_id']\n",
    "    gat_ids  = gat_data['mol_ids']\n",
    "\n",
    "    # Tạo dictionary để tìm index nhanh\n",
    "    bert_dict = {str(m): i for i, m in enumerate(bert_ids)}\n",
    "    gat_dict  = {str(m): i for i, m in enumerate(gat_ids)}\n",
    "\n",
    "    # Lấy phần giao của mol_id\n",
    "    common_ids = sorted(set(bert_dict.keys()) & set(gat_dict.keys()))\n",
    "\n",
    "    print(f\"BERT có {len(bert_ids)} mẫu\")\n",
    "    print(f\"GAT  có {len(gat_ids)} mẫu\")\n",
    "    print(f\"Trùng mol_id: {len(common_ids)} mẫu\")\n",
    "\n",
    "    bert_indices = [bert_dict[m] for m in common_ids]\n",
    "    gat_indices  = [gat_dict[m] for m in common_ids]\n",
    "\n",
    "    # Embeddings\n",
    "    bert_embs = bert_data[\"embeddings\"][bert_indices]\n",
    "    gat_embs  = gat_data[\"embeddings\"][gat_indices]\n",
    "\n",
    "    # Labels\n",
    "    bert_labels = bert_data[\"labels\"][bert_indices]\n",
    "    gat_labels  = gat_data[\"labels\"][gat_indices]\n",
    "\n",
    "    aligned_bert = {\n",
    "        \"embeddings\": bert_embs,\n",
    "        \"labels\": bert_labels,\n",
    "        \"mol_id\": common_ids\n",
    "    }\n",
    "\n",
    "    aligned_gat = {\n",
    "        \"embeddings\": gat_embs,\n",
    "        \"labels\": gat_labels,\n",
    "        \"mol_ids\": common_ids\n",
    "    }\n",
    "\n",
    "    return aligned_bert, aligned_gat\n",
    "\n",
    "\n",
    "# Align từng split\n",
    "\n",
    "all_bert_aligned, all_gat_aligned = align_datasets(bert_all, gat_all)\n",
    "# Tạo datasets\n",
    "\n",
    "\n",
    "# print(f\"\\n Datasets created:\")\n",
    "# print(f\"Train: {len(train_dataset)} samples\")\n",
    "# print(f\"Val: {len(val_dataset)} samples\")\n",
    "# print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b14fea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total aligned samples: 8006\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N = len(all_bert_aligned[\"mol_id\"])\n",
    "print(\"Total aligned samples:\", N)\n",
    "indices = np.arange(N)\n",
    "\n",
    "print(type(all_bert_aligned[\"labels\"]))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c4dee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Datasets created:\n",
      "Train: 6404 samples\n",
      "Val: 801 samples\n",
      "Test: 801 samples\n"
     ]
    }
   ],
   "source": [
    "def split_data(aligned_data, idx, mol_id):\n",
    "    return {\n",
    "        \"embeddings\": aligned_data[\"embeddings\"][idx],\n",
    "        \"labels\": aligned_data[\"labels\"][idx],\n",
    "        \"mol_id\": [aligned_data[mol_id][i] for i in idx]\n",
    "    }\n",
    "train_bert_aligned = split_data(all_bert_aligned, train_idx, \"mol_id\")\n",
    "val_bert_aligned = split_data(all_bert_aligned, val_idx, \"mol_id\")\n",
    "test_bert_aligned = split_data(all_bert_aligned, test_idx, \"mol_id\")\n",
    "\n",
    "train_dataset = FusionDataset(train_bert_aligned, split_data(all_gat_aligned, train_idx, \"mol_ids\"))\n",
    "val_dataset = FusionDataset(val_bert_aligned, split_data(all_gat_aligned, val_idx, \"mol_ids\"))\n",
    "test_dataset = FusionDataset(test_bert_aligned, split_data(all_gat_aligned, test_idx, \"mol_ids\"))\n",
    "print(f\"\\n Datasets created:\")\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1868d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Missing Samples Analysis (ALL) ===\n"
     ]
    }
   ],
   "source": [
    "# Check xem có pattern nào không\n",
    "def analyze_missing_samples(toxbert_data, gat_data, split_name):\n",
    "    \"\"\"Phân tích samples bị missing\"\"\"\n",
    "    toxbert_ids = set(str(x) for x in toxbert_data['mol_id'])\n",
    "    gat_ids = set(str(x) for x in gat_data['mol_ids'])\n",
    "    \n",
    "    only_toxbert = list(toxbert_ids - gat_ids)\n",
    "    only_gat = list(gat_ids - toxbert_ids)\n",
    "    \n",
    "    print(f\"\\n=== Missing Samples Analysis ({split_name}) ===\")\n",
    "    \n",
    "    if only_toxbert:\n",
    "        print(f\"\\nSamples only in ToxBERT (first 10):\")\n",
    "        for mol_id in only_toxbert:\n",
    "            idx = [i for i, x in enumerate(toxbert_data['mol_id']) if str(x) == mol_id][0]\n",
    "            label = toxbert_data['labels'][idx]\n",
    "            print(f\"  {mol_id}: label shape = {label.shape if torch.is_tensor(label) else 'N/A'}\")\n",
    "    \n",
    "    if only_gat:\n",
    "        print(f\"\\nSamples only in GAT (first 10):\")\n",
    "        for mol_id in only_gat:\n",
    "            idx = [i for i, x in enumerate(gat_data['mol_ids']) if str(x) == mol_id][0]\n",
    "            label = gat_data['labels'][idx]\n",
    "            print(f\"  {mol_id}: label shape = {label.shape if torch.is_tensor(label) else 'N/A'}\")\n",
    "\n",
    "analyze_missing_samples(all_bert_aligned, all_gat_aligned, \"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8920b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFusionMultiLabel(nn.Module):\n",
    "    \"\"\"Multi-label classification version\"\"\"\n",
    "    def __init__(self, text_dim=768, graph_dim=512, num_labels=12):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Projection layers\n",
    "        hidden_dim = 256\n",
    "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
    "        self.graph_proj = nn.Linear(graph_dim, hidden_dim)\n",
    "        \n",
    "        # Learnable query for attention\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Multi-label classifier (sigmoid for each label)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)  # Output: [batch, 12]\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_emb, graph_emb):\n",
    "        batch_size = text_emb.size(0)\n",
    "        \n",
    "        # Project to same dimension\n",
    "        text_proj = self.text_proj(text_emb)\n",
    "        graph_proj = self.graph_proj(graph_emb)\n",
    "        \n",
    "        # Stack: [B, 2, hidden_dim]\n",
    "        embeddings = torch.stack([text_proj, graph_proj], dim=1)\n",
    "        \n",
    "        # Attention fusion\n",
    "        query = self.query.expand(batch_size, -1, -1)\n",
    "        fused, attention_weights = self.attention(query, embeddings, embeddings)\n",
    "        fused = fused.squeeze(1)\n",
    "        \n",
    "        # Multi-label logits (no softmax, use sigmoid later)\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fc0b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6404 samples\n",
      "Val: 801 samples\n",
      "Test: 801 samples\n"
     ]
    }
   ],
   "source": [
    "# Tạo DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37a03af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dimension Check ===\n",
      "Text embedding shape: torch.Size([32, 768])\n",
      "Graph embedding shape: torch.Size([32, 512])\n",
      "Label shape: torch.Size([32, 12])\n",
      "Label unique values: tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra trước khi train\n",
    "print(\"=== Dimension Check ===\")\n",
    "sample = next(iter(train_loader))\n",
    "print(f\"Text embedding shape: {sample['text_emb'].shape}\")\n",
    "print(f\"Graph embedding shape: {sample['graph_emb'].shape}\")\n",
    "print(f\"Label shape: {sample['label'].shape}\")\n",
    "print(f\"Label unique values: {torch.unique(sample['label'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47d8b10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "=== Configuration ===\n",
      "Text dim: 768\n",
      "Graph dim: 512\n",
      "Num labels (organs): 12\n",
      "=== Batch Debug ===\n",
      "text_emb: torch.Size([32, 768]), dtype: torch.float32\n",
      "graph_emb: torch.Size([32, 512]), dtype: torch.float32\n",
      "label: torch.Size([32, 12]), dtype: torch.float32\n",
      "label values: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "logits shape: torch.Size([32, 12])\n",
      "Expected: [batch_size, num_classes]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get dimensions\n",
    "sample_batch = next(iter(train_loader))\n",
    "text_dim = sample_batch['text_emb'].shape[1]  # 768\n",
    "graph_dim = sample_batch['graph_emb'].shape[1]  # 512\n",
    "num_labels = sample_batch['label'].shape[1]  # 12\n",
    "\n",
    "print(f\"\\n=== Configuration ===\")\n",
    "print(f\"Text dim: {text_dim}\")\n",
    "print(f\"Graph dim: {graph_dim}\")\n",
    "print(f\"Num labels (organs): {num_labels}\")\n",
    "\n",
    "# Initialize model\n",
    "model = AttentionFusionMultiLabel(\n",
    "    text_dim=text_dim, \n",
    "    graph_dim=graph_dim, \n",
    "    num_labels=num_labels\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Chạy này trước khi train để kiểm tra\n",
    "batch = next(iter(train_loader))\n",
    "print(\"=== Batch Debug ===\")\n",
    "print(f\"text_emb: {batch['text_emb'].shape}, dtype: {batch['text_emb'].dtype}\")\n",
    "print(f\"graph_emb: {batch['graph_emb'].shape}, dtype: {batch['graph_emb'].dtype}\")\n",
    "print(f\"label: {batch['label'].shape}, dtype: {batch['label'].dtype}\")\n",
    "print(f\"label values: {batch['label'][:5]}\")  # First 5 labels\n",
    "\n",
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits, _ = model(batch['text_emb'].to(device), batch['graph_emb'].to(device))\n",
    "    print(f\"logits shape: {logits.shape}\")\n",
    "    print(f\"Expected: [batch_size, num_classes]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d502292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label frequencies (train):\n",
      "  Organ 0: 236 samples (3.7%)\n",
      "  Organ 1: 178 samples (2.8%)\n",
      "  Organ 2: 638 samples (10.0%)\n",
      "  Organ 3: 250 samples (3.9%)\n",
      "  Organ 4: 626 samples (9.8%)\n",
      "  Organ 5: 279 samples (4.4%)\n",
      "  Organ 6: 141 samples (2.2%)\n",
      "  Organ 7: 765 samples (11.9%)\n",
      "  Organ 8: 208 samples (3.2%)\n",
      "  Organ 9: 295 samples (4.6%)\n",
      "  Organ 10: 763 samples (11.9%)\n",
      "  Organ 11: 340 samples (5.3%)\n",
      "\n",
      "Total parameters: 626,572\n",
      "\n",
      "Pos weights (for imbalanced labels): tensor([26.1356, 34.9775,  9.0376, 24.6160,  9.2300, 21.9534, 44.4184,  7.3712,\n",
      "        29.7885, 20.7085,  7.3932, 17.8353])\n",
      "\n",
      "=== Training Started ===\n",
      "Epoch   1 | Loss: 0.4967 | EM: 0.8764 | Hamming: 0.0241 | F1-Macro: 0.8217 | F1-Micro: 0.8366 | AUC: 0.9997\n",
      "   Saved best model with F1-Macro: 0.8217\n",
      "Epoch   2 | Loss: 0.1608 | EM: 0.9251 | Hamming: 0.0160 | F1-Macro: 0.8668 | F1-Micro: 0.8852 | AUC: 0.9998\n",
      "   Saved best model with F1-Macro: 0.8668\n",
      "Epoch   3 | Loss: 0.0996 | EM: 0.9426 | Hamming: 0.0086 | F1-Macro: 0.9206 | F1-Micro: 0.9347 | AUC: 0.9998\n",
      "   Saved best model with F1-Macro: 0.9206\n",
      "Epoch   4 | Loss: 0.0711 | EM: 0.9576 | Hamming: 0.0054 | F1-Macro: 0.9486 | F1-Micro: 0.9580 | AUC: 0.9998\n",
      "   Saved best model with F1-Macro: 0.9486\n",
      "Epoch   5 | Loss: 0.0585 | EM: 0.9563 | Hamming: 0.0053 | F1-Macro: 0.9475 | F1-Micro: 0.9588 | AUC: 0.9999\n",
      "Epoch   6 | Loss: 0.0462 | EM: 0.9376 | Hamming: 0.0076 | F1-Macro: 0.9264 | F1-Micro: 0.9421 | AUC: 0.9997\n",
      "Epoch   7 | Loss: 0.0396 | EM: 0.9613 | Hamming: 0.0047 | F1-Macro: 0.9562 | F1-Micro: 0.9635 | AUC: 0.9999\n",
      "   Saved best model with F1-Macro: 0.9562\n",
      "\n",
      "=== Final Evaluation ===\n",
      " Test Results:\n",
      "   Exact Match: 0.9538\n",
      "   Hamming Loss: 0.0063\n",
      "   F1-Macro: 0.9520\n",
      "   F1-Micro: 0.9545\n",
      "   AUC-Macro: 0.9999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, hamming_loss\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        text_emb = batch['text_emb'].to(device)\n",
    "        graph_emb = batch['graph_emb'].to(device)\n",
    "        labels = batch['label'].to(device)  # [batch, 12]\n",
    "        \n",
    "        # Forward\n",
    "        logits, _ = model(text_emb, graph_emb)  # [batch, 12]\n",
    "        \n",
    "        # Multi-label loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            text_emb = batch['text_emb'].to(device)\n",
    "            graph_emb = batch['graph_emb'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits, _ = model(text_emb, graph_emb)\n",
    "            probs = torch.sigmoid(logits)  # [batch, 12] in range [0, 1]\n",
    "            preds = (probs > threshold).float()  # Binary predictions\n",
    "            \n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()  # [N, 12]\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()  # [N, 12]\n",
    "    all_probs = torch.cat(all_probs, dim=0).numpy()  # [N, 12]\n",
    "    \n",
    "    # Multi-label metrics\n",
    "    # 1. Exact match ratio (all labels correct)\n",
    "    exact_match = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # 2. Hamming loss (average per-label error)\n",
    "    hamming = hamming_loss(all_labels, all_preds)\n",
    "    \n",
    "    # 3. Macro F1 (average F1 across labels)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # 4. Micro F1 (global F1)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    \n",
    "    # 5. AUC-ROC per label (if possible)\n",
    "    try:\n",
    "        auc_macro = roc_auc_score(all_labels, all_probs, average='macro')\n",
    "    except:\n",
    "        auc_macro = 0.0\n",
    "    \n",
    "    return {\n",
    "        'exact_match': exact_match,\n",
    "        'hamming_loss': hamming,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'auc_macro': auc_macro\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check label distribution\n",
    "all_labels = torch.cat([batch['label'] for batch in train_loader], dim=0)\n",
    "label_freq = all_labels.sum(dim=0)\n",
    "print(f\"\\nLabel frequencies (train):\")\n",
    "for i, freq in enumerate(label_freq):\n",
    "    print(f\"  Organ {i}: {freq.item():.0f} samples ({freq.item()/len(all_labels)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Multi-label loss: BCEWithLogitsLoss (combines sigmoid + BCE)\n",
    "# Use pos_weight to handle class imbalance\n",
    "pos_weight = (len(all_labels) - label_freq) / (label_freq + 1e-6)\n",
    "pos_weight = pos_weight.to(device)\n",
    "print(f\"\\nPos weights (for imbalanced labels): {pos_weight}\")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n=== Training Started ===\")\n",
    "best_val_f1 = 0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(7):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_metrics = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:3d} | \"\n",
    "          f\"Loss: {train_loss:.4f} | \"\n",
    "          f\"EM: {val_metrics['exact_match']:.4f} | \"\n",
    "          f\"Hamming: {val_metrics['hamming_loss']:.4f} | \"\n",
    "          f\"F1-Macro: {val_metrics['f1_macro']:.4f} | \"\n",
    "          f\"F1-Micro: {val_metrics['f1_micro']:.4f} | \"\n",
    "          f\"AUC: {val_metrics['auc_macro']:.4f}\")\n",
    "    \n",
    "    # Early stopping based on macro F1\n",
    "    if val_metrics['f1_macro'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['f1_macro']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_metrics': val_metrics,\n",
    "        }, 'best_fusion_multilabel_model.pt')\n",
    "        patience_counter = 0\n",
    "        print(f\"   Saved best model with F1-Macro: {best_val_f1:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Test evaluation\n",
    "print(\"\\n=== Final Evaluation ===\")\n",
    "checkpoint = torch.load('best_fusion_multilabel_model.pt', map_location=device, weights_only = False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "test_metrics = evaluate(model, test_loader, device)\n",
    "print(f\" Test Results:\")\n",
    "print(f\"   Exact Match: {test_metrics['exact_match']:.4f}\")\n",
    "print(f\"   Hamming Loss: {test_metrics['hamming_loss']:.4f}\")\n",
    "print(f\"   F1-Macro: {test_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   F1-Micro: {test_metrics['f1_micro']:.4f}\")\n",
    "print(f\"   AUC-Macro: {test_metrics['auc_macro']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
