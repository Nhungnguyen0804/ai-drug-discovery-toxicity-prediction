{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de80b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49201d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constant\n",
    "TEST_EMBEDDING_FILE = \"test_embeddings.pt\"\n",
    "TRAIN_EMBEDDING_FILE = \"train_embeddings.pt\"\n",
    "VAL_EMBEDDING_FILE = \"val_embeddings.pt\"\n",
    "\n",
    "GNN_TEST_EMBEDDING = f\"../GATNN/embeddings/{TEST_EMBEDDING_FILE}\"\n",
    "GNN_TRAIN_EMBEDDING = f\"../GATNN/embeddings/{TRAIN_EMBEDDING_FILE}\"\n",
    "GNN_VAL_EMBEDDING = f\"../GATNN/embeddings/{VAL_EMBEDDING_FILE}\"\n",
    "BERT_TEST_EMBEDDING = f\"../BERT/embedding/{TEST_EMBEDDING_FILE}\"\n",
    "BERT_TRAIN_EMBEDDING = f\"../BERT/embedding/{TRAIN_EMBEDDING_FILE}\"\n",
    "BERT_VAL_EMBEDDING = f\"../BERT/embedding/{VAL_EMBEDDING_FILE}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df0c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_file(path):\n",
    "    \"\"\"\n",
    "    Hàm load file embedding từ đường dẫn `path`.\n",
    "    Tự động nhận dạng nhiều kiểu dữ liệu khác nhau:\n",
    "    - dict có key 'embeddings' hoặc 'emb', 'vectors', 'features'\n",
    "    - trực tiếp là Tensor\n",
    "    - list/tuple (chuyển sang Tensor)\n",
    "    \"\"\"\n",
    "    # ---- Bỏ chặn numpy pickle cổ ----\n",
    "    try:\n",
    "        torch.serialization.add_safe_globals([np.core.multiarray._reconstruct])\n",
    "    except Exception:\n",
    "        pass  # không sao\n",
    "\n",
    "    # Load thử với weights_only=False ----\n",
    "    try:\n",
    "        data = torch.load(path, weights_only=False)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LỖI load file embedding: {e}\")\n",
    "    \n",
    "    # data = torch.load(path, map_location='cpu')\n",
    "\n",
    "    #Chuẩn hóa về tensor ----\n",
    "    # Nếu là tensor trực tiếp\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return {'embeddings': data}\n",
    "    \n",
    "\n",
    "    # Nếu là numpy\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return {'embeddings': torch.tensor(data)}\n",
    "    \n",
    "\n",
    "    # Trường hợp 1: File lưu dạng dict\n",
    "    if isinstance(data, dict):\n",
    "\n",
    "        # TH1.1: Dict có key phổ biến 'embeddings'\n",
    "        if 'embeddings' in data:\n",
    "            return {\n",
    "                'embeddings': torch.as_tensor(data['embeddings']),\n",
    "                'ids': data.get('mol_ids', None)  or data.get('mol_id',None) # có thể không có\n",
    "            }\n",
    "\n",
    "        # TH1.2: Một số file embedding dùng key khác\n",
    "        for key in ['emb', 'vectors', 'features']:\n",
    "            if key in data:\n",
    "                return {\n",
    "                    'embeddings': torch.as_tensor(data[key]),\n",
    "                    'ids': data.get('mol_ids', None) or data.get('mol_id',None)\n",
    "                }\n",
    "\n",
    "        # TH1.3: Dict không rõ cấu trúc → thử convert cả dict sang tensor\n",
    "        try:\n",
    "            return {'embeddings': torch.as_tensor(data)}\n",
    "        except Exception:\n",
    "            raise ValueError(\n",
    "                f\"Lỗi: Dict trong file {path} có cấu trúc không hỗ trợ để chuyển sang Tensor\"\n",
    "            )\n",
    "\n",
    "    \n",
    "\n",
    "    # Trường hợp 3: File là list hoặc tuple → convert sang Tensor\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        try:\n",
    "            return {'embeddings': torch.as_tensor(data)}\n",
    "        except Exception:\n",
    "            raise ValueError(\n",
    "                f\"Lỗi: Không thể chuyển list/tuple trong file {path} sang Tensor\"\n",
    "            )\n",
    "\n",
    "    # Trường hợp không thuộc loại nào\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Lỗi: Định dạng dữ liệu trong file {path} không được hỗ trợ\"\n",
    "            f\"Loại: {type(data)}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73042a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KIỂM TRA BERT BRANCH==================================\n",
      "KIỂM TRA FILE: ../BERT/embedding/test_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (802, 768)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../BERT/embedding/train_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (6411, 768)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../BERT/embedding/val_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (801, 768)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA GNN BRANCH====================================\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/test_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (801, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/train_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (6404, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n",
      "KIỂM TRA FILE: ../GATNN/embeddings/val_embeddings.pt ======\n",
      "✅File hợp lệ!\n",
      "Kích thước: (801, 512)___Kiểu dữ liệu: torch.float32___Có IDs không? Có\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_embedding_file(path):\n",
    "    \"\"\"Kiểm tra 1 file embedding và trả về True/False + in thông tin chi tiết.\"\"\"\n",
    "\n",
    "    print(f\"KIỂM TRA FILE: {path} ======\")\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Lỗi: File không tồn tại.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        result = load_embedding_file(path)\n",
    "        emb = result[\"embeddings\"]\n",
    "\n",
    "        # --- Kiểm tra embedding có phải Tensor ---\n",
    "        if not isinstance(emb, torch.Tensor):\n",
    "            print(\"Lỗi: embeddings không phải Torch.Tensor.\")\n",
    "            return False\n",
    "\n",
    "        # --- Kiểm tra số chiều ---\n",
    "        if emb.ndim < 2:\n",
    "            print(f\"Lỗi: embeddings phải >= 2 chiều, hiện tại: {emb.ndim}\")\n",
    "            return False\n",
    "\n",
    "        # --- In thông tin hợp lệ ---\n",
    "        print(\"✅File hợp lệ!\")\n",
    "        print(f\"Kích thước: {tuple(emb.shape)}___Kiểu dữ liệu: {emb.dtype}___Có IDs không? { 'Có' if result.get('ids') is not None else 'Không' }\")\n",
    "       \n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi load file: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_all_embeddings():\n",
    "    \"\"\"Test toàn bộ file BERT + GNN\"\"\"\n",
    "\n",
    "    bert_files = [\n",
    "        BERT_TEST_EMBEDDING,\n",
    "        BERT_TRAIN_EMBEDDING,\n",
    "        BERT_VAL_EMBEDDING,\n",
    "    ]\n",
    "\n",
    "    gnn_files = [\n",
    "        GNN_TEST_EMBEDDING,\n",
    "        GNN_TRAIN_EMBEDDING,\n",
    "        GNN_VAL_EMBEDDING,\n",
    "    ]\n",
    "\n",
    "   \n",
    "    print(\"KIỂM TRA BERT BRANCH==================================\")\n",
    "  \n",
    "\n",
    "    for f in bert_files:\n",
    "        check_embedding_file(f)\n",
    "\n",
    "    \n",
    "    print(\"KIỂM TRA GNN BRANCH====================================\")\n",
    "\n",
    "    for f in gnn_files:\n",
    "        check_embedding_file(f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_all_embeddings()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d87694bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load BERT embeddings\n",
    "bert_train = torch.load(BERT_TRAIN_EMBEDDING, weights_only=False)\n",
    "bert_val = torch.load(BERT_VAL_EMBEDDING, weights_only=False)\n",
    "bert_test = torch.load(BERT_TEST_EMBEDDING, weights_only=False)\n",
    "\n",
    "# Load GAT embeddings\n",
    "gat_train = torch.load(GNN_TRAIN_EMBEDDING, weights_only=False)\n",
    "gat_val = torch.load(GNN_VAL_EMBEDDING, weights_only=False)\n",
    "gat_test = torch.load(GNN_TEST_EMBEDDING, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24793905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: Số lượng không khớp! BERT=6411, GAT=6404\n",
      "Train: Mol IDs KHÔNG khớp, cần align lại!\n",
      "Val: Mol IDs KHÔNG khớp, cần align lại!\n",
      " Test: Số lượng không khớp! BERT=802, GAT=801\n",
      "Test: Mol IDs KHÔNG khớp, cần align lại!\n"
     ]
    }
   ],
   "source": [
    "def check_alignment(bert_data, gat_data, split_name):\n",
    "    \"\"\"Kiểm tra xem mol_ids có khớp nhau không\"\"\"\n",
    "    bert_ids = bert_data['mol_id']  # hoặc 'mol_ids'\n",
    "    gat_ids = gat_data['mol_ids']\n",
    "    \n",
    "    # So sánh\n",
    "    if len(bert_ids) != len(gat_ids):\n",
    "        print(f\" {split_name}: Số lượng không khớp! BERT={len(bert_ids)}, GAT={len(gat_ids)}\")\n",
    "    \n",
    "    # Kiểm tra thứ tự\n",
    "    if isinstance(bert_ids[0], str) and isinstance(gat_ids[0], str):\n",
    "        match = all(t == g for t, g in zip(bert_ids, gat_ids))\n",
    "    else:\n",
    "        match = torch.equal(bert_ids, gat_ids)\n",
    "    \n",
    "    if match:\n",
    "        print(f\" {split_name}: Mol IDs khớp hoàn toàn!\")\n",
    "    else:\n",
    "        print(f\"{split_name}: Mol IDs KHÔNG khớp, cần align lại!\")\n",
    "    \n",
    "    return match\n",
    "\n",
    "# Check tất cả splits\n",
    "train_match = check_alignment(bert_train, gat_train, \"Train\")\n",
    "val_match = check_alignment(bert_val, gat_val, \"Val\")\n",
    "test_match = check_alignment(bert_test, gat_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f891e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, bert_data, gat_data):\n",
    "        # Embeddings\n",
    "        self.text_embs = bert_data['embeddings']\n",
    "        self.graph_embs = gat_data['embeddings']\n",
    "        \n",
    "        # Labels (lấy từ 1 trong 2, giả sử giống nhau)\n",
    "        self.labels = bert_data['labels']\n",
    "        \n",
    "        # Optional: mol_ids để track\n",
    "        self.mol_ids = bert_data['mol_id']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text_emb': self.text_embs[idx],\n",
    "            'graph_emb': self.graph_embs[idx],\n",
    "            'label': self.labels[idx],\n",
    "            'mol_id': self.mol_ids[idx]\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecbee277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT có 6411 samples\n",
      "GAT có 6404 samples\n",
      "Chung: 5134 samples\n",
      "BERT có 801 samples\n",
      "GAT có 801 samples\n",
      "Chung: 80 samples\n",
      "BERT có 802 samples\n",
      "GAT có 801 samples\n",
      "Chung: 95 samples\n",
      "\n",
      " Datasets created:\n",
      "Train: 5134 samples\n",
      "Val: 80 samples\n",
      "Test: 95 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def align_datasets(bert_data, gat_data):\n",
    "    \"\"\"Align 2 datasets theo mol_id\"\"\"\n",
    "    bert_ids = bert_data['mol_id']\n",
    "    gat_ids = gat_data['mol_ids']\n",
    "    \n",
    "    # Tạo dictionaries\n",
    "    bert_dict = {str(mol_id): idx for idx, mol_id in enumerate(bert_ids)}\n",
    "    gat_dict = {str(mol_id): idx for idx, mol_id in enumerate(gat_ids)}\n",
    "    \n",
    "    # Tìm common IDs\n",
    "    common_ids = sorted(set(bert_dict.keys()) & set(gat_dict.keys()))\n",
    "    \n",
    "    print(f\"BERT có {len(bert_ids)} samples\")\n",
    "    print(f\"GAT có {len(gat_ids)} samples\")\n",
    "    print(f\"Chung: {len(common_ids)} samples\")\n",
    "    \n",
    "    # Extract aligned data\n",
    "    bert_indices = [bert_dict[mol_id] for mol_id in common_ids]\n",
    "    gat_indices = [gat_dict[mol_id] for mol_id in common_ids]\n",
    "    \n",
    "    # Convert to tensor nếu là list\n",
    "    bert_embeddings = bert_data['embeddings']\n",
    "    bert_labels = bert_data['labels']\n",
    "    gat_embeddings = gat_data['embeddings']\n",
    "    gat_labels = gat_data['labels']\n",
    "    \n",
    "    # Handle indexing based on type\n",
    "    if isinstance(bert_embeddings, list):\n",
    "        aligned_bert_embs = torch.stack([bert_embeddings[i] for i in bert_indices])\n",
    "    else:\n",
    "        aligned_bert_embs = bert_embeddings[bert_indices]\n",
    "    \n",
    "    if isinstance(bert_labels, list):\n",
    "        aligned_bert_labels = torch.tensor([bert_labels[i] for i in bert_indices])\n",
    "    else:\n",
    "        aligned_bert_labels = bert_labels[bert_indices]\n",
    "    \n",
    "    if isinstance(gat_embeddings, list):\n",
    "        aligned_gat_embs = torch.stack([gat_embeddings[i] for i in gat_indices])\n",
    "    else:\n",
    "        aligned_gat_embs = gat_embeddings[gat_indices]\n",
    "    \n",
    "    if isinstance(gat_labels, list):\n",
    "        aligned_gat_labels = torch.tensor([gat_labels[i] for i in gat_indices])\n",
    "    else:\n",
    "        aligned_gat_labels = gat_labels[gat_indices]\n",
    "    \n",
    "    aligned_bert = {\n",
    "        'embeddings': aligned_bert_embs,\n",
    "        'labels': aligned_bert_labels,\n",
    "        'mol_id': [bert_ids[i] for i in bert_indices]\n",
    "    }\n",
    "    \n",
    "    aligned_gat = {\n",
    "        'embeddings': aligned_gat_embs,\n",
    "        'labels': aligned_gat_labels,\n",
    "        'mol_ids': [gat_ids[i] for i in gat_indices]\n",
    "    }\n",
    "    \n",
    "    return aligned_bert, aligned_gat\n",
    "\n",
    "# Align từng split\n",
    "train_bert_aligned, train_gat_aligned = align_datasets(bert_train, gat_train)\n",
    "val_bert_aligned, val_gat_aligned = align_datasets(bert_val, gat_val)\n",
    "test_bert_aligned, test_gat_aligned = align_datasets(bert_test, gat_test)\n",
    "\n",
    "# Tạo datasets\n",
    "train_dataset = FusionDataset(train_bert_aligned, train_gat_aligned)\n",
    "val_dataset = FusionDataset(val_bert_aligned, val_gat_aligned)\n",
    "test_dataset = FusionDataset(test_bert_aligned, test_gat_aligned)\n",
    "\n",
    "print(f\"\\n Datasets created:\")\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8920b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFusionMultiLabel(nn.Module):\n",
    "    \"\"\"Multi-label classification version\"\"\"\n",
    "    def __init__(self, text_dim=768, graph_dim=512, num_labels=12):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Projection layers\n",
    "        hidden_dim = 256\n",
    "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
    "        self.graph_proj = nn.Linear(graph_dim, hidden_dim)\n",
    "        \n",
    "        # Learnable query for attention\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Multi-label classifier (sigmoid for each label)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)  # Output: [batch, 12]\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_emb, graph_emb):\n",
    "        batch_size = text_emb.size(0)\n",
    "        \n",
    "        # Project to same dimension\n",
    "        text_proj = self.text_proj(text_emb)\n",
    "        graph_proj = self.graph_proj(graph_emb)\n",
    "        \n",
    "        # Stack: [B, 2, hidden_dim]\n",
    "        embeddings = torch.stack([text_proj, graph_proj], dim=1)\n",
    "        \n",
    "        # Attention fusion\n",
    "        query = self.query.expand(batch_size, -1, -1)\n",
    "        fused, attention_weights = self.attention(query, embeddings, embeddings)\n",
    "        fused = fused.squeeze(1)\n",
    "        \n",
    "        # Multi-label logits (no softmax, use sigmoid later)\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fc0b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5134 samples\n",
      "Val: 80 samples\n",
      "Test: 95 samples\n"
     ]
    }
   ],
   "source": [
    "# Tạo DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37a03af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dimension Check ===\n",
      "Text embedding shape: torch.Size([32, 768])\n",
      "Graph embedding shape: torch.Size([32, 512])\n",
      "Label shape: torch.Size([32, 12])\n",
      "Label unique values: tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra trước khi train\n",
    "print(\"=== Dimension Check ===\")\n",
    "sample = next(iter(train_loader))\n",
    "print(f\"Text embedding shape: {sample['text_emb'].shape}\")\n",
    "print(f\"Graph embedding shape: {sample['graph_emb'].shape}\")\n",
    "print(f\"Label shape: {sample['label'].shape}\")\n",
    "print(f\"Label unique values: {torch.unique(sample['label'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47d8b10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "=== Configuration ===\n",
      "Text dim: 768\n",
      "Graph dim: 512\n",
      "Num labels (organs): 12\n",
      "=== Batch Debug ===\n",
      "text_emb: torch.Size([32, 768]), dtype: torch.float32\n",
      "graph_emb: torch.Size([32, 512]), dtype: torch.float32\n",
      "label: torch.Size([32, 12]), dtype: torch.float32\n",
      "label values: tensor([[0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "logits shape: torch.Size([32, 12])\n",
      "Expected: [batch_size, num_classes]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get dimensions\n",
    "sample_batch = next(iter(train_loader))\n",
    "text_dim = sample_batch['text_emb'].shape[1]  # 768\n",
    "graph_dim = sample_batch['graph_emb'].shape[1]  # 512\n",
    "num_labels = sample_batch['label'].shape[1]  # 12\n",
    "\n",
    "print(f\"\\n=== Configuration ===\")\n",
    "print(f\"Text dim: {text_dim}\")\n",
    "print(f\"Graph dim: {graph_dim}\")\n",
    "print(f\"Num labels (organs): {num_labels}\")\n",
    "\n",
    "# Initialize model\n",
    "model = AttentionFusionMultiLabel(\n",
    "    text_dim=text_dim, \n",
    "    graph_dim=graph_dim, \n",
    "    num_labels=num_labels\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Chạy này trước khi train để kiểm tra\n",
    "batch = next(iter(train_loader))\n",
    "print(\"=== Batch Debug ===\")\n",
    "print(f\"text_emb: {batch['text_emb'].shape}, dtype: {batch['text_emb'].dtype}\")\n",
    "print(f\"graph_emb: {batch['graph_emb'].shape}, dtype: {batch['graph_emb'].dtype}\")\n",
    "print(f\"label: {batch['label'].shape}, dtype: {batch['label'].dtype}\")\n",
    "print(f\"label values: {batch['label'][:5]}\")  # First 5 labels\n",
    "\n",
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits, _ = model(batch['text_emb'].to(device), batch['graph_emb'].to(device))\n",
    "    print(f\"logits shape: {logits.shape}\")\n",
    "    print(f\"Expected: [batch_size, num_classes]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d502292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label frequencies (train):\n",
      "  Organ 0: 203 samples (4.0%)\n",
      "  Organ 1: 166 samples (3.2%)\n",
      "  Organ 2: 498 samples (9.7%)\n",
      "  Organ 3: 188 samples (3.7%)\n",
      "  Organ 4: 494 samples (9.6%)\n",
      "  Organ 5: 225 samples (4.4%)\n",
      "  Organ 6: 121 samples (2.4%)\n",
      "  Organ 7: 606 samples (11.8%)\n",
      "  Organ 8: 171 samples (3.3%)\n",
      "  Organ 9: 237 samples (4.6%)\n",
      "  Organ 10: 591 samples (11.5%)\n",
      "  Organ 11: 279 samples (5.4%)\n",
      "\n",
      "Total parameters: 626,572\n",
      "\n",
      "Pos weights (for imbalanced labels): tensor([24.2906, 29.9277,  9.3092, 26.3085,  9.3927, 21.8178, 41.4298,  7.4719,\n",
      "        29.0234, 20.6624,  7.6870, 17.4014])\n",
      "\n",
      "=== Training Started ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ai-drug-discovery-toxicity-prediction\\HD_env\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Loss: 0.0392 | EM: 0.9875 | Hamming: 0.0010 | F1-Macro: 0.9048 | F1-Micro: 0.9867 | AUC: nan\n",
      "   Saved best model with F1-Macro: 0.9048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ai-drug-discovery-toxicity-prediction\\HD_env\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2 | Loss: 0.0291 | EM: 0.9875 | Hamming: 0.0010 | F1-Macro: 0.9048 | F1-Micro: 0.9867 | AUC: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ai-drug-discovery-toxicity-prediction\\HD_env\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3 | Loss: 0.0233 | EM: 0.9875 | Hamming: 0.0010 | F1-Macro: 0.9048 | F1-Micro: 0.9867 | AUC: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ai-drug-discovery-toxicity-prediction\\HD_env\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4 | Loss: 0.0225 | EM: 0.9750 | Hamming: 0.0021 | F1-Macro: 0.9004 | F1-Micro: 0.9737 | AUC: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ai-drug-discovery-toxicity-prediction\\HD_env\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5 | Loss: 0.0184 | EM: 0.9875 | Hamming: 0.0010 | F1-Macro: 0.9048 | F1-Micro: 0.9867 | AUC: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ai-drug-discovery-toxicity-prediction\\HD_env\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6 | Loss: 0.0184 | EM: 0.9750 | Hamming: 0.0021 | F1-Macro: 0.9004 | F1-Micro: 0.9737 | AUC: nan\n",
      "Epoch   7 | Loss: 0.0149 | EM: 0.9750 | Hamming: 0.0021 | F1-Macro: 0.9004 | F1-Micro: 0.9737 | AUC: nan\n",
      "\n",
      "=== Final Evaluation ===\n",
      " Test Results:\n",
      "   Exact Match: 0.9684\n",
      "   Hamming Loss: 0.0035\n",
      "   F1-Macro: 0.9599\n",
      "   F1-Micro: 0.9722\n",
      "   AUC-Macro: 0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ai-drug-discovery-toxicity-prediction\\HD_env\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, hamming_loss\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        text_emb = batch['text_emb'].to(device)\n",
    "        graph_emb = batch['graph_emb'].to(device)\n",
    "        labels = batch['label'].to(device)  # [batch, 12]\n",
    "        \n",
    "        # Forward\n",
    "        logits, _ = model(text_emb, graph_emb)  # [batch, 12]\n",
    "        \n",
    "        # Multi-label loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            text_emb = batch['text_emb'].to(device)\n",
    "            graph_emb = batch['graph_emb'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits, _ = model(text_emb, graph_emb)\n",
    "            probs = torch.sigmoid(logits)  # [batch, 12] in range [0, 1]\n",
    "            preds = (probs > threshold).float()  # Binary predictions\n",
    "            \n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()  # [N, 12]\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()  # [N, 12]\n",
    "    all_probs = torch.cat(all_probs, dim=0).numpy()  # [N, 12]\n",
    "    \n",
    "    # Multi-label metrics\n",
    "    # 1. Exact match ratio (all labels correct)\n",
    "    exact_match = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # 2. Hamming loss (average per-label error)\n",
    "    hamming = hamming_loss(all_labels, all_preds)\n",
    "    \n",
    "    # 3. Macro F1 (average F1 across labels)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # 4. Micro F1 (global F1)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    \n",
    "    # 5. AUC-ROC per label (if possible)\n",
    "    try:\n",
    "        auc_macro = roc_auc_score(all_labels, all_probs, average='macro')\n",
    "    except:\n",
    "        auc_macro = 0.0\n",
    "    \n",
    "    return {\n",
    "        'exact_match': exact_match,\n",
    "        'hamming_loss': hamming,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'auc_macro': auc_macro\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check label distribution\n",
    "all_labels = torch.cat([batch['label'] for batch in train_loader], dim=0)\n",
    "label_freq = all_labels.sum(dim=0)\n",
    "print(f\"\\nLabel frequencies (train):\")\n",
    "for i, freq in enumerate(label_freq):\n",
    "    print(f\"  Organ {i}: {freq.item():.0f} samples ({freq.item()/len(all_labels)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Multi-label loss: BCEWithLogitsLoss (combines sigmoid + BCE)\n",
    "# Use pos_weight to handle class imbalance\n",
    "pos_weight = (len(all_labels) - label_freq) / (label_freq + 1e-6)\n",
    "pos_weight = pos_weight.to(device)\n",
    "print(f\"\\nPos weights (for imbalanced labels): {pos_weight}\")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n=== Training Started ===\")\n",
    "best_val_f1 = 0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(7):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_metrics = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:3d} | \"\n",
    "          f\"Loss: {train_loss:.4f} | \"\n",
    "          f\"EM: {val_metrics['exact_match']:.4f} | \"\n",
    "          f\"Hamming: {val_metrics['hamming_loss']:.4f} | \"\n",
    "          f\"F1-Macro: {val_metrics['f1_macro']:.4f} | \"\n",
    "          f\"F1-Micro: {val_metrics['f1_micro']:.4f} | \"\n",
    "          f\"AUC: {val_metrics['auc_macro']:.4f}\")\n",
    "    \n",
    "    # Early stopping based on macro F1\n",
    "    if val_metrics['f1_macro'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['f1_macro']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_metrics': val_metrics,\n",
    "        }, 'best_fusion_multilabel_model.pt')\n",
    "        patience_counter = 0\n",
    "        print(f\"   Saved best model with F1-Macro: {best_val_f1:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Test evaluation\n",
    "print(\"\\n=== Final Evaluation ===\")\n",
    "checkpoint = torch.load('best_fusion_multilabel_model.pt', map_location=device, weights_only = False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "test_metrics = evaluate(model, test_loader, device)\n",
    "print(f\" Test Results:\")\n",
    "print(f\"   Exact Match: {test_metrics['exact_match']:.4f}\")\n",
    "print(f\"   Hamming Loss: {test_metrics['hamming_loss']:.4f}\")\n",
    "print(f\"   F1-Macro: {test_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   F1-Micro: {test_metrics['f1_micro']:.4f}\")\n",
    "print(f\"   AUC-Macro: {test_metrics['auc_macro']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HD_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
